{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR4qfYrVoO4v"
      },
      "source": [
        "# Installs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd5aNaLVoR_g"
      },
      "source": [
        "## wandb\n",
        "\n",
        "You will need to fetch your api key from wandb.ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mA9qZoIDcx-h"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONgAWhqdoYy-"
      },
      "source": [
        "## Misc\n",
        "\n",
        "This may take a while"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS7a7xeEoaV9",
        "outputId": "cd9b8d3f-1fd9-492d-c996-842acf44d053"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummaryX in /usr/local/lib/python3.8/dist-packages (1.3.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (from torchsummaryX) (1.13.0+cu116)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from torchsummaryX) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchsummaryX) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->torchsummaryX) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->torchsummaryX) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->torchsummaryX) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch->torchsummaryX) (4.4.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: slugify in /usr/local/lib/python3.8/dist-packages (0.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.8/dist-packages (0.6.2)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (1.13.0+cu116)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (1.26.27)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (4.64.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from pytorch_pretrained_bert) (2022.6.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.4.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from boto3->pytorch_pretrained_bert) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from boto3->pytorch_pretrained_bert) (0.6.0)\n",
            "Requirement already satisfied: botocore<1.30.0,>=1.29.27 in /usr/local/lib/python3.8/dist-packages (from boto3->pytorch_pretrained_bert) (1.29.27)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.8/dist-packages (from botocore<1.30.0,>=1.29.27->boto3->pytorch_pretrained_bert) (1.25.11)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.30.0,>=1.29.27->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.27->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->pytorch_pretrained_bert) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchsummaryX\n",
        "!pip install slugify\n",
        "!pip install pytorch_pretrained_bert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWVONJxCobPc"
      },
      "source": [
        "## imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78ZTCIXoof2f",
        "outputId": "13aa2d5f-6c2b-4c5a-9e3d-77958f968fad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import time as Time\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
        "from pytorch_pretrained_bert import BertConfig\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummaryX import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import datetime\n",
        "import wandb\n",
        "import copy\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9v5ewZDMpYA"
      },
      "source": [
        "# Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Cp-716IMZRd",
        "outputId": "8f3fa027-7ef6-41f4-e33e-276d54a96b35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnIMttYa5EGc"
      },
      "source": [
        "# Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NeEA5A_y5HCw"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"num_labels\" : 2, # True or False Classification\n",
        "    \"lr\" : 2e-3,\n",
        "    \"epochs\" : 20,\n",
        "    'batch_size' : 8,\n",
        "    #'LR scheduler': 'CosineAnnealingLR',\n",
        "    'LR scheduler': 'ReduceLROnPlateau',\n",
        "                'scheduler factor': 0.5,\n",
        "    'scheduler threshold': 0.01,\n",
        "            'scheduler patience': 5,\n",
        "   #'scheduler Tmax': 0,\n",
        "    } # Feel free to add more items here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n37yR-fl5Yxg"
      },
      "source": [
        "# Read Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gCf4pk9y5bH3"
      },
      "outputs": [],
      "source": [
        "# remeber to change the path if you are not on google colab and directly uploading\n",
        "train_path = '/content/train2.tsv'\n",
        "test_path = '/content/test2.tsv'\n",
        "val_path = '/content/val2.tsv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "P9zWt35f5pDj"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(train_path, sep=\"\\t\", header=None)\n",
        "test_df = pd.read_csv(test_path, sep=\"\\t\", header=None)\n",
        "val_df = pd.read_csv(val_path, sep=\"\\t\", header=None)\n",
        "\n",
        "# Fill nan (empty boxes) with 0\n",
        "train_df = train_df.fillna(0)\n",
        "test_df = test_df.fillna(0)\n",
        "val_df = val_df.fillna(0)\n",
        "\n",
        "train = train_df.values\n",
        "test = test_df.values\n",
        "val = val_df.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "A3TkMhc78nKV"
      },
      "outputs": [],
      "source": [
        "# label: ground truth results from politifacts\n",
        "labels = {'train':[train[i][2] for i in range(len(train))], 'test':[test[i][2] for i in range(len(test))], 'val':[val[i][2] for i in range(len(val))]}\n",
        "# Short Statement\n",
        "statements = {'train':[train[i][3] for i in range(len(train))], 'test':[test[i][3] for i in range(len(test))], 'val':[val[i][3] for i in range(len(val))]}\n",
        "# Topic of Statement\n",
        "subjects = {'train':[train[i][4] for i in range(len(train))], 'test':[test[i][4] for i in range(len(test))], 'val':[val[i][4] for i in range(len(val))]}\n",
        "# Speaker\n",
        "speakers = {'train':[train[i][5] for i in range(len(train))], 'test':[test[i][5] for i in range(len(test))], 'val':[val[i][5] for i in range(len(val))]}\n",
        "# Speaker job or title\n",
        "jobs = {'train':[train[i][6] for i in range(len(train))], 'test':[test[i][6] for i in range(len(test))], 'val':[val[i][6] for i in range(len(val))]}\n",
        "# State of Relevance\n",
        "states = {'train':[train[i][7] for i in range(len(train))], 'test':[test[i][7] for i in range(len(test))], 'val':[val[i][7] for i in range(len(val))]}\n",
        "# party affiliation\n",
        "affiliations = {'train':[train[i][8] for i in range(len(train))], 'test':[test[i][8] for i in range(len(test))], 'val':[val[i][8] for i in range(len(val))]}\n",
        "# total history of speaker(count of barely true, false, half true, mostly true, pants on fire respectively)\n",
        "credits = {'train':[train[i][9:14] for i in range(len(train))], 'test':[test[i][9:14] for i in range(len(test))], 'val':[val[i][9:14] for i in range(len(val))]}\n",
        "# venue of statement\n",
        "contexts = {'train':[train[i][14] for i in range(len(train))], 'test':[test[i][14] for i in range(len(test))], 'val':[val[i][14] for i in range(len(val))]}\n",
        "# verdict justification from politifacts\n",
        "justification = {'train':[train[i][15] for i in range(len(train))], 'test':[test[i][15] for i in range(len(test))], 'val':[val[i][15] for i in range(len(val))]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0yBDQOrR93wo"
      },
      "outputs": [],
      "source": [
        "# currently only do 2 way classfication & simplify 6 way label into true and false\n",
        "# convert label to 2 hot based on verdict label\n",
        "if config[\"num_labels\"] ==2:\n",
        "  def onehot(label):\n",
        "    label_onehot = [0]*len(label)\n",
        "    for i in range(len(label)):\n",
        "      if label[i] =='true' or label[i] =='mostly-true' or label[i] =='half-true':\n",
        "        label_onehot[i] = [1,0]\n",
        "      elif label[i] =='barely-true' or label[i] =='false' or label[i] =='pants-fire':\n",
        "        label_onehot[i] = [0,1]\n",
        "      else:\n",
        "        print('Unexpected Label. Set vector to [0]')\n",
        "    return label_onehot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "N9Qc774Z_lVc"
      },
      "outputs": [],
      "source": [
        "# Convert to one hot\n",
        "label_onehot = {'train':onehot(labels['train']), 'test':onehot(labels['test']), 'val':onehot(labels['val'])}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWIGKC5TBWjz",
        "outputId": "cc4e853d-2aeb-4e80-8887-d6db6b96ab37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "abortion\n",
            "abortion dwayne-bohac State representative Texas republican a mailer\n"
          ]
        }
      ],
      "source": [
        "# Meta data\n",
        "metadata = {'train':[0]*len(train), 'val':[0]*len(val), 'test':[0]*len(test)}\n",
        "\n",
        "for i in range(len(train)):\n",
        "    subject = subjects['train'][i]\n",
        "    if subject == 0:\n",
        "        subject = 'None'\n",
        "\n",
        "    speaker = speakers['train'][i]\n",
        "    if speaker == 0:\n",
        "        speaker = 'None'\n",
        "\n",
        "    job = jobs['train'][i]\n",
        "    if job == 0:\n",
        "        job = 'None'\n",
        "\n",
        "    state = states['train'][i]\n",
        "    if state == 0:\n",
        "        state = 'None'\n",
        "\n",
        "    affiliation = affiliations['train'][i]\n",
        "    if affiliation == 0:\n",
        "        affiliation = 'None'\n",
        "\n",
        "    context = contexts['train'][i]\n",
        "    if context == 0 :\n",
        "        context = 'None'\n",
        "    if i == 0:\n",
        "      print(subject)\n",
        "    meta = subject + ' ' + speaker + ' ' + job + ' ' + state + ' ' + affiliation + ' ' + context\n",
        "    if i == 0:\n",
        "      print(meta)\n",
        "    metadata['train'][i] = meta\n",
        "\n",
        "for i in range(len(val)):\n",
        "    subject = subjects['val'][i]\n",
        "    if subject == 0:\n",
        "        subject = 'None'\n",
        "\n",
        "    speaker = speakers['val'][i]\n",
        "    if speaker == 0:\n",
        "        speaker = 'None'\n",
        "\n",
        "    job = jobs['val'][i]\n",
        "    if job == 0:\n",
        "        job = 'None'\n",
        "\n",
        "    state = states['val'][i]\n",
        "    if state == 0:\n",
        "        state = 'None'\n",
        "\n",
        "    affiliation = affiliations['val'][i]\n",
        "    if affiliation == 0:\n",
        "        affiliation = 'None'\n",
        "\n",
        "    context = contexts['val'][i]\n",
        "    if context == 0 :\n",
        "        context = 'None'\n",
        "\n",
        "    meta = subject + ' ' + speaker + ' ' + job + ' ' + state + ' ' + affiliation + ' ' + context\n",
        "\n",
        "    metadata['val'][i] = meta\n",
        "\n",
        "for i in range(len(test)):\n",
        "    subject = subjects['test'][i]\n",
        "    if subject == 0:\n",
        "        subject = 'None'\n",
        "\n",
        "    speaker = speakers['test'][i]\n",
        "    if speaker == 0:\n",
        "        speaker = 'None'\n",
        "\n",
        "    job = jobs['test'][i]\n",
        "    if job == 0:\n",
        "        job = 'None'\n",
        "\n",
        "    state = states['test'][i]\n",
        "    if state == 0:\n",
        "        state = 'None'\n",
        "\n",
        "    affiliation = affiliations['test'][i]\n",
        "    if affiliation == 0:\n",
        "        affiliation = 'None'\n",
        "\n",
        "    context = contexts['test'][i]\n",
        "    if context == 0 :\n",
        "        context = 'None'\n",
        "\n",
        "    meta = subject + ' ' + speaker + ' ' + job + ' ' + state + ' ' + affiliation + ' ' + context\n",
        "\n",
        "    metadata['test'][i] = meta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "jF4_ku_TDV_G"
      },
      "outputs": [],
      "source": [
        "# Credit score calculation\n",
        "# barely true weighs 0.75, false weighs 0.9, half true weigh 0.5, mostly true weigh 0.2, pants on fire weigh 1\n",
        "credit_score = {'train':[0]*len(train), 'val':[0]*len(val), 'test':[0]*len(test)}\n",
        "for i in range(len(train)):\n",
        "    credit = credits['train'][i]\n",
        "    if sum(credit) == 0:\n",
        "        score = 0.5\n",
        "    else:\n",
        "        score = (credit[3]*0.2 + credit[2]*0.5 + credit[0]*0.75 + credit[1]*0.9 + credit[4]*1)/(sum(credit))\n",
        "    credit_score['train'][i] = [score for i in range(2304)]\n",
        "\n",
        "for i in range(len(val)):\n",
        "    credit = credits['val'][i]\n",
        "    if sum(credit) == 0:\n",
        "        score = 0.5\n",
        "    else:\n",
        "        score = (credit[3]*0.2 + credit[2]*0.5 + credit[0]*0.75 + credit[1]*0.9 + credit[4]*1)/(sum(credit))\n",
        "    credit_score['val'][i] = [score for i in range(2304)]\n",
        "\n",
        "for i in range(len(test)):\n",
        "    credit = credits['test'][i]\n",
        "    if sum(credit) == 0:\n",
        "        score = 0.5\n",
        "    else:\n",
        "        score = (credit[3]*0.2 + credit[2]*0.5 + credit[0]*0.75 + credit[1]*0.9 + credit[4]*1)/(sum(credit))\n",
        "    credit_score['test'][i] = [score for i in range(2304)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ORNHnSFroP0"
      },
      "source": [
        "# Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xRVPUbWMFzBk"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Loading the statements\n",
        "X_train = statements['train']\n",
        "y_train = label_onehot['train']\n",
        "\n",
        "X_val = statements['val']\n",
        "y_val = label_onehot['val']\n",
        "\n",
        "\n",
        "X_test = statements['test']\n",
        "y_test = label_onehot['test']\n",
        "\n",
        "# Loading the justification\n",
        "X_train_just = justification['train']\n",
        "\n",
        "X_val_just = justification['val']\n",
        "\n",
        "\n",
        "X_test_just = justification['test']\n",
        "\n",
        "\n",
        "# Loading the meta data\n",
        "X_train_meta = metadata['train']\n",
        "X_val_meta = metadata['val']\n",
        "X_test_meta = metadata['test']\n",
        "\n",
        "# Loading Credit scores\n",
        "\n",
        "X_train_credit = credit_score['train']\n",
        "X_val_credit = credit_score['val']\n",
        "X_test_credit = credit_score['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zrLbmyvZGTAJ"
      },
      "outputs": [],
      "source": [
        "max_seq_length_stat = 64\n",
        "max_seq_length_just = 256\n",
        "max_seq_length_meta = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agmNBKf4JrLV"
      },
      "source": [
        "### Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "afd0_vlbJmr_"
      },
      "outputs": [],
      "source": [
        "class TextDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, xy_list ,transform=None): \n",
        "        '''\n",
        "        Initializes the dataset.\n",
        "\n",
        "        '''\n",
        "\n",
        "        # Load the xy list\n",
        "\n",
        "        self.x_y_list = xy_list\n",
        "        self.length = len(xy_list[0])\n",
        "        for i in range(self.length):\n",
        "          \n",
        "        #   Load in each statement and tokenize\n",
        "            tokenized_stat = tokenizer.tokenize(self.x_y_list[0][i])\n",
        "            if len(tokenized_stat) > max_seq_length_stat:\n",
        "              # clip if the statement is too long\n",
        "                tokenized_stat = tokenized_stat[:max_seq_length_stat]\n",
        "\n",
        "            # convert statement to ids\n",
        "            ids_stat  = tokenizer.convert_tokens_to_ids(tokenized_stat)\n",
        "            # pad the statement to given length\n",
        "            padding = [0] * (max_seq_length_stat - len(ids_stat))\n",
        "\n",
        "            ids_stat += padding\n",
        "            # sanity check\n",
        "            assert len(ids_stat) == max_seq_length_stat\n",
        "            \n",
        "            ids_stat = torch.tensor(ids_stat)\n",
        "            \n",
        "            if self.x_y_list[1][i] == 0:\n",
        "                self.x_y_list[1][i] = 'No justification'\n",
        "\n",
        "\n",
        "            tokenized_just = tokenizer.tokenize(self.x_y_list[1][i])\n",
        "            if len(tokenized_just) > max_seq_length_just:\n",
        "              # clip if the statement is too long\n",
        "                tokenized_just = tokenized_just[:max_seq_length_just]\n",
        "\n",
        "            # convert statement to ids\n",
        "            ids_just  = tokenizer.convert_tokens_to_ids(tokenized_just)\n",
        "            # pad the statement to given length\n",
        "            padding = [0] * (max_seq_length_just - len(ids_just))\n",
        "\n",
        "            ids_just += padding\n",
        "            # sanity check\n",
        "            assert len(ids_just) == max_seq_length_just\n",
        "\n",
        "            #if i == 1:\n",
        "            #  print(ids_just)\n",
        "            ids_just = torch.tensor(ids_just)\n",
        "\n",
        "            tokenized_meta = tokenizer.tokenize(self.x_y_list[2][i])\n",
        "            if len(tokenized_meta) > max_seq_length_meta:\n",
        "              # clip if the statement is too long\n",
        "                tokenized_meta = tokenized_meta[:max_seq_length_meta]\n",
        "\n",
        "            # convert statement to ids\n",
        "            ids_meta  = tokenizer.convert_tokens_to_ids(tokenized_meta)\n",
        "            # pad the statement to given length\n",
        "            padding = [0] * (max_seq_length_meta - len(ids_meta))\n",
        "\n",
        "            ids_meta += padding\n",
        "            # sanity check\n",
        "            assert len(ids_meta) == max_seq_length_meta\n",
        "\n",
        "            ids_meta = torch.tensor(ids_meta)\n",
        "            \n",
        "            credit_scr = torch.tensor(self.x_y_list[3][i]) # Credit score\n",
        "\n",
        "            label = torch.from_numpy(np.array(self.x_y_list[4][i]))\n",
        "\n",
        "            self.x_y_list[0][i] = ids_stat\n",
        "            self.x_y_list[1][i] = ids_just\n",
        "            self.x_y_list[2][i] = ids_meta\n",
        "            self.x_y_list[3][i] = credit_scr\n",
        "            self.x_y_list[4][i] = label\n",
        "\n",
        "    def __len__(self):\n",
        "        \n",
        "        '''\n",
        "        TODO: What do we return here?\n",
        "        '''\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        '''\n",
        "        TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n",
        "\n",
        "        If you didn't do the loading and processing of the data in __init__,\n",
        "        do that here.\n",
        "\n",
        "        Once done, return a tuple of features and labels.\n",
        "        '''\n",
        "        \n",
        "        ids_stat = self.x_y_list[0][ind] \n",
        "        ids_just = self.x_y_list[1][ind]\n",
        "        ids_meta = self.x_y_list[2][ind] \n",
        "        credit_scr = self.x_y_list[3][ind] \n",
        "        label = self.x_y_list[4][ind]\n",
        "\n",
        "        return ids_stat, ids_just, ids_meta, credit_scr, label\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt-veYcdL6Fe"
      },
      "source": [
        "### Data - Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4icymeX1ImUN"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = config['batch_size'] # Increase if your device can handle it\n",
        "\n",
        "transforms = [] # set of tranformations\n",
        "# You may pass this as a parameter to the dataset class above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmuPk9J6L8dz"
      },
      "source": [
        "### Data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_kG0gU2x4hH",
        "outputId": "b7e283e2-b399-4951-bfa8-e81d89cfa208"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# get me RAMMM!!!! \n",
        "import gc \n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mzoYfTKu14s",
        "outputId": "4f992a57-f043-47d2-86ba-3b2594f316da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size:  8\n",
            "Train dataset samples = 10240, batches = 1280\n",
            "Val dataset samples = 1284, batches = 161\n",
            "Test dataset samples = 1267, batches = 159\n"
          ]
        }
      ],
      "source": [
        "# Create objects for the dataset class\n",
        "#train_data = TextDataset([X_train[:100], X_train_just[:100], X_train_meta[:100], X_train_credit[:100], y_train[:100]])\n",
        "train_data = TextDataset([X_train, X_train_just, X_train_meta, X_train_credit, y_train])\n",
        "val_data = TextDataset([X_val, X_val_just, X_val_meta, X_val_credit, y_val]) \n",
        "test_data = TextDataset([X_test, X_test_just, X_test_meta, X_test_credit, y_test]) \n",
        "\n",
        "# Do NOT forget to pass in the collate function as parameter while creating the dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train_data, num_workers= 4,\n",
        "                                           batch_size=BATCH_SIZE, pin_memory= True,\n",
        "                                           shuffle= True)\n",
        "val_loader = torch.utils.data.DataLoader(val_data, num_workers= 2,\n",
        "                                           batch_size=BATCH_SIZE, pin_memory= True,\n",
        "                                           shuffle= True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, num_workers= 2,\n",
        "                                           batch_size=BATCH_SIZE, pin_memory= True,\n",
        "                                           shuffle= True)\n",
        "\n",
        "print(\"Batch size: \", BATCH_SIZE)\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXMtwyviKaxK",
        "outputId": "52101c4b-3703-4c32-a32d-09dfefb59049"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 64]) torch.Size([8, 256]) torch.Size([8, 32])\n",
            "tensor([[ 1999,  2010,  2034,  2694,  4357,  2004,  2343,  1010,  8112,  2056,\n",
            "          2057,  2323,  2831,  2000,  4238,  1012,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0],\n",
            "        [ 2758,  6969, 26033, 20265,  2253,  1998, 26421,  2044,  2017,  5045,\n",
            "          2032,  2013,  3519,  1012,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0],\n",
            "        [ 3060,  1011,  4841,  2024,  2062,  3497,  2000,  2022,  4727,  2011,\n",
            "          2610,  1998,  7331,  2000,  2936,  3827,  3408,  2005,  2725,  1996,\n",
            "          2168,  2518,  2008, 12461,  2079,  1012,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0],\n",
            "        [ 2119,  8096,  1998,  6820, 26282,  2024,  4394,  1996,  6565,  3484,\n",
            "          1997,  2037,  4494,  1012,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0],\n",
            "        [ 1037, 24051,  2207,  2011,  1051, 14693, 16454,  2389,  2267, 16481,\n",
            "          2008, 13857,  8112,  2001,  2019,  9003,  6926,  1012,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0],\n",
            "        [ 3841,  9806,  2003,  2770,  2005,  2343,  2000, 11027, 24394,  2006,\n",
            "          2231,  1012,  2021,  2002,  2987,  2102,  5254,  2008,  2002,  3473,\n",
            "          2039,  1999,  2270,  2816,  1010,  2288,  2270,  3847,  1998,  2833,\n",
            "         12133,  1010,  2288,  2489,  7877,  2013,  1037,  2231,  2565,  1010,\n",
            "          2001,  3271,  2011, 27352,  2895,  1010,  1998,  2288, 21877,  3363,\n",
            "          8624,  2005,  2267,  1012,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0],\n",
            "        [ 2104,  6986, 23680, 29165,  2100,  1010,  1006,  2167,  3792,  1007,\n",
            "          2038,  5357,  2000, 24233,  1999,  3836,  3477,  1012,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0],\n",
            "        [ 2005,  2416,  2086,  1010,  1045,  2366,  2256,  2451,  2004,  1037,\n",
            "          4603,  2110, 22964,  1012,  1999,  2296,  2028,  1997,  2216,  2086,\n",
            "          1010,  1045,  3271,  5703,  1996,  2110,  1005,  1055,  5166,  1012,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0]])\n",
            "tensor([[ 2011, 11671,  1010,  ...,     0,     0,     0],\n",
            "        [13042,  2056,  2008,  ...,     0,     0,     0],\n",
            "        [ 2088,  7159, 21351,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [ 3660,  2001,  1999,  ...,     0,     0,     0],\n",
            "        [ 2021,  1996, 12515,  ...,     0,     0,     0],\n",
            "        [ 2008,  5997,  2196,  ...,     0,     0,     0]])\n",
            "tensor([[ 3097,  1011,  3343, 10210,  2102,  1011, 19615,  2280,  3099,  4404,\n",
            "          3951, 12629,  2012,  1996,  3951,  2120,  4680,  1999,  9925,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0],\n",
            "        [ 5347,  1011,  8308,  1010,  9615,  2120,  1011,  3951,  1011,  7740,\n",
            "          1011,  2837,  3904,  3904,  3951,  1037,  2694,  4748,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0],\n",
            "        [ 4126, 18520,  1011,  7207,  4883,  4018,  2047,  2259,  7672,  1037,\n",
            "          2694,  4748,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0],\n",
            "        [ 3519,  1010,  6830,  1011,  2501, 14566,  1011,  2703,  4018,  2005,\n",
            "          1057,  1012,  1055,  1012,  4001,  1998,  7522,  5612,  3951,  2019,\n",
            "          4357,  2006,  6788,  1005,  1055,  1000,  3113,  1996,  2811,  1000,\n",
            "             0,     0],\n",
            "        [ 5347,  1011,  8308,  1010,  8112,  1011,  4182,  1011,  8196,  4677,\n",
            "          1011, 10373,  3904,  3904,  3904,  1037,  4677,  1041,  1011,  5653,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0],\n",
            "        [ 5347,  1011,  8308,  1010,  7574, 11494,  1011,  8037,  3904,  3904,\n",
            "          3029,  1037,  9130,  2033,  4168,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0],\n",
            "        [ 5347,  1011,  8308,  1010,  2495,  1010,  2110,  1011,  5166,  1010,\n",
            "          2163,  6060,  1011,  6201,  4905,  2236,  2167,  3792,  7672,  8466,\n",
            "          2006,  2591,  2865,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0],\n",
            "        [ 2110,  1011,  5166,  5726,  1011,  9574,  3904,  3904,  3951,  1037,\n",
            "          3049,  5653,  2121,  1012,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0]])\n",
            "tensor([[0.6187, 0.6187, 0.6187,  ..., 0.6187, 0.6187, 0.6187],\n",
            "        [0.7208, 0.7208, 0.7208,  ..., 0.7208, 0.7208, 0.7208],\n",
            "        [0.5104, 0.5104, 0.5104,  ..., 0.5104, 0.5104, 0.5104],\n",
            "        ...,\n",
            "        [0.7750, 0.7750, 0.7750,  ..., 0.7750, 0.7750, 0.7750],\n",
            "        [0.4900, 0.4900, 0.4900,  ..., 0.4900, 0.4900, 0.4900],\n",
            "        [0.7500, 0.7500, 0.7500,  ..., 0.7500, 0.7500, 0.7500]])\n",
            "tensor([[1, 0],\n",
            "        [0, 1],\n",
            "        [1, 0],\n",
            "        [0, 1],\n",
            "        [0, 1],\n",
            "        [1, 0],\n",
            "        [0, 1],\n",
            "        [0, 1]])\n"
          ]
        }
      ],
      "source": [
        "# sanity check\n",
        "for data in train_loader:\n",
        "    x, y, z, m, n = data\n",
        "    print(x.shape, y.shape, z.shape)\n",
        "    print(x)\n",
        "    print(y)\n",
        "    print(z)\n",
        "    print(m)\n",
        "    print(n)\n",
        "    x, y, z, m, n = x.to(device), y.to(device), z.to(device), m.to(device), n.to(device) \n",
        "    break "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly4mjUUUuJhy"
      },
      "source": [
        "# Model Config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLad4pChcuvX"
      },
      "source": [
        "## Basic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "cmocsazoDQF_"
      },
      "outputs": [],
      "source": [
        "config_bert = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "CukOseHrDQF_"
      },
      "outputs": [],
      "source": [
        "class BertLayerNorm(nn.Module):\n",
        "        def __init__(self, hidden_size, eps=1e-12):\n",
        "\n",
        "            super(BertLayerNorm, self).__init__()\n",
        "            self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "            self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
        "            self.variance_epsilon = eps\n",
        "\n",
        "        def forward(self, x):\n",
        "            u = x.mean(-1, keepdim=True)\n",
        "            s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "            x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
        "            return self.weight * x + self.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EQhvHr71GJfq"
      },
      "outputs": [],
      "source": [
        "class BertForSequenceClassification(nn.Module):\n",
        "    def __init__(self, num_labels=2): # Change number of labels here.\n",
        "        super(BertForSequenceClassification, self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(config_bert.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config_bert.hidden_size*3, num_labels)\n",
        "        nn.init.xavier_normal_(self.classifier.weight)\n",
        "\n",
        "    def forward_once(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
        "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "\n",
        "\n",
        "        return pooled_output\n",
        "\n",
        "    def forward(self, input_ids1, input_ids2, input_ids3, credit_sc):\n",
        "        # forward pass of input 1\n",
        "        output1 = self.forward_once(input_ids1, token_type_ids=None, attention_mask=None, labels=None)\n",
        "        # forward pass of input 2\n",
        "        output2 = self.forward_once(input_ids2, token_type_ids=None, attention_mask=None, labels=None)\n",
        "\n",
        "        output3 = self.forward_once(input_ids3, token_type_ids=None, attention_mask=None, labels=None)\n",
        "\n",
        "        out = torch.cat((output1, output2, output3), 1)\n",
        "\n",
        "        #Adding the credit score with the output\n",
        "\n",
        "        out = torch.add(credit_sc, out)\n",
        "\n",
        "        logits = self.classifier(out)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def freeze_bert_encoder(self):\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def unfreeze_bert_encoder(self):\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUThsowyQdN7"
      },
      "source": [
        "## INIT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "CGoiXd70tb5z"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "model = BertForSequenceClassification().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F1jmeZiwHJn8",
        "outputId": "ad5685ea-2031-4955-fe1c-1bf9e2b12484"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===========================================================================================================================\n",
            "                                                    Kernel Shape  \\\n",
            "Layer                                                              \n",
            "0_bert.embeddings.Embedding_word_embeddings         [768, 30522]   \n",
            "1_bert.embeddings.Embedding_position_embeddings       [768, 512]   \n",
            "2_bert.embeddings.Embedding_token_type_embeddings       [768, 2]   \n",
            "3_bert.embeddings.BertLayerNorm_LayerNorm                  [768]   \n",
            "4_bert.embeddings.Dropout_dropout                              -   \n",
            "5_bert.encoder.layer.0.attention.self.Linear_query    [768, 768]   \n",
            "6_bert.encoder.layer.0.attention.self.Linear_key      [768, 768]   \n",
            "7_bert.encoder.layer.0.attention.self.Linear_value    [768, 768]   \n",
            "8_bert.encoder.layer.0.attention.self.Dropout_d...             -   \n",
            "9_bert.encoder.layer.0.attention.output.Linear_...    [768, 768]   \n",
            "10_bert.encoder.layer.0.attention.output.Dropou...             -   \n",
            "11_bert.encoder.layer.0.attention.output.BertLa...         [768]   \n",
            "12_bert.encoder.layer.0.intermediate.Linear_dense    [768, 3072]   \n",
            "13_bert.encoder.layer.0.output.Linear_dense          [3072, 768]   \n",
            "14_bert.encoder.layer.0.output.Dropout_dropout                 -   \n",
            "15_bert.encoder.layer.0.output.BertLayerNorm_La...         [768]   \n",
            "16_bert.encoder.layer.1.attention.self.Linear_q...    [768, 768]   \n",
            "17_bert.encoder.layer.1.attention.self.Linear_key     [768, 768]   \n",
            "18_bert.encoder.layer.1.attention.self.Linear_v...    [768, 768]   \n",
            "19_bert.encoder.layer.1.attention.self.Dropout_...             -   \n",
            "20_bert.encoder.layer.1.attention.output.Linear...    [768, 768]   \n",
            "21_bert.encoder.layer.1.attention.output.Dropou...             -   \n",
            "22_bert.encoder.layer.1.attention.output.BertLa...         [768]   \n",
            "23_bert.encoder.layer.1.intermediate.Linear_dense    [768, 3072]   \n",
            "24_bert.encoder.layer.1.output.Linear_dense          [3072, 768]   \n",
            "25_bert.encoder.layer.1.output.Dropout_dropout                 -   \n",
            "26_bert.encoder.layer.1.output.BertLayerNorm_La...         [768]   \n",
            "27_bert.encoder.layer.2.attention.self.Linear_q...    [768, 768]   \n",
            "28_bert.encoder.layer.2.attention.self.Linear_key     [768, 768]   \n",
            "29_bert.encoder.layer.2.attention.self.Linear_v...    [768, 768]   \n",
            "30_bert.encoder.layer.2.attention.self.Dropout_...             -   \n",
            "31_bert.encoder.layer.2.attention.output.Linear...    [768, 768]   \n",
            "32_bert.encoder.layer.2.attention.output.Dropou...             -   \n",
            "33_bert.encoder.layer.2.attention.output.BertLa...         [768]   \n",
            "34_bert.encoder.layer.2.intermediate.Linear_dense    [768, 3072]   \n",
            "35_bert.encoder.layer.2.output.Linear_dense          [3072, 768]   \n",
            "36_bert.encoder.layer.2.output.Dropout_dropout                 -   \n",
            "37_bert.encoder.layer.2.output.BertLayerNorm_La...         [768]   \n",
            "38_bert.encoder.layer.3.attention.self.Linear_q...    [768, 768]   \n",
            "39_bert.encoder.layer.3.attention.self.Linear_key     [768, 768]   \n",
            "40_bert.encoder.layer.3.attention.self.Linear_v...    [768, 768]   \n",
            "41_bert.encoder.layer.3.attention.self.Dropout_...             -   \n",
            "42_bert.encoder.layer.3.attention.output.Linear...    [768, 768]   \n",
            "43_bert.encoder.layer.3.attention.output.Dropou...             -   \n",
            "44_bert.encoder.layer.3.attention.output.BertLa...         [768]   \n",
            "45_bert.encoder.layer.3.intermediate.Linear_dense    [768, 3072]   \n",
            "46_bert.encoder.layer.3.output.Linear_dense          [3072, 768]   \n",
            "47_bert.encoder.layer.3.output.Dropout_dropout                 -   \n",
            "48_bert.encoder.layer.3.output.BertLayerNorm_La...         [768]   \n",
            "49_bert.encoder.layer.4.attention.self.Linear_q...    [768, 768]   \n",
            "50_bert.encoder.layer.4.attention.self.Linear_key     [768, 768]   \n",
            "51_bert.encoder.layer.4.attention.self.Linear_v...    [768, 768]   \n",
            "52_bert.encoder.layer.4.attention.self.Dropout_...             -   \n",
            "53_bert.encoder.layer.4.attention.output.Linear...    [768, 768]   \n",
            "54_bert.encoder.layer.4.attention.output.Dropou...             -   \n",
            "55_bert.encoder.layer.4.attention.output.BertLa...         [768]   \n",
            "56_bert.encoder.layer.4.intermediate.Linear_dense    [768, 3072]   \n",
            "57_bert.encoder.layer.4.output.Linear_dense          [3072, 768]   \n",
            "58_bert.encoder.layer.4.output.Dropout_dropout                 -   \n",
            "59_bert.encoder.layer.4.output.BertLayerNorm_La...         [768]   \n",
            "60_bert.encoder.layer.5.attention.self.Linear_q...    [768, 768]   \n",
            "61_bert.encoder.layer.5.attention.self.Linear_key     [768, 768]   \n",
            "62_bert.encoder.layer.5.attention.self.Linear_v...    [768, 768]   \n",
            "63_bert.encoder.layer.5.attention.self.Dropout_...             -   \n",
            "64_bert.encoder.layer.5.attention.output.Linear...    [768, 768]   \n",
            "65_bert.encoder.layer.5.attention.output.Dropou...             -   \n",
            "66_bert.encoder.layer.5.attention.output.BertLa...         [768]   \n",
            "67_bert.encoder.layer.5.intermediate.Linear_dense    [768, 3072]   \n",
            "68_bert.encoder.layer.5.output.Linear_dense          [3072, 768]   \n",
            "69_bert.encoder.layer.5.output.Dropout_dropout                 -   \n",
            "70_bert.encoder.layer.5.output.BertLayerNorm_La...         [768]   \n",
            "71_bert.encoder.layer.6.attention.self.Linear_q...    [768, 768]   \n",
            "72_bert.encoder.layer.6.attention.self.Linear_key     [768, 768]   \n",
            "73_bert.encoder.layer.6.attention.self.Linear_v...    [768, 768]   \n",
            "74_bert.encoder.layer.6.attention.self.Dropout_...             -   \n",
            "75_bert.encoder.layer.6.attention.output.Linear...    [768, 768]   \n",
            "76_bert.encoder.layer.6.attention.output.Dropou...             -   \n",
            "77_bert.encoder.layer.6.attention.output.BertLa...         [768]   \n",
            "78_bert.encoder.layer.6.intermediate.Linear_dense    [768, 3072]   \n",
            "79_bert.encoder.layer.6.output.Linear_dense          [3072, 768]   \n",
            "80_bert.encoder.layer.6.output.Dropout_dropout                 -   \n",
            "81_bert.encoder.layer.6.output.BertLayerNorm_La...         [768]   \n",
            "82_bert.encoder.layer.7.attention.self.Linear_q...    [768, 768]   \n",
            "83_bert.encoder.layer.7.attention.self.Linear_key     [768, 768]   \n",
            "84_bert.encoder.layer.7.attention.self.Linear_v...    [768, 768]   \n",
            "85_bert.encoder.layer.7.attention.self.Dropout_...             -   \n",
            "86_bert.encoder.layer.7.attention.output.Linear...    [768, 768]   \n",
            "87_bert.encoder.layer.7.attention.output.Dropou...             -   \n",
            "88_bert.encoder.layer.7.attention.output.BertLa...         [768]   \n",
            "89_bert.encoder.layer.7.intermediate.Linear_dense    [768, 3072]   \n",
            "90_bert.encoder.layer.7.output.Linear_dense          [3072, 768]   \n",
            "91_bert.encoder.layer.7.output.Dropout_dropout                 -   \n",
            "92_bert.encoder.layer.7.output.BertLayerNorm_La...         [768]   \n",
            "93_bert.encoder.layer.8.attention.self.Linear_q...    [768, 768]   \n",
            "94_bert.encoder.layer.8.attention.self.Linear_key     [768, 768]   \n",
            "95_bert.encoder.layer.8.attention.self.Linear_v...    [768, 768]   \n",
            "96_bert.encoder.layer.8.attention.self.Dropout_...             -   \n",
            "97_bert.encoder.layer.8.attention.output.Linear...    [768, 768]   \n",
            "98_bert.encoder.layer.8.attention.output.Dropou...             -   \n",
            "99_bert.encoder.layer.8.attention.output.BertLa...         [768]   \n",
            "100_bert.encoder.layer.8.intermediate.Linear_dense   [768, 3072]   \n",
            "101_bert.encoder.layer.8.output.Linear_dense         [3072, 768]   \n",
            "102_bert.encoder.layer.8.output.Dropout_dropout                -   \n",
            "103_bert.encoder.layer.8.output.BertLayerNorm_L...         [768]   \n",
            "104_bert.encoder.layer.9.attention.self.Linear_...    [768, 768]   \n",
            "105_bert.encoder.layer.9.attention.self.Linear_key    [768, 768]   \n",
            "106_bert.encoder.layer.9.attention.self.Linear_...    [768, 768]   \n",
            "107_bert.encoder.layer.9.attention.self.Dropout...             -   \n",
            "108_bert.encoder.layer.9.attention.output.Linea...    [768, 768]   \n",
            "109_bert.encoder.layer.9.attention.output.Dropo...             -   \n",
            "110_bert.encoder.layer.9.attention.output.BertL...         [768]   \n",
            "111_bert.encoder.layer.9.intermediate.Linear_dense   [768, 3072]   \n",
            "112_bert.encoder.layer.9.output.Linear_dense         [3072, 768]   \n",
            "113_bert.encoder.layer.9.output.Dropout_dropout                -   \n",
            "114_bert.encoder.layer.9.output.BertLayerNorm_L...         [768]   \n",
            "115_bert.encoder.layer.10.attention.self.Linear...    [768, 768]   \n",
            "116_bert.encoder.layer.10.attention.self.Linear...    [768, 768]   \n",
            "117_bert.encoder.layer.10.attention.self.Linear...    [768, 768]   \n",
            "118_bert.encoder.layer.10.attention.self.Dropou...             -   \n",
            "119_bert.encoder.layer.10.attention.output.Line...    [768, 768]   \n",
            "120_bert.encoder.layer.10.attention.output.Drop...             -   \n",
            "121_bert.encoder.layer.10.attention.output.Bert...         [768]   \n",
            "122_bert.encoder.layer.10.intermediate.Linear_d...   [768, 3072]   \n",
            "123_bert.encoder.layer.10.output.Linear_dense        [3072, 768]   \n",
            "124_bert.encoder.layer.10.output.Dropout_dropout               -   \n",
            "125_bert.encoder.layer.10.output.BertLayerNorm_...         [768]   \n",
            "126_bert.encoder.layer.11.attention.self.Linear...    [768, 768]   \n",
            "127_bert.encoder.layer.11.attention.self.Linear...    [768, 768]   \n",
            "128_bert.encoder.layer.11.attention.self.Linear...    [768, 768]   \n",
            "129_bert.encoder.layer.11.attention.self.Dropou...             -   \n",
            "130_bert.encoder.layer.11.attention.output.Line...    [768, 768]   \n",
            "131_bert.encoder.layer.11.attention.output.Drop...             -   \n",
            "132_bert.encoder.layer.11.attention.output.Bert...         [768]   \n",
            "133_bert.encoder.layer.11.intermediate.Linear_d...   [768, 3072]   \n",
            "134_bert.encoder.layer.11.output.Linear_dense        [3072, 768]   \n",
            "135_bert.encoder.layer.11.output.Dropout_dropout               -   \n",
            "136_bert.encoder.layer.11.output.BertLayerNorm_...         [768]   \n",
            "137_bert.pooler.Linear_dense                          [768, 768]   \n",
            "138_bert.pooler.Tanh_activation                                -   \n",
            "139_dropout                                                    -   \n",
            "140_bert.embeddings.Embedding_word_embeddings       [768, 30522]   \n",
            "141_bert.embeddings.Embedding_position_embeddings     [768, 512]   \n",
            "142_bert.embeddings.Embedding_token_type_embedd...      [768, 2]   \n",
            "143_bert.embeddings.BertLayerNorm_LayerNorm                [768]   \n",
            "144_bert.embeddings.Dropout_dropout                            -   \n",
            "145_bert.encoder.layer.0.attention.self.Linear_...    [768, 768]   \n",
            "146_bert.encoder.layer.0.attention.self.Linear_key    [768, 768]   \n",
            "147_bert.encoder.layer.0.attention.self.Linear_...    [768, 768]   \n",
            "148_bert.encoder.layer.0.attention.self.Dropout...             -   \n",
            "149_bert.encoder.layer.0.attention.output.Linea...    [768, 768]   \n",
            "150_bert.encoder.layer.0.attention.output.Dropo...             -   \n",
            "151_bert.encoder.layer.0.attention.output.BertL...         [768]   \n",
            "152_bert.encoder.layer.0.intermediate.Linear_dense   [768, 3072]   \n",
            "153_bert.encoder.layer.0.output.Linear_dense         [3072, 768]   \n",
            "154_bert.encoder.layer.0.output.Dropout_dropout                -   \n",
            "155_bert.encoder.layer.0.output.BertLayerNorm_L...         [768]   \n",
            "156_bert.encoder.layer.1.attention.self.Linear_...    [768, 768]   \n",
            "157_bert.encoder.layer.1.attention.self.Linear_key    [768, 768]   \n",
            "158_bert.encoder.layer.1.attention.self.Linear_...    [768, 768]   \n",
            "159_bert.encoder.layer.1.attention.self.Dropout...             -   \n",
            "160_bert.encoder.layer.1.attention.output.Linea...    [768, 768]   \n",
            "161_bert.encoder.layer.1.attention.output.Dropo...             -   \n",
            "162_bert.encoder.layer.1.attention.output.BertL...         [768]   \n",
            "163_bert.encoder.layer.1.intermediate.Linear_dense   [768, 3072]   \n",
            "164_bert.encoder.layer.1.output.Linear_dense         [3072, 768]   \n",
            "165_bert.encoder.layer.1.output.Dropout_dropout                -   \n",
            "166_bert.encoder.layer.1.output.BertLayerNorm_L...         [768]   \n",
            "167_bert.encoder.layer.2.attention.self.Linear_...    [768, 768]   \n",
            "168_bert.encoder.layer.2.attention.self.Linear_key    [768, 768]   \n",
            "169_bert.encoder.layer.2.attention.self.Linear_...    [768, 768]   \n",
            "170_bert.encoder.layer.2.attention.self.Dropout...             -   \n",
            "171_bert.encoder.layer.2.attention.output.Linea...    [768, 768]   \n",
            "172_bert.encoder.layer.2.attention.output.Dropo...             -   \n",
            "173_bert.encoder.layer.2.attention.output.BertL...         [768]   \n",
            "174_bert.encoder.layer.2.intermediate.Linear_dense   [768, 3072]   \n",
            "175_bert.encoder.layer.2.output.Linear_dense         [3072, 768]   \n",
            "176_bert.encoder.layer.2.output.Dropout_dropout                -   \n",
            "177_bert.encoder.layer.2.output.BertLayerNorm_L...         [768]   \n",
            "178_bert.encoder.layer.3.attention.self.Linear_...    [768, 768]   \n",
            "179_bert.encoder.layer.3.attention.self.Linear_key    [768, 768]   \n",
            "180_bert.encoder.layer.3.attention.self.Linear_...    [768, 768]   \n",
            "181_bert.encoder.layer.3.attention.self.Dropout...             -   \n",
            "182_bert.encoder.layer.3.attention.output.Linea...    [768, 768]   \n",
            "183_bert.encoder.layer.3.attention.output.Dropo...             -   \n",
            "184_bert.encoder.layer.3.attention.output.BertL...         [768]   \n",
            "185_bert.encoder.layer.3.intermediate.Linear_dense   [768, 3072]   \n",
            "186_bert.encoder.layer.3.output.Linear_dense         [3072, 768]   \n",
            "187_bert.encoder.layer.3.output.Dropout_dropout                -   \n",
            "188_bert.encoder.layer.3.output.BertLayerNorm_L...         [768]   \n",
            "189_bert.encoder.layer.4.attention.self.Linear_...    [768, 768]   \n",
            "190_bert.encoder.layer.4.attention.self.Linear_key    [768, 768]   \n",
            "191_bert.encoder.layer.4.attention.self.Linear_...    [768, 768]   \n",
            "192_bert.encoder.layer.4.attention.self.Dropout...             -   \n",
            "193_bert.encoder.layer.4.attention.output.Linea...    [768, 768]   \n",
            "194_bert.encoder.layer.4.attention.output.Dropo...             -   \n",
            "195_bert.encoder.layer.4.attention.output.BertL...         [768]   \n",
            "196_bert.encoder.layer.4.intermediate.Linear_dense   [768, 3072]   \n",
            "197_bert.encoder.layer.4.output.Linear_dense         [3072, 768]   \n",
            "198_bert.encoder.layer.4.output.Dropout_dropout                -   \n",
            "199_bert.encoder.layer.4.output.BertLayerNorm_L...         [768]   \n",
            "200_bert.encoder.layer.5.attention.self.Linear_...    [768, 768]   \n",
            "201_bert.encoder.layer.5.attention.self.Linear_key    [768, 768]   \n",
            "202_bert.encoder.layer.5.attention.self.Linear_...    [768, 768]   \n",
            "203_bert.encoder.layer.5.attention.self.Dropout...             -   \n",
            "204_bert.encoder.layer.5.attention.output.Linea...    [768, 768]   \n",
            "205_bert.encoder.layer.5.attention.output.Dropo...             -   \n",
            "206_bert.encoder.layer.5.attention.output.BertL...         [768]   \n",
            "207_bert.encoder.layer.5.intermediate.Linear_dense   [768, 3072]   \n",
            "208_bert.encoder.layer.5.output.Linear_dense         [3072, 768]   \n",
            "209_bert.encoder.layer.5.output.Dropout_dropout                -   \n",
            "210_bert.encoder.layer.5.output.BertLayerNorm_L...         [768]   \n",
            "211_bert.encoder.layer.6.attention.self.Linear_...    [768, 768]   \n",
            "212_bert.encoder.layer.6.attention.self.Linear_key    [768, 768]   \n",
            "213_bert.encoder.layer.6.attention.self.Linear_...    [768, 768]   \n",
            "214_bert.encoder.layer.6.attention.self.Dropout...             -   \n",
            "215_bert.encoder.layer.6.attention.output.Linea...    [768, 768]   \n",
            "216_bert.encoder.layer.6.attention.output.Dropo...             -   \n",
            "217_bert.encoder.layer.6.attention.output.BertL...         [768]   \n",
            "218_bert.encoder.layer.6.intermediate.Linear_dense   [768, 3072]   \n",
            "219_bert.encoder.layer.6.output.Linear_dense         [3072, 768]   \n",
            "220_bert.encoder.layer.6.output.Dropout_dropout                -   \n",
            "221_bert.encoder.layer.6.output.BertLayerNorm_L...         [768]   \n",
            "222_bert.encoder.layer.7.attention.self.Linear_...    [768, 768]   \n",
            "223_bert.encoder.layer.7.attention.self.Linear_key    [768, 768]   \n",
            "224_bert.encoder.layer.7.attention.self.Linear_...    [768, 768]   \n",
            "225_bert.encoder.layer.7.attention.self.Dropout...             -   \n",
            "226_bert.encoder.layer.7.attention.output.Linea...    [768, 768]   \n",
            "227_bert.encoder.layer.7.attention.output.Dropo...             -   \n",
            "228_bert.encoder.layer.7.attention.output.BertL...         [768]   \n",
            "229_bert.encoder.layer.7.intermediate.Linear_dense   [768, 3072]   \n",
            "230_bert.encoder.layer.7.output.Linear_dense         [3072, 768]   \n",
            "231_bert.encoder.layer.7.output.Dropout_dropout                -   \n",
            "232_bert.encoder.layer.7.output.BertLayerNorm_L...         [768]   \n",
            "233_bert.encoder.layer.8.attention.self.Linear_...    [768, 768]   \n",
            "234_bert.encoder.layer.8.attention.self.Linear_key    [768, 768]   \n",
            "235_bert.encoder.layer.8.attention.self.Linear_...    [768, 768]   \n",
            "236_bert.encoder.layer.8.attention.self.Dropout...             -   \n",
            "237_bert.encoder.layer.8.attention.output.Linea...    [768, 768]   \n",
            "238_bert.encoder.layer.8.attention.output.Dropo...             -   \n",
            "239_bert.encoder.layer.8.attention.output.BertL...         [768]   \n",
            "240_bert.encoder.layer.8.intermediate.Linear_dense   [768, 3072]   \n",
            "241_bert.encoder.layer.8.output.Linear_dense         [3072, 768]   \n",
            "242_bert.encoder.layer.8.output.Dropout_dropout                -   \n",
            "243_bert.encoder.layer.8.output.BertLayerNorm_L...         [768]   \n",
            "244_bert.encoder.layer.9.attention.self.Linear_...    [768, 768]   \n",
            "245_bert.encoder.layer.9.attention.self.Linear_key    [768, 768]   \n",
            "246_bert.encoder.layer.9.attention.self.Linear_...    [768, 768]   \n",
            "247_bert.encoder.layer.9.attention.self.Dropout...             -   \n",
            "248_bert.encoder.layer.9.attention.output.Linea...    [768, 768]   \n",
            "249_bert.encoder.layer.9.attention.output.Dropo...             -   \n",
            "250_bert.encoder.layer.9.attention.output.BertL...         [768]   \n",
            "251_bert.encoder.layer.9.intermediate.Linear_dense   [768, 3072]   \n",
            "252_bert.encoder.layer.9.output.Linear_dense         [3072, 768]   \n",
            "253_bert.encoder.layer.9.output.Dropout_dropout                -   \n",
            "254_bert.encoder.layer.9.output.BertLayerNorm_L...         [768]   \n",
            "255_bert.encoder.layer.10.attention.self.Linear...    [768, 768]   \n",
            "256_bert.encoder.layer.10.attention.self.Linear...    [768, 768]   \n",
            "257_bert.encoder.layer.10.attention.self.Linear...    [768, 768]   \n",
            "258_bert.encoder.layer.10.attention.self.Dropou...             -   \n",
            "259_bert.encoder.layer.10.attention.output.Line...    [768, 768]   \n",
            "260_bert.encoder.layer.10.attention.output.Drop...             -   \n",
            "261_bert.encoder.layer.10.attention.output.Bert...         [768]   \n",
            "262_bert.encoder.layer.10.intermediate.Linear_d...   [768, 3072]   \n",
            "263_bert.encoder.layer.10.output.Linear_dense        [3072, 768]   \n",
            "264_bert.encoder.layer.10.output.Dropout_dropout               -   \n",
            "265_bert.encoder.layer.10.output.BertLayerNorm_...         [768]   \n",
            "266_bert.encoder.layer.11.attention.self.Linear...    [768, 768]   \n",
            "267_bert.encoder.layer.11.attention.self.Linear...    [768, 768]   \n",
            "268_bert.encoder.layer.11.attention.self.Linear...    [768, 768]   \n",
            "269_bert.encoder.layer.11.attention.self.Dropou...             -   \n",
            "270_bert.encoder.layer.11.attention.output.Line...    [768, 768]   \n",
            "271_bert.encoder.layer.11.attention.output.Drop...             -   \n",
            "272_bert.encoder.layer.11.attention.output.Bert...         [768]   \n",
            "273_bert.encoder.layer.11.intermediate.Linear_d...   [768, 3072]   \n",
            "274_bert.encoder.layer.11.output.Linear_dense        [3072, 768]   \n",
            "275_bert.encoder.layer.11.output.Dropout_dropout               -   \n",
            "276_bert.encoder.layer.11.output.BertLayerNorm_...         [768]   \n",
            "277_bert.pooler.Linear_dense                          [768, 768]   \n",
            "278_bert.pooler.Tanh_activation                                -   \n",
            "279_dropout                                                    -   \n",
            "280_bert.embeddings.Embedding_word_embeddings       [768, 30522]   \n",
            "281_bert.embeddings.Embedding_position_embeddings     [768, 512]   \n",
            "282_bert.embeddings.Embedding_token_type_embedd...      [768, 2]   \n",
            "283_bert.embeddings.BertLayerNorm_LayerNorm                [768]   \n",
            "284_bert.embeddings.Dropout_dropout                            -   \n",
            "285_bert.encoder.layer.0.attention.self.Linear_...    [768, 768]   \n",
            "286_bert.encoder.layer.0.attention.self.Linear_key    [768, 768]   \n",
            "287_bert.encoder.layer.0.attention.self.Linear_...    [768, 768]   \n",
            "288_bert.encoder.layer.0.attention.self.Dropout...             -   \n",
            "289_bert.encoder.layer.0.attention.output.Linea...    [768, 768]   \n",
            "290_bert.encoder.layer.0.attention.output.Dropo...             -   \n",
            "291_bert.encoder.layer.0.attention.output.BertL...         [768]   \n",
            "292_bert.encoder.layer.0.intermediate.Linear_dense   [768, 3072]   \n",
            "293_bert.encoder.layer.0.output.Linear_dense         [3072, 768]   \n",
            "294_bert.encoder.layer.0.output.Dropout_dropout                -   \n",
            "295_bert.encoder.layer.0.output.BertLayerNorm_L...         [768]   \n",
            "296_bert.encoder.layer.1.attention.self.Linear_...    [768, 768]   \n",
            "297_bert.encoder.layer.1.attention.self.Linear_key    [768, 768]   \n",
            "298_bert.encoder.layer.1.attention.self.Linear_...    [768, 768]   \n",
            "299_bert.encoder.layer.1.attention.self.Dropout...             -   \n",
            "300_bert.encoder.layer.1.attention.output.Linea...    [768, 768]   \n",
            "301_bert.encoder.layer.1.attention.output.Dropo...             -   \n",
            "302_bert.encoder.layer.1.attention.output.BertL...         [768]   \n",
            "303_bert.encoder.layer.1.intermediate.Linear_dense   [768, 3072]   \n",
            "304_bert.encoder.layer.1.output.Linear_dense         [3072, 768]   \n",
            "305_bert.encoder.layer.1.output.Dropout_dropout                -   \n",
            "306_bert.encoder.layer.1.output.BertLayerNorm_L...         [768]   \n",
            "307_bert.encoder.layer.2.attention.self.Linear_...    [768, 768]   \n",
            "308_bert.encoder.layer.2.attention.self.Linear_key    [768, 768]   \n",
            "309_bert.encoder.layer.2.attention.self.Linear_...    [768, 768]   \n",
            "310_bert.encoder.layer.2.attention.self.Dropout...             -   \n",
            "311_bert.encoder.layer.2.attention.output.Linea...    [768, 768]   \n",
            "312_bert.encoder.layer.2.attention.output.Dropo...             -   \n",
            "313_bert.encoder.layer.2.attention.output.BertL...         [768]   \n",
            "314_bert.encoder.layer.2.intermediate.Linear_dense   [768, 3072]   \n",
            "315_bert.encoder.layer.2.output.Linear_dense         [3072, 768]   \n",
            "316_bert.encoder.layer.2.output.Dropout_dropout                -   \n",
            "317_bert.encoder.layer.2.output.BertLayerNorm_L...         [768]   \n",
            "318_bert.encoder.layer.3.attention.self.Linear_...    [768, 768]   \n",
            "319_bert.encoder.layer.3.attention.self.Linear_key    [768, 768]   \n",
            "320_bert.encoder.layer.3.attention.self.Linear_...    [768, 768]   \n",
            "321_bert.encoder.layer.3.attention.self.Dropout...             -   \n",
            "322_bert.encoder.layer.3.attention.output.Linea...    [768, 768]   \n",
            "323_bert.encoder.layer.3.attention.output.Dropo...             -   \n",
            "324_bert.encoder.layer.3.attention.output.BertL...         [768]   \n",
            "325_bert.encoder.layer.3.intermediate.Linear_dense   [768, 3072]   \n",
            "326_bert.encoder.layer.3.output.Linear_dense         [3072, 768]   \n",
            "327_bert.encoder.layer.3.output.Dropout_dropout                -   \n",
            "328_bert.encoder.layer.3.output.BertLayerNorm_L...         [768]   \n",
            "329_bert.encoder.layer.4.attention.self.Linear_...    [768, 768]   \n",
            "330_bert.encoder.layer.4.attention.self.Linear_key    [768, 768]   \n",
            "331_bert.encoder.layer.4.attention.self.Linear_...    [768, 768]   \n",
            "332_bert.encoder.layer.4.attention.self.Dropout...             -   \n",
            "333_bert.encoder.layer.4.attention.output.Linea...    [768, 768]   \n",
            "334_bert.encoder.layer.4.attention.output.Dropo...             -   \n",
            "335_bert.encoder.layer.4.attention.output.BertL...         [768]   \n",
            "336_bert.encoder.layer.4.intermediate.Linear_dense   [768, 3072]   \n",
            "337_bert.encoder.layer.4.output.Linear_dense         [3072, 768]   \n",
            "338_bert.encoder.layer.4.output.Dropout_dropout                -   \n",
            "339_bert.encoder.layer.4.output.BertLayerNorm_L...         [768]   \n",
            "340_bert.encoder.layer.5.attention.self.Linear_...    [768, 768]   \n",
            "341_bert.encoder.layer.5.attention.self.Linear_key    [768, 768]   \n",
            "342_bert.encoder.layer.5.attention.self.Linear_...    [768, 768]   \n",
            "343_bert.encoder.layer.5.attention.self.Dropout...             -   \n",
            "344_bert.encoder.layer.5.attention.output.Linea...    [768, 768]   \n",
            "345_bert.encoder.layer.5.attention.output.Dropo...             -   \n",
            "346_bert.encoder.layer.5.attention.output.BertL...         [768]   \n",
            "347_bert.encoder.layer.5.intermediate.Linear_dense   [768, 3072]   \n",
            "348_bert.encoder.layer.5.output.Linear_dense         [3072, 768]   \n",
            "349_bert.encoder.layer.5.output.Dropout_dropout                -   \n",
            "350_bert.encoder.layer.5.output.BertLayerNorm_L...         [768]   \n",
            "351_bert.encoder.layer.6.attention.self.Linear_...    [768, 768]   \n",
            "352_bert.encoder.layer.6.attention.self.Linear_key    [768, 768]   \n",
            "353_bert.encoder.layer.6.attention.self.Linear_...    [768, 768]   \n",
            "354_bert.encoder.layer.6.attention.self.Dropout...             -   \n",
            "355_bert.encoder.layer.6.attention.output.Linea...    [768, 768]   \n",
            "356_bert.encoder.layer.6.attention.output.Dropo...             -   \n",
            "357_bert.encoder.layer.6.attention.output.BertL...         [768]   \n",
            "358_bert.encoder.layer.6.intermediate.Linear_dense   [768, 3072]   \n",
            "359_bert.encoder.layer.6.output.Linear_dense         [3072, 768]   \n",
            "360_bert.encoder.layer.6.output.Dropout_dropout                -   \n",
            "361_bert.encoder.layer.6.output.BertLayerNorm_L...         [768]   \n",
            "362_bert.encoder.layer.7.attention.self.Linear_...    [768, 768]   \n",
            "363_bert.encoder.layer.7.attention.self.Linear_key    [768, 768]   \n",
            "364_bert.encoder.layer.7.attention.self.Linear_...    [768, 768]   \n",
            "365_bert.encoder.layer.7.attention.self.Dropout...             -   \n",
            "366_bert.encoder.layer.7.attention.output.Linea...    [768, 768]   \n",
            "367_bert.encoder.layer.7.attention.output.Dropo...             -   \n",
            "368_bert.encoder.layer.7.attention.output.BertL...         [768]   \n",
            "369_bert.encoder.layer.7.intermediate.Linear_dense   [768, 3072]   \n",
            "370_bert.encoder.layer.7.output.Linear_dense         [3072, 768]   \n",
            "371_bert.encoder.layer.7.output.Dropout_dropout                -   \n",
            "372_bert.encoder.layer.7.output.BertLayerNorm_L...         [768]   \n",
            "373_bert.encoder.layer.8.attention.self.Linear_...    [768, 768]   \n",
            "374_bert.encoder.layer.8.attention.self.Linear_key    [768, 768]   \n",
            "375_bert.encoder.layer.8.attention.self.Linear_...    [768, 768]   \n",
            "376_bert.encoder.layer.8.attention.self.Dropout...             -   \n",
            "377_bert.encoder.layer.8.attention.output.Linea...    [768, 768]   \n",
            "378_bert.encoder.layer.8.attention.output.Dropo...             -   \n",
            "379_bert.encoder.layer.8.attention.output.BertL...         [768]   \n",
            "380_bert.encoder.layer.8.intermediate.Linear_dense   [768, 3072]   \n",
            "381_bert.encoder.layer.8.output.Linear_dense         [3072, 768]   \n",
            "382_bert.encoder.layer.8.output.Dropout_dropout                -   \n",
            "383_bert.encoder.layer.8.output.BertLayerNorm_L...         [768]   \n",
            "384_bert.encoder.layer.9.attention.self.Linear_...    [768, 768]   \n",
            "385_bert.encoder.layer.9.attention.self.Linear_key    [768, 768]   \n",
            "386_bert.encoder.layer.9.attention.self.Linear_...    [768, 768]   \n",
            "387_bert.encoder.layer.9.attention.self.Dropout...             -   \n",
            "388_bert.encoder.layer.9.attention.output.Linea...    [768, 768]   \n",
            "389_bert.encoder.layer.9.attention.output.Dropo...             -   \n",
            "390_bert.encoder.layer.9.attention.output.BertL...         [768]   \n",
            "391_bert.encoder.layer.9.intermediate.Linear_dense   [768, 3072]   \n",
            "392_bert.encoder.layer.9.output.Linear_dense         [3072, 768]   \n",
            "393_bert.encoder.layer.9.output.Dropout_dropout                -   \n",
            "394_bert.encoder.layer.9.output.BertLayerNorm_L...         [768]   \n",
            "395_bert.encoder.layer.10.attention.self.Linear...    [768, 768]   \n",
            "396_bert.encoder.layer.10.attention.self.Linear...    [768, 768]   \n",
            "397_bert.encoder.layer.10.attention.self.Linear...    [768, 768]   \n",
            "398_bert.encoder.layer.10.attention.self.Dropou...             -   \n",
            "399_bert.encoder.layer.10.attention.output.Line...    [768, 768]   \n",
            "400_bert.encoder.layer.10.attention.output.Drop...             -   \n",
            "401_bert.encoder.layer.10.attention.output.Bert...         [768]   \n",
            "402_bert.encoder.layer.10.intermediate.Linear_d...   [768, 3072]   \n",
            "403_bert.encoder.layer.10.output.Linear_dense        [3072, 768]   \n",
            "404_bert.encoder.layer.10.output.Dropout_dropout               -   \n",
            "405_bert.encoder.layer.10.output.BertLayerNorm_...         [768]   \n",
            "406_bert.encoder.layer.11.attention.self.Linear...    [768, 768]   \n",
            "407_bert.encoder.layer.11.attention.self.Linear...    [768, 768]   \n",
            "408_bert.encoder.layer.11.attention.self.Linear...    [768, 768]   \n",
            "409_bert.encoder.layer.11.attention.self.Dropou...             -   \n",
            "410_bert.encoder.layer.11.attention.output.Line...    [768, 768]   \n",
            "411_bert.encoder.layer.11.attention.output.Drop...             -   \n",
            "412_bert.encoder.layer.11.attention.output.Bert...         [768]   \n",
            "413_bert.encoder.layer.11.intermediate.Linear_d...   [768, 3072]   \n",
            "414_bert.encoder.layer.11.output.Linear_dense        [3072, 768]   \n",
            "415_bert.encoder.layer.11.output.Dropout_dropout               -   \n",
            "416_bert.encoder.layer.11.output.BertLayerNorm_...         [768]   \n",
            "417_bert.pooler.Linear_dense                          [768, 768]   \n",
            "418_bert.pooler.Tanh_activation                                -   \n",
            "419_dropout                                                    -   \n",
            "420_classifier                                         [2304, 2]   \n",
            "\n",
            "                                                         Output Shape  \\\n",
            "Layer                                                                   \n",
            "0_bert.embeddings.Embedding_word_embeddings              [8, 64, 768]   \n",
            "1_bert.embeddings.Embedding_position_embeddings          [8, 64, 768]   \n",
            "2_bert.embeddings.Embedding_token_type_embeddings        [8, 64, 768]   \n",
            "3_bert.embeddings.BertLayerNorm_LayerNorm                [8, 64, 768]   \n",
            "4_bert.embeddings.Dropout_dropout                        [8, 64, 768]   \n",
            "5_bert.encoder.layer.0.attention.self.Linear_query       [8, 64, 768]   \n",
            "6_bert.encoder.layer.0.attention.self.Linear_key         [8, 64, 768]   \n",
            "7_bert.encoder.layer.0.attention.self.Linear_value       [8, 64, 768]   \n",
            "8_bert.encoder.layer.0.attention.self.Dropout_d...    [8, 12, 64, 64]   \n",
            "9_bert.encoder.layer.0.attention.output.Linear_...       [8, 64, 768]   \n",
            "10_bert.encoder.layer.0.attention.output.Dropou...       [8, 64, 768]   \n",
            "11_bert.encoder.layer.0.attention.output.BertLa...       [8, 64, 768]   \n",
            "12_bert.encoder.layer.0.intermediate.Linear_dense       [8, 64, 3072]   \n",
            "13_bert.encoder.layer.0.output.Linear_dense              [8, 64, 768]   \n",
            "14_bert.encoder.layer.0.output.Dropout_dropout           [8, 64, 768]   \n",
            "15_bert.encoder.layer.0.output.BertLayerNorm_La...       [8, 64, 768]   \n",
            "16_bert.encoder.layer.1.attention.self.Linear_q...       [8, 64, 768]   \n",
            "17_bert.encoder.layer.1.attention.self.Linear_key        [8, 64, 768]   \n",
            "18_bert.encoder.layer.1.attention.self.Linear_v...       [8, 64, 768]   \n",
            "19_bert.encoder.layer.1.attention.self.Dropout_...    [8, 12, 64, 64]   \n",
            "20_bert.encoder.layer.1.attention.output.Linear...       [8, 64, 768]   \n",
            "21_bert.encoder.layer.1.attention.output.Dropou...       [8, 64, 768]   \n",
            "22_bert.encoder.layer.1.attention.output.BertLa...       [8, 64, 768]   \n",
            "23_bert.encoder.layer.1.intermediate.Linear_dense       [8, 64, 3072]   \n",
            "24_bert.encoder.layer.1.output.Linear_dense              [8, 64, 768]   \n",
            "25_bert.encoder.layer.1.output.Dropout_dropout           [8, 64, 768]   \n",
            "26_bert.encoder.layer.1.output.BertLayerNorm_La...       [8, 64, 768]   \n",
            "27_bert.encoder.layer.2.attention.self.Linear_q...       [8, 64, 768]   \n",
            "28_bert.encoder.layer.2.attention.self.Linear_key        [8, 64, 768]   \n",
            "29_bert.encoder.layer.2.attention.self.Linear_v...       [8, 64, 768]   \n",
            "30_bert.encoder.layer.2.attention.self.Dropout_...    [8, 12, 64, 64]   \n",
            "31_bert.encoder.layer.2.attention.output.Linear...       [8, 64, 768]   \n",
            "32_bert.encoder.layer.2.attention.output.Dropou...       [8, 64, 768]   \n",
            "33_bert.encoder.layer.2.attention.output.BertLa...       [8, 64, 768]   \n",
            "34_bert.encoder.layer.2.intermediate.Linear_dense       [8, 64, 3072]   \n",
            "35_bert.encoder.layer.2.output.Linear_dense              [8, 64, 768]   \n",
            "36_bert.encoder.layer.2.output.Dropout_dropout           [8, 64, 768]   \n",
            "37_bert.encoder.layer.2.output.BertLayerNorm_La...       [8, 64, 768]   \n",
            "38_bert.encoder.layer.3.attention.self.Linear_q...       [8, 64, 768]   \n",
            "39_bert.encoder.layer.3.attention.self.Linear_key        [8, 64, 768]   \n",
            "40_bert.encoder.layer.3.attention.self.Linear_v...       [8, 64, 768]   \n",
            "41_bert.encoder.layer.3.attention.self.Dropout_...    [8, 12, 64, 64]   \n",
            "42_bert.encoder.layer.3.attention.output.Linear...       [8, 64, 768]   \n",
            "43_bert.encoder.layer.3.attention.output.Dropou...       [8, 64, 768]   \n",
            "44_bert.encoder.layer.3.attention.output.BertLa...       [8, 64, 768]   \n",
            "45_bert.encoder.layer.3.intermediate.Linear_dense       [8, 64, 3072]   \n",
            "46_bert.encoder.layer.3.output.Linear_dense              [8, 64, 768]   \n",
            "47_bert.encoder.layer.3.output.Dropout_dropout           [8, 64, 768]   \n",
            "48_bert.encoder.layer.3.output.BertLayerNorm_La...       [8, 64, 768]   \n",
            "49_bert.encoder.layer.4.attention.self.Linear_q...       [8, 64, 768]   \n",
            "50_bert.encoder.layer.4.attention.self.Linear_key        [8, 64, 768]   \n",
            "51_bert.encoder.layer.4.attention.self.Linear_v...       [8, 64, 768]   \n",
            "52_bert.encoder.layer.4.attention.self.Dropout_...    [8, 12, 64, 64]   \n",
            "53_bert.encoder.layer.4.attention.output.Linear...       [8, 64, 768]   \n",
            "54_bert.encoder.layer.4.attention.output.Dropou...       [8, 64, 768]   \n",
            "55_bert.encoder.layer.4.attention.output.BertLa...       [8, 64, 768]   \n",
            "56_bert.encoder.layer.4.intermediate.Linear_dense       [8, 64, 3072]   \n",
            "57_bert.encoder.layer.4.output.Linear_dense              [8, 64, 768]   \n",
            "58_bert.encoder.layer.4.output.Dropout_dropout           [8, 64, 768]   \n",
            "59_bert.encoder.layer.4.output.BertLayerNorm_La...       [8, 64, 768]   \n",
            "60_bert.encoder.layer.5.attention.self.Linear_q...       [8, 64, 768]   \n",
            "61_bert.encoder.layer.5.attention.self.Linear_key        [8, 64, 768]   \n",
            "62_bert.encoder.layer.5.attention.self.Linear_v...       [8, 64, 768]   \n",
            "63_bert.encoder.layer.5.attention.self.Dropout_...    [8, 12, 64, 64]   \n",
            "64_bert.encoder.layer.5.attention.output.Linear...       [8, 64, 768]   \n",
            "65_bert.encoder.layer.5.attention.output.Dropou...       [8, 64, 768]   \n",
            "66_bert.encoder.layer.5.attention.output.BertLa...       [8, 64, 768]   \n",
            "67_bert.encoder.layer.5.intermediate.Linear_dense       [8, 64, 3072]   \n",
            "68_bert.encoder.layer.5.output.Linear_dense              [8, 64, 768]   \n",
            "69_bert.encoder.layer.5.output.Dropout_dropout           [8, 64, 768]   \n",
            "70_bert.encoder.layer.5.output.BertLayerNorm_La...       [8, 64, 768]   \n",
            "71_bert.encoder.layer.6.attention.self.Linear_q...       [8, 64, 768]   \n",
            "72_bert.encoder.layer.6.attention.self.Linear_key        [8, 64, 768]   \n",
            "73_bert.encoder.layer.6.attention.self.Linear_v...       [8, 64, 768]   \n",
            "74_bert.encoder.layer.6.attention.self.Dropout_...    [8, 12, 64, 64]   \n",
            "75_bert.encoder.layer.6.attention.output.Linear...       [8, 64, 768]   \n",
            "76_bert.encoder.layer.6.attention.output.Dropou...       [8, 64, 768]   \n",
            "77_bert.encoder.layer.6.attention.output.BertLa...       [8, 64, 768]   \n",
            "78_bert.encoder.layer.6.intermediate.Linear_dense       [8, 64, 3072]   \n",
            "79_bert.encoder.layer.6.output.Linear_dense              [8, 64, 768]   \n",
            "80_bert.encoder.layer.6.output.Dropout_dropout           [8, 64, 768]   \n",
            "81_bert.encoder.layer.6.output.BertLayerNorm_La...       [8, 64, 768]   \n",
            "82_bert.encoder.layer.7.attention.self.Linear_q...       [8, 64, 768]   \n",
            "83_bert.encoder.layer.7.attention.self.Linear_key        [8, 64, 768]   \n",
            "84_bert.encoder.layer.7.attention.self.Linear_v...       [8, 64, 768]   \n",
            "85_bert.encoder.layer.7.attention.self.Dropout_...    [8, 12, 64, 64]   \n",
            "86_bert.encoder.layer.7.attention.output.Linear...       [8, 64, 768]   \n",
            "87_bert.encoder.layer.7.attention.output.Dropou...       [8, 64, 768]   \n",
            "88_bert.encoder.layer.7.attention.output.BertLa...       [8, 64, 768]   \n",
            "89_bert.encoder.layer.7.intermediate.Linear_dense       [8, 64, 3072]   \n",
            "90_bert.encoder.layer.7.output.Linear_dense              [8, 64, 768]   \n",
            "91_bert.encoder.layer.7.output.Dropout_dropout           [8, 64, 768]   \n",
            "92_bert.encoder.layer.7.output.BertLayerNorm_La...       [8, 64, 768]   \n",
            "93_bert.encoder.layer.8.attention.self.Linear_q...       [8, 64, 768]   \n",
            "94_bert.encoder.layer.8.attention.self.Linear_key        [8, 64, 768]   \n",
            "95_bert.encoder.layer.8.attention.self.Linear_v...       [8, 64, 768]   \n",
            "96_bert.encoder.layer.8.attention.self.Dropout_...    [8, 12, 64, 64]   \n",
            "97_bert.encoder.layer.8.attention.output.Linear...       [8, 64, 768]   \n",
            "98_bert.encoder.layer.8.attention.output.Dropou...       [8, 64, 768]   \n",
            "99_bert.encoder.layer.8.attention.output.BertLa...       [8, 64, 768]   \n",
            "100_bert.encoder.layer.8.intermediate.Linear_dense      [8, 64, 3072]   \n",
            "101_bert.encoder.layer.8.output.Linear_dense             [8, 64, 768]   \n",
            "102_bert.encoder.layer.8.output.Dropout_dropout          [8, 64, 768]   \n",
            "103_bert.encoder.layer.8.output.BertLayerNorm_L...       [8, 64, 768]   \n",
            "104_bert.encoder.layer.9.attention.self.Linear_...       [8, 64, 768]   \n",
            "105_bert.encoder.layer.9.attention.self.Linear_key       [8, 64, 768]   \n",
            "106_bert.encoder.layer.9.attention.self.Linear_...       [8, 64, 768]   \n",
            "107_bert.encoder.layer.9.attention.self.Dropout...    [8, 12, 64, 64]   \n",
            "108_bert.encoder.layer.9.attention.output.Linea...       [8, 64, 768]   \n",
            "109_bert.encoder.layer.9.attention.output.Dropo...       [8, 64, 768]   \n",
            "110_bert.encoder.layer.9.attention.output.BertL...       [8, 64, 768]   \n",
            "111_bert.encoder.layer.9.intermediate.Linear_dense      [8, 64, 3072]   \n",
            "112_bert.encoder.layer.9.output.Linear_dense             [8, 64, 768]   \n",
            "113_bert.encoder.layer.9.output.Dropout_dropout          [8, 64, 768]   \n",
            "114_bert.encoder.layer.9.output.BertLayerNorm_L...       [8, 64, 768]   \n",
            "115_bert.encoder.layer.10.attention.self.Linear...       [8, 64, 768]   \n",
            "116_bert.encoder.layer.10.attention.self.Linear...       [8, 64, 768]   \n",
            "117_bert.encoder.layer.10.attention.self.Linear...       [8, 64, 768]   \n",
            "118_bert.encoder.layer.10.attention.self.Dropou...    [8, 12, 64, 64]   \n",
            "119_bert.encoder.layer.10.attention.output.Line...       [8, 64, 768]   \n",
            "120_bert.encoder.layer.10.attention.output.Drop...       [8, 64, 768]   \n",
            "121_bert.encoder.layer.10.attention.output.Bert...       [8, 64, 768]   \n",
            "122_bert.encoder.layer.10.intermediate.Linear_d...      [8, 64, 3072]   \n",
            "123_bert.encoder.layer.10.output.Linear_dense            [8, 64, 768]   \n",
            "124_bert.encoder.layer.10.output.Dropout_dropout         [8, 64, 768]   \n",
            "125_bert.encoder.layer.10.output.BertLayerNorm_...       [8, 64, 768]   \n",
            "126_bert.encoder.layer.11.attention.self.Linear...       [8, 64, 768]   \n",
            "127_bert.encoder.layer.11.attention.self.Linear...       [8, 64, 768]   \n",
            "128_bert.encoder.layer.11.attention.self.Linear...       [8, 64, 768]   \n",
            "129_bert.encoder.layer.11.attention.self.Dropou...    [8, 12, 64, 64]   \n",
            "130_bert.encoder.layer.11.attention.output.Line...       [8, 64, 768]   \n",
            "131_bert.encoder.layer.11.attention.output.Drop...       [8, 64, 768]   \n",
            "132_bert.encoder.layer.11.attention.output.Bert...       [8, 64, 768]   \n",
            "133_bert.encoder.layer.11.intermediate.Linear_d...      [8, 64, 3072]   \n",
            "134_bert.encoder.layer.11.output.Linear_dense            [8, 64, 768]   \n",
            "135_bert.encoder.layer.11.output.Dropout_dropout         [8, 64, 768]   \n",
            "136_bert.encoder.layer.11.output.BertLayerNorm_...       [8, 64, 768]   \n",
            "137_bert.pooler.Linear_dense                                 [8, 768]   \n",
            "138_bert.pooler.Tanh_activation                              [8, 768]   \n",
            "139_dropout                                                  [8, 768]   \n",
            "140_bert.embeddings.Embedding_word_embeddings           [8, 256, 768]   \n",
            "141_bert.embeddings.Embedding_position_embeddings       [8, 256, 768]   \n",
            "142_bert.embeddings.Embedding_token_type_embedd...      [8, 256, 768]   \n",
            "143_bert.embeddings.BertLayerNorm_LayerNorm             [8, 256, 768]   \n",
            "144_bert.embeddings.Dropout_dropout                     [8, 256, 768]   \n",
            "145_bert.encoder.layer.0.attention.self.Linear_...      [8, 256, 768]   \n",
            "146_bert.encoder.layer.0.attention.self.Linear_key      [8, 256, 768]   \n",
            "147_bert.encoder.layer.0.attention.self.Linear_...      [8, 256, 768]   \n",
            "148_bert.encoder.layer.0.attention.self.Dropout...  [8, 12, 256, 256]   \n",
            "149_bert.encoder.layer.0.attention.output.Linea...      [8, 256, 768]   \n",
            "150_bert.encoder.layer.0.attention.output.Dropo...      [8, 256, 768]   \n",
            "151_bert.encoder.layer.0.attention.output.BertL...      [8, 256, 768]   \n",
            "152_bert.encoder.layer.0.intermediate.Linear_dense     [8, 256, 3072]   \n",
            "153_bert.encoder.layer.0.output.Linear_dense            [8, 256, 768]   \n",
            "154_bert.encoder.layer.0.output.Dropout_dropout         [8, 256, 768]   \n",
            "155_bert.encoder.layer.0.output.BertLayerNorm_L...      [8, 256, 768]   \n",
            "156_bert.encoder.layer.1.attention.self.Linear_...      [8, 256, 768]   \n",
            "157_bert.encoder.layer.1.attention.self.Linear_key      [8, 256, 768]   \n",
            "158_bert.encoder.layer.1.attention.self.Linear_...      [8, 256, 768]   \n",
            "159_bert.encoder.layer.1.attention.self.Dropout...  [8, 12, 256, 256]   \n",
            "160_bert.encoder.layer.1.attention.output.Linea...      [8, 256, 768]   \n",
            "161_bert.encoder.layer.1.attention.output.Dropo...      [8, 256, 768]   \n",
            "162_bert.encoder.layer.1.attention.output.BertL...      [8, 256, 768]   \n",
            "163_bert.encoder.layer.1.intermediate.Linear_dense     [8, 256, 3072]   \n",
            "164_bert.encoder.layer.1.output.Linear_dense            [8, 256, 768]   \n",
            "165_bert.encoder.layer.1.output.Dropout_dropout         [8, 256, 768]   \n",
            "166_bert.encoder.layer.1.output.BertLayerNorm_L...      [8, 256, 768]   \n",
            "167_bert.encoder.layer.2.attention.self.Linear_...      [8, 256, 768]   \n",
            "168_bert.encoder.layer.2.attention.self.Linear_key      [8, 256, 768]   \n",
            "169_bert.encoder.layer.2.attention.self.Linear_...      [8, 256, 768]   \n",
            "170_bert.encoder.layer.2.attention.self.Dropout...  [8, 12, 256, 256]   \n",
            "171_bert.encoder.layer.2.attention.output.Linea...      [8, 256, 768]   \n",
            "172_bert.encoder.layer.2.attention.output.Dropo...      [8, 256, 768]   \n",
            "173_bert.encoder.layer.2.attention.output.BertL...      [8, 256, 768]   \n",
            "174_bert.encoder.layer.2.intermediate.Linear_dense     [8, 256, 3072]   \n",
            "175_bert.encoder.layer.2.output.Linear_dense            [8, 256, 768]   \n",
            "176_bert.encoder.layer.2.output.Dropout_dropout         [8, 256, 768]   \n",
            "177_bert.encoder.layer.2.output.BertLayerNorm_L...      [8, 256, 768]   \n",
            "178_bert.encoder.layer.3.attention.self.Linear_...      [8, 256, 768]   \n",
            "179_bert.encoder.layer.3.attention.self.Linear_key      [8, 256, 768]   \n",
            "180_bert.encoder.layer.3.attention.self.Linear_...      [8, 256, 768]   \n",
            "181_bert.encoder.layer.3.attention.self.Dropout...  [8, 12, 256, 256]   \n",
            "182_bert.encoder.layer.3.attention.output.Linea...      [8, 256, 768]   \n",
            "183_bert.encoder.layer.3.attention.output.Dropo...      [8, 256, 768]   \n",
            "184_bert.encoder.layer.3.attention.output.BertL...      [8, 256, 768]   \n",
            "185_bert.encoder.layer.3.intermediate.Linear_dense     [8, 256, 3072]   \n",
            "186_bert.encoder.layer.3.output.Linear_dense            [8, 256, 768]   \n",
            "187_bert.encoder.layer.3.output.Dropout_dropout         [8, 256, 768]   \n",
            "188_bert.encoder.layer.3.output.BertLayerNorm_L...      [8, 256, 768]   \n",
            "189_bert.encoder.layer.4.attention.self.Linear_...      [8, 256, 768]   \n",
            "190_bert.encoder.layer.4.attention.self.Linear_key      [8, 256, 768]   \n",
            "191_bert.encoder.layer.4.attention.self.Linear_...      [8, 256, 768]   \n",
            "192_bert.encoder.layer.4.attention.self.Dropout...  [8, 12, 256, 256]   \n",
            "193_bert.encoder.layer.4.attention.output.Linea...      [8, 256, 768]   \n",
            "194_bert.encoder.layer.4.attention.output.Dropo...      [8, 256, 768]   \n",
            "195_bert.encoder.layer.4.attention.output.BertL...      [8, 256, 768]   \n",
            "196_bert.encoder.layer.4.intermediate.Linear_dense     [8, 256, 3072]   \n",
            "197_bert.encoder.layer.4.output.Linear_dense            [8, 256, 768]   \n",
            "198_bert.encoder.layer.4.output.Dropout_dropout         [8, 256, 768]   \n",
            "199_bert.encoder.layer.4.output.BertLayerNorm_L...      [8, 256, 768]   \n",
            "200_bert.encoder.layer.5.attention.self.Linear_...      [8, 256, 768]   \n",
            "201_bert.encoder.layer.5.attention.self.Linear_key      [8, 256, 768]   \n",
            "202_bert.encoder.layer.5.attention.self.Linear_...      [8, 256, 768]   \n",
            "203_bert.encoder.layer.5.attention.self.Dropout...  [8, 12, 256, 256]   \n",
            "204_bert.encoder.layer.5.attention.output.Linea...      [8, 256, 768]   \n",
            "205_bert.encoder.layer.5.attention.output.Dropo...      [8, 256, 768]   \n",
            "206_bert.encoder.layer.5.attention.output.BertL...      [8, 256, 768]   \n",
            "207_bert.encoder.layer.5.intermediate.Linear_dense     [8, 256, 3072]   \n",
            "208_bert.encoder.layer.5.output.Linear_dense            [8, 256, 768]   \n",
            "209_bert.encoder.layer.5.output.Dropout_dropout         [8, 256, 768]   \n",
            "210_bert.encoder.layer.5.output.BertLayerNorm_L...      [8, 256, 768]   \n",
            "211_bert.encoder.layer.6.attention.self.Linear_...      [8, 256, 768]   \n",
            "212_bert.encoder.layer.6.attention.self.Linear_key      [8, 256, 768]   \n",
            "213_bert.encoder.layer.6.attention.self.Linear_...      [8, 256, 768]   \n",
            "214_bert.encoder.layer.6.attention.self.Dropout...  [8, 12, 256, 256]   \n",
            "215_bert.encoder.layer.6.attention.output.Linea...      [8, 256, 768]   \n",
            "216_bert.encoder.layer.6.attention.output.Dropo...      [8, 256, 768]   \n",
            "217_bert.encoder.layer.6.attention.output.BertL...      [8, 256, 768]   \n",
            "218_bert.encoder.layer.6.intermediate.Linear_dense     [8, 256, 3072]   \n",
            "219_bert.encoder.layer.6.output.Linear_dense            [8, 256, 768]   \n",
            "220_bert.encoder.layer.6.output.Dropout_dropout         [8, 256, 768]   \n",
            "221_bert.encoder.layer.6.output.BertLayerNorm_L...      [8, 256, 768]   \n",
            "222_bert.encoder.layer.7.attention.self.Linear_...      [8, 256, 768]   \n",
            "223_bert.encoder.layer.7.attention.self.Linear_key      [8, 256, 768]   \n",
            "224_bert.encoder.layer.7.attention.self.Linear_...      [8, 256, 768]   \n",
            "225_bert.encoder.layer.7.attention.self.Dropout...  [8, 12, 256, 256]   \n",
            "226_bert.encoder.layer.7.attention.output.Linea...      [8, 256, 768]   \n",
            "227_bert.encoder.layer.7.attention.output.Dropo...      [8, 256, 768]   \n",
            "228_bert.encoder.layer.7.attention.output.BertL...      [8, 256, 768]   \n",
            "229_bert.encoder.layer.7.intermediate.Linear_dense     [8, 256, 3072]   \n",
            "230_bert.encoder.layer.7.output.Linear_dense            [8, 256, 768]   \n",
            "231_bert.encoder.layer.7.output.Dropout_dropout         [8, 256, 768]   \n",
            "232_bert.encoder.layer.7.output.BertLayerNorm_L...      [8, 256, 768]   \n",
            "233_bert.encoder.layer.8.attention.self.Linear_...      [8, 256, 768]   \n",
            "234_bert.encoder.layer.8.attention.self.Linear_key      [8, 256, 768]   \n",
            "235_bert.encoder.layer.8.attention.self.Linear_...      [8, 256, 768]   \n",
            "236_bert.encoder.layer.8.attention.self.Dropout...  [8, 12, 256, 256]   \n",
            "237_bert.encoder.layer.8.attention.output.Linea...      [8, 256, 768]   \n",
            "238_bert.encoder.layer.8.attention.output.Dropo...      [8, 256, 768]   \n",
            "239_bert.encoder.layer.8.attention.output.BertL...      [8, 256, 768]   \n",
            "240_bert.encoder.layer.8.intermediate.Linear_dense     [8, 256, 3072]   \n",
            "241_bert.encoder.layer.8.output.Linear_dense            [8, 256, 768]   \n",
            "242_bert.encoder.layer.8.output.Dropout_dropout         [8, 256, 768]   \n",
            "243_bert.encoder.layer.8.output.BertLayerNorm_L...      [8, 256, 768]   \n",
            "244_bert.encoder.layer.9.attention.self.Linear_...      [8, 256, 768]   \n",
            "245_bert.encoder.layer.9.attention.self.Linear_key      [8, 256, 768]   \n",
            "246_bert.encoder.layer.9.attention.self.Linear_...      [8, 256, 768]   \n",
            "247_bert.encoder.layer.9.attention.self.Dropout...  [8, 12, 256, 256]   \n",
            "248_bert.encoder.layer.9.attention.output.Linea...      [8, 256, 768]   \n",
            "249_bert.encoder.layer.9.attention.output.Dropo...      [8, 256, 768]   \n",
            "250_bert.encoder.layer.9.attention.output.BertL...      [8, 256, 768]   \n",
            "251_bert.encoder.layer.9.intermediate.Linear_dense     [8, 256, 3072]   \n",
            "252_bert.encoder.layer.9.output.Linear_dense            [8, 256, 768]   \n",
            "253_bert.encoder.layer.9.output.Dropout_dropout         [8, 256, 768]   \n",
            "254_bert.encoder.layer.9.output.BertLayerNorm_L...      [8, 256, 768]   \n",
            "255_bert.encoder.layer.10.attention.self.Linear...      [8, 256, 768]   \n",
            "256_bert.encoder.layer.10.attention.self.Linear...      [8, 256, 768]   \n",
            "257_bert.encoder.layer.10.attention.self.Linear...      [8, 256, 768]   \n",
            "258_bert.encoder.layer.10.attention.self.Dropou...  [8, 12, 256, 256]   \n",
            "259_bert.encoder.layer.10.attention.output.Line...      [8, 256, 768]   \n",
            "260_bert.encoder.layer.10.attention.output.Drop...      [8, 256, 768]   \n",
            "261_bert.encoder.layer.10.attention.output.Bert...      [8, 256, 768]   \n",
            "262_bert.encoder.layer.10.intermediate.Linear_d...     [8, 256, 3072]   \n",
            "263_bert.encoder.layer.10.output.Linear_dense           [8, 256, 768]   \n",
            "264_bert.encoder.layer.10.output.Dropout_dropout        [8, 256, 768]   \n",
            "265_bert.encoder.layer.10.output.BertLayerNorm_...      [8, 256, 768]   \n",
            "266_bert.encoder.layer.11.attention.self.Linear...      [8, 256, 768]   \n",
            "267_bert.encoder.layer.11.attention.self.Linear...      [8, 256, 768]   \n",
            "268_bert.encoder.layer.11.attention.self.Linear...      [8, 256, 768]   \n",
            "269_bert.encoder.layer.11.attention.self.Dropou...  [8, 12, 256, 256]   \n",
            "270_bert.encoder.layer.11.attention.output.Line...      [8, 256, 768]   \n",
            "271_bert.encoder.layer.11.attention.output.Drop...      [8, 256, 768]   \n",
            "272_bert.encoder.layer.11.attention.output.Bert...      [8, 256, 768]   \n",
            "273_bert.encoder.layer.11.intermediate.Linear_d...     [8, 256, 3072]   \n",
            "274_bert.encoder.layer.11.output.Linear_dense           [8, 256, 768]   \n",
            "275_bert.encoder.layer.11.output.Dropout_dropout        [8, 256, 768]   \n",
            "276_bert.encoder.layer.11.output.BertLayerNorm_...      [8, 256, 768]   \n",
            "277_bert.pooler.Linear_dense                                 [8, 768]   \n",
            "278_bert.pooler.Tanh_activation                              [8, 768]   \n",
            "279_dropout                                                  [8, 768]   \n",
            "280_bert.embeddings.Embedding_word_embeddings            [8, 32, 768]   \n",
            "281_bert.embeddings.Embedding_position_embeddings        [8, 32, 768]   \n",
            "282_bert.embeddings.Embedding_token_type_embedd...       [8, 32, 768]   \n",
            "283_bert.embeddings.BertLayerNorm_LayerNorm              [8, 32, 768]   \n",
            "284_bert.embeddings.Dropout_dropout                      [8, 32, 768]   \n",
            "285_bert.encoder.layer.0.attention.self.Linear_...       [8, 32, 768]   \n",
            "286_bert.encoder.layer.0.attention.self.Linear_key       [8, 32, 768]   \n",
            "287_bert.encoder.layer.0.attention.self.Linear_...       [8, 32, 768]   \n",
            "288_bert.encoder.layer.0.attention.self.Dropout...    [8, 12, 32, 32]   \n",
            "289_bert.encoder.layer.0.attention.output.Linea...       [8, 32, 768]   \n",
            "290_bert.encoder.layer.0.attention.output.Dropo...       [8, 32, 768]   \n",
            "291_bert.encoder.layer.0.attention.output.BertL...       [8, 32, 768]   \n",
            "292_bert.encoder.layer.0.intermediate.Linear_dense      [8, 32, 3072]   \n",
            "293_bert.encoder.layer.0.output.Linear_dense             [8, 32, 768]   \n",
            "294_bert.encoder.layer.0.output.Dropout_dropout          [8, 32, 768]   \n",
            "295_bert.encoder.layer.0.output.BertLayerNorm_L...       [8, 32, 768]   \n",
            "296_bert.encoder.layer.1.attention.self.Linear_...       [8, 32, 768]   \n",
            "297_bert.encoder.layer.1.attention.self.Linear_key       [8, 32, 768]   \n",
            "298_bert.encoder.layer.1.attention.self.Linear_...       [8, 32, 768]   \n",
            "299_bert.encoder.layer.1.attention.self.Dropout...    [8, 12, 32, 32]   \n",
            "300_bert.encoder.layer.1.attention.output.Linea...       [8, 32, 768]   \n",
            "301_bert.encoder.layer.1.attention.output.Dropo...       [8, 32, 768]   \n",
            "302_bert.encoder.layer.1.attention.output.BertL...       [8, 32, 768]   \n",
            "303_bert.encoder.layer.1.intermediate.Linear_dense      [8, 32, 3072]   \n",
            "304_bert.encoder.layer.1.output.Linear_dense             [8, 32, 768]   \n",
            "305_bert.encoder.layer.1.output.Dropout_dropout          [8, 32, 768]   \n",
            "306_bert.encoder.layer.1.output.BertLayerNorm_L...       [8, 32, 768]   \n",
            "307_bert.encoder.layer.2.attention.self.Linear_...       [8, 32, 768]   \n",
            "308_bert.encoder.layer.2.attention.self.Linear_key       [8, 32, 768]   \n",
            "309_bert.encoder.layer.2.attention.self.Linear_...       [8, 32, 768]   \n",
            "310_bert.encoder.layer.2.attention.self.Dropout...    [8, 12, 32, 32]   \n",
            "311_bert.encoder.layer.2.attention.output.Linea...       [8, 32, 768]   \n",
            "312_bert.encoder.layer.2.attention.output.Dropo...       [8, 32, 768]   \n",
            "313_bert.encoder.layer.2.attention.output.BertL...       [8, 32, 768]   \n",
            "314_bert.encoder.layer.2.intermediate.Linear_dense      [8, 32, 3072]   \n",
            "315_bert.encoder.layer.2.output.Linear_dense             [8, 32, 768]   \n",
            "316_bert.encoder.layer.2.output.Dropout_dropout          [8, 32, 768]   \n",
            "317_bert.encoder.layer.2.output.BertLayerNorm_L...       [8, 32, 768]   \n",
            "318_bert.encoder.layer.3.attention.self.Linear_...       [8, 32, 768]   \n",
            "319_bert.encoder.layer.3.attention.self.Linear_key       [8, 32, 768]   \n",
            "320_bert.encoder.layer.3.attention.self.Linear_...       [8, 32, 768]   \n",
            "321_bert.encoder.layer.3.attention.self.Dropout...    [8, 12, 32, 32]   \n",
            "322_bert.encoder.layer.3.attention.output.Linea...       [8, 32, 768]   \n",
            "323_bert.encoder.layer.3.attention.output.Dropo...       [8, 32, 768]   \n",
            "324_bert.encoder.layer.3.attention.output.BertL...       [8, 32, 768]   \n",
            "325_bert.encoder.layer.3.intermediate.Linear_dense      [8, 32, 3072]   \n",
            "326_bert.encoder.layer.3.output.Linear_dense             [8, 32, 768]   \n",
            "327_bert.encoder.layer.3.output.Dropout_dropout          [8, 32, 768]   \n",
            "328_bert.encoder.layer.3.output.BertLayerNorm_L...       [8, 32, 768]   \n",
            "329_bert.encoder.layer.4.attention.self.Linear_...       [8, 32, 768]   \n",
            "330_bert.encoder.layer.4.attention.self.Linear_key       [8, 32, 768]   \n",
            "331_bert.encoder.layer.4.attention.self.Linear_...       [8, 32, 768]   \n",
            "332_bert.encoder.layer.4.attention.self.Dropout...    [8, 12, 32, 32]   \n",
            "333_bert.encoder.layer.4.attention.output.Linea...       [8, 32, 768]   \n",
            "334_bert.encoder.layer.4.attention.output.Dropo...       [8, 32, 768]   \n",
            "335_bert.encoder.layer.4.attention.output.BertL...       [8, 32, 768]   \n",
            "336_bert.encoder.layer.4.intermediate.Linear_dense      [8, 32, 3072]   \n",
            "337_bert.encoder.layer.4.output.Linear_dense             [8, 32, 768]   \n",
            "338_bert.encoder.layer.4.output.Dropout_dropout          [8, 32, 768]   \n",
            "339_bert.encoder.layer.4.output.BertLayerNorm_L...       [8, 32, 768]   \n",
            "340_bert.encoder.layer.5.attention.self.Linear_...       [8, 32, 768]   \n",
            "341_bert.encoder.layer.5.attention.self.Linear_key       [8, 32, 768]   \n",
            "342_bert.encoder.layer.5.attention.self.Linear_...       [8, 32, 768]   \n",
            "343_bert.encoder.layer.5.attention.self.Dropout...    [8, 12, 32, 32]   \n",
            "344_bert.encoder.layer.5.attention.output.Linea...       [8, 32, 768]   \n",
            "345_bert.encoder.layer.5.attention.output.Dropo...       [8, 32, 768]   \n",
            "346_bert.encoder.layer.5.attention.output.BertL...       [8, 32, 768]   \n",
            "347_bert.encoder.layer.5.intermediate.Linear_dense      [8, 32, 3072]   \n",
            "348_bert.encoder.layer.5.output.Linear_dense             [8, 32, 768]   \n",
            "349_bert.encoder.layer.5.output.Dropout_dropout          [8, 32, 768]   \n",
            "350_bert.encoder.layer.5.output.BertLayerNorm_L...       [8, 32, 768]   \n",
            "351_bert.encoder.layer.6.attention.self.Linear_...       [8, 32, 768]   \n",
            "352_bert.encoder.layer.6.attention.self.Linear_key       [8, 32, 768]   \n",
            "353_bert.encoder.layer.6.attention.self.Linear_...       [8, 32, 768]   \n",
            "354_bert.encoder.layer.6.attention.self.Dropout...    [8, 12, 32, 32]   \n",
            "355_bert.encoder.layer.6.attention.output.Linea...       [8, 32, 768]   \n",
            "356_bert.encoder.layer.6.attention.output.Dropo...       [8, 32, 768]   \n",
            "357_bert.encoder.layer.6.attention.output.BertL...       [8, 32, 768]   \n",
            "358_bert.encoder.layer.6.intermediate.Linear_dense      [8, 32, 3072]   \n",
            "359_bert.encoder.layer.6.output.Linear_dense             [8, 32, 768]   \n",
            "360_bert.encoder.layer.6.output.Dropout_dropout          [8, 32, 768]   \n",
            "361_bert.encoder.layer.6.output.BertLayerNorm_L...       [8, 32, 768]   \n",
            "362_bert.encoder.layer.7.attention.self.Linear_...       [8, 32, 768]   \n",
            "363_bert.encoder.layer.7.attention.self.Linear_key       [8, 32, 768]   \n",
            "364_bert.encoder.layer.7.attention.self.Linear_...       [8, 32, 768]   \n",
            "365_bert.encoder.layer.7.attention.self.Dropout...    [8, 12, 32, 32]   \n",
            "366_bert.encoder.layer.7.attention.output.Linea...       [8, 32, 768]   \n",
            "367_bert.encoder.layer.7.attention.output.Dropo...       [8, 32, 768]   \n",
            "368_bert.encoder.layer.7.attention.output.BertL...       [8, 32, 768]   \n",
            "369_bert.encoder.layer.7.intermediate.Linear_dense      [8, 32, 3072]   \n",
            "370_bert.encoder.layer.7.output.Linear_dense             [8, 32, 768]   \n",
            "371_bert.encoder.layer.7.output.Dropout_dropout          [8, 32, 768]   \n",
            "372_bert.encoder.layer.7.output.BertLayerNorm_L...       [8, 32, 768]   \n",
            "373_bert.encoder.layer.8.attention.self.Linear_...       [8, 32, 768]   \n",
            "374_bert.encoder.layer.8.attention.self.Linear_key       [8, 32, 768]   \n",
            "375_bert.encoder.layer.8.attention.self.Linear_...       [8, 32, 768]   \n",
            "376_bert.encoder.layer.8.attention.self.Dropout...    [8, 12, 32, 32]   \n",
            "377_bert.encoder.layer.8.attention.output.Linea...       [8, 32, 768]   \n",
            "378_bert.encoder.layer.8.attention.output.Dropo...       [8, 32, 768]   \n",
            "379_bert.encoder.layer.8.attention.output.BertL...       [8, 32, 768]   \n",
            "380_bert.encoder.layer.8.intermediate.Linear_dense      [8, 32, 3072]   \n",
            "381_bert.encoder.layer.8.output.Linear_dense             [8, 32, 768]   \n",
            "382_bert.encoder.layer.8.output.Dropout_dropout          [8, 32, 768]   \n",
            "383_bert.encoder.layer.8.output.BertLayerNorm_L...       [8, 32, 768]   \n",
            "384_bert.encoder.layer.9.attention.self.Linear_...       [8, 32, 768]   \n",
            "385_bert.encoder.layer.9.attention.self.Linear_key       [8, 32, 768]   \n",
            "386_bert.encoder.layer.9.attention.self.Linear_...       [8, 32, 768]   \n",
            "387_bert.encoder.layer.9.attention.self.Dropout...    [8, 12, 32, 32]   \n",
            "388_bert.encoder.layer.9.attention.output.Linea...       [8, 32, 768]   \n",
            "389_bert.encoder.layer.9.attention.output.Dropo...       [8, 32, 768]   \n",
            "390_bert.encoder.layer.9.attention.output.BertL...       [8, 32, 768]   \n",
            "391_bert.encoder.layer.9.intermediate.Linear_dense      [8, 32, 3072]   \n",
            "392_bert.encoder.layer.9.output.Linear_dense             [8, 32, 768]   \n",
            "393_bert.encoder.layer.9.output.Dropout_dropout          [8, 32, 768]   \n",
            "394_bert.encoder.layer.9.output.BertLayerNorm_L...       [8, 32, 768]   \n",
            "395_bert.encoder.layer.10.attention.self.Linear...       [8, 32, 768]   \n",
            "396_bert.encoder.layer.10.attention.self.Linear...       [8, 32, 768]   \n",
            "397_bert.encoder.layer.10.attention.self.Linear...       [8, 32, 768]   \n",
            "398_bert.encoder.layer.10.attention.self.Dropou...    [8, 12, 32, 32]   \n",
            "399_bert.encoder.layer.10.attention.output.Line...       [8, 32, 768]   \n",
            "400_bert.encoder.layer.10.attention.output.Drop...       [8, 32, 768]   \n",
            "401_bert.encoder.layer.10.attention.output.Bert...       [8, 32, 768]   \n",
            "402_bert.encoder.layer.10.intermediate.Linear_d...      [8, 32, 3072]   \n",
            "403_bert.encoder.layer.10.output.Linear_dense            [8, 32, 768]   \n",
            "404_bert.encoder.layer.10.output.Dropout_dropout         [8, 32, 768]   \n",
            "405_bert.encoder.layer.10.output.BertLayerNorm_...       [8, 32, 768]   \n",
            "406_bert.encoder.layer.11.attention.self.Linear...       [8, 32, 768]   \n",
            "407_bert.encoder.layer.11.attention.self.Linear...       [8, 32, 768]   \n",
            "408_bert.encoder.layer.11.attention.self.Linear...       [8, 32, 768]   \n",
            "409_bert.encoder.layer.11.attention.self.Dropou...    [8, 12, 32, 32]   \n",
            "410_bert.encoder.layer.11.attention.output.Line...       [8, 32, 768]   \n",
            "411_bert.encoder.layer.11.attention.output.Drop...       [8, 32, 768]   \n",
            "412_bert.encoder.layer.11.attention.output.Bert...       [8, 32, 768]   \n",
            "413_bert.encoder.layer.11.intermediate.Linear_d...      [8, 32, 3072]   \n",
            "414_bert.encoder.layer.11.output.Linear_dense            [8, 32, 768]   \n",
            "415_bert.encoder.layer.11.output.Dropout_dropout         [8, 32, 768]   \n",
            "416_bert.encoder.layer.11.output.BertLayerNorm_...       [8, 32, 768]   \n",
            "417_bert.pooler.Linear_dense                                 [8, 768]   \n",
            "418_bert.pooler.Tanh_activation                              [8, 768]   \n",
            "419_dropout                                                  [8, 768]   \n",
            "420_classifier                                                 [8, 2]   \n",
            "\n",
            "                                                        Params   Mult-Adds  \n",
            "Layer                                                                       \n",
            "0_bert.embeddings.Embedding_word_embeddings         23.440896M  23.440896M  \n",
            "1_bert.embeddings.Embedding_position_embeddings       393.216k    393.216k  \n",
            "2_bert.embeddings.Embedding_token_type_embeddings       1.536k      1.536k  \n",
            "3_bert.embeddings.BertLayerNorm_LayerNorm               1.536k       768.0  \n",
            "4_bert.embeddings.Dropout_dropout                            -           -  \n",
            "5_bert.encoder.layer.0.attention.self.Linear_query    590.592k    589.824k  \n",
            "6_bert.encoder.layer.0.attention.self.Linear_key      590.592k    589.824k  \n",
            "7_bert.encoder.layer.0.attention.self.Linear_value    590.592k    589.824k  \n",
            "8_bert.encoder.layer.0.attention.self.Dropout_d...           -           -  \n",
            "9_bert.encoder.layer.0.attention.output.Linear_...    590.592k    589.824k  \n",
            "10_bert.encoder.layer.0.attention.output.Dropou...           -           -  \n",
            "11_bert.encoder.layer.0.attention.output.BertLa...      1.536k       768.0  \n",
            "12_bert.encoder.layer.0.intermediate.Linear_dense    2.362368M   2.359296M  \n",
            "13_bert.encoder.layer.0.output.Linear_dense          2.360064M   2.359296M  \n",
            "14_bert.encoder.layer.0.output.Dropout_dropout               -           -  \n",
            "15_bert.encoder.layer.0.output.BertLayerNorm_La...      1.536k       768.0  \n",
            "16_bert.encoder.layer.1.attention.self.Linear_q...    590.592k    589.824k  \n",
            "17_bert.encoder.layer.1.attention.self.Linear_key     590.592k    589.824k  \n",
            "18_bert.encoder.layer.1.attention.self.Linear_v...    590.592k    589.824k  \n",
            "19_bert.encoder.layer.1.attention.self.Dropout_...           -           -  \n",
            "20_bert.encoder.layer.1.attention.output.Linear...    590.592k    589.824k  \n",
            "21_bert.encoder.layer.1.attention.output.Dropou...           -           -  \n",
            "22_bert.encoder.layer.1.attention.output.BertLa...      1.536k       768.0  \n",
            "23_bert.encoder.layer.1.intermediate.Linear_dense    2.362368M   2.359296M  \n",
            "24_bert.encoder.layer.1.output.Linear_dense          2.360064M   2.359296M  \n",
            "25_bert.encoder.layer.1.output.Dropout_dropout               -           -  \n",
            "26_bert.encoder.layer.1.output.BertLayerNorm_La...      1.536k       768.0  \n",
            "27_bert.encoder.layer.2.attention.self.Linear_q...    590.592k    589.824k  \n",
            "28_bert.encoder.layer.2.attention.self.Linear_key     590.592k    589.824k  \n",
            "29_bert.encoder.layer.2.attention.self.Linear_v...    590.592k    589.824k  \n",
            "30_bert.encoder.layer.2.attention.self.Dropout_...           -           -  \n",
            "31_bert.encoder.layer.2.attention.output.Linear...    590.592k    589.824k  \n",
            "32_bert.encoder.layer.2.attention.output.Dropou...           -           -  \n",
            "33_bert.encoder.layer.2.attention.output.BertLa...      1.536k       768.0  \n",
            "34_bert.encoder.layer.2.intermediate.Linear_dense    2.362368M   2.359296M  \n",
            "35_bert.encoder.layer.2.output.Linear_dense          2.360064M   2.359296M  \n",
            "36_bert.encoder.layer.2.output.Dropout_dropout               -           -  \n",
            "37_bert.encoder.layer.2.output.BertLayerNorm_La...      1.536k       768.0  \n",
            "38_bert.encoder.layer.3.attention.self.Linear_q...    590.592k    589.824k  \n",
            "39_bert.encoder.layer.3.attention.self.Linear_key     590.592k    589.824k  \n",
            "40_bert.encoder.layer.3.attention.self.Linear_v...    590.592k    589.824k  \n",
            "41_bert.encoder.layer.3.attention.self.Dropout_...           -           -  \n",
            "42_bert.encoder.layer.3.attention.output.Linear...    590.592k    589.824k  \n",
            "43_bert.encoder.layer.3.attention.output.Dropou...           -           -  \n",
            "44_bert.encoder.layer.3.attention.output.BertLa...      1.536k       768.0  \n",
            "45_bert.encoder.layer.3.intermediate.Linear_dense    2.362368M   2.359296M  \n",
            "46_bert.encoder.layer.3.output.Linear_dense          2.360064M   2.359296M  \n",
            "47_bert.encoder.layer.3.output.Dropout_dropout               -           -  \n",
            "48_bert.encoder.layer.3.output.BertLayerNorm_La...      1.536k       768.0  \n",
            "49_bert.encoder.layer.4.attention.self.Linear_q...    590.592k    589.824k  \n",
            "50_bert.encoder.layer.4.attention.self.Linear_key     590.592k    589.824k  \n",
            "51_bert.encoder.layer.4.attention.self.Linear_v...    590.592k    589.824k  \n",
            "52_bert.encoder.layer.4.attention.self.Dropout_...           -           -  \n",
            "53_bert.encoder.layer.4.attention.output.Linear...    590.592k    589.824k  \n",
            "54_bert.encoder.layer.4.attention.output.Dropou...           -           -  \n",
            "55_bert.encoder.layer.4.attention.output.BertLa...      1.536k       768.0  \n",
            "56_bert.encoder.layer.4.intermediate.Linear_dense    2.362368M   2.359296M  \n",
            "57_bert.encoder.layer.4.output.Linear_dense          2.360064M   2.359296M  \n",
            "58_bert.encoder.layer.4.output.Dropout_dropout               -           -  \n",
            "59_bert.encoder.layer.4.output.BertLayerNorm_La...      1.536k       768.0  \n",
            "60_bert.encoder.layer.5.attention.self.Linear_q...    590.592k    589.824k  \n",
            "61_bert.encoder.layer.5.attention.self.Linear_key     590.592k    589.824k  \n",
            "62_bert.encoder.layer.5.attention.self.Linear_v...    590.592k    589.824k  \n",
            "63_bert.encoder.layer.5.attention.self.Dropout_...           -           -  \n",
            "64_bert.encoder.layer.5.attention.output.Linear...    590.592k    589.824k  \n",
            "65_bert.encoder.layer.5.attention.output.Dropou...           -           -  \n",
            "66_bert.encoder.layer.5.attention.output.BertLa...      1.536k       768.0  \n",
            "67_bert.encoder.layer.5.intermediate.Linear_dense    2.362368M   2.359296M  \n",
            "68_bert.encoder.layer.5.output.Linear_dense          2.360064M   2.359296M  \n",
            "69_bert.encoder.layer.5.output.Dropout_dropout               -           -  \n",
            "70_bert.encoder.layer.5.output.BertLayerNorm_La...      1.536k       768.0  \n",
            "71_bert.encoder.layer.6.attention.self.Linear_q...    590.592k    589.824k  \n",
            "72_bert.encoder.layer.6.attention.self.Linear_key     590.592k    589.824k  \n",
            "73_bert.encoder.layer.6.attention.self.Linear_v...    590.592k    589.824k  \n",
            "74_bert.encoder.layer.6.attention.self.Dropout_...           -           -  \n",
            "75_bert.encoder.layer.6.attention.output.Linear...    590.592k    589.824k  \n",
            "76_bert.encoder.layer.6.attention.output.Dropou...           -           -  \n",
            "77_bert.encoder.layer.6.attention.output.BertLa...      1.536k       768.0  \n",
            "78_bert.encoder.layer.6.intermediate.Linear_dense    2.362368M   2.359296M  \n",
            "79_bert.encoder.layer.6.output.Linear_dense          2.360064M   2.359296M  \n",
            "80_bert.encoder.layer.6.output.Dropout_dropout               -           -  \n",
            "81_bert.encoder.layer.6.output.BertLayerNorm_La...      1.536k       768.0  \n",
            "82_bert.encoder.layer.7.attention.self.Linear_q...    590.592k    589.824k  \n",
            "83_bert.encoder.layer.7.attention.self.Linear_key     590.592k    589.824k  \n",
            "84_bert.encoder.layer.7.attention.self.Linear_v...    590.592k    589.824k  \n",
            "85_bert.encoder.layer.7.attention.self.Dropout_...           -           -  \n",
            "86_bert.encoder.layer.7.attention.output.Linear...    590.592k    589.824k  \n",
            "87_bert.encoder.layer.7.attention.output.Dropou...           -           -  \n",
            "88_bert.encoder.layer.7.attention.output.BertLa...      1.536k       768.0  \n",
            "89_bert.encoder.layer.7.intermediate.Linear_dense    2.362368M   2.359296M  \n",
            "90_bert.encoder.layer.7.output.Linear_dense          2.360064M   2.359296M  \n",
            "91_bert.encoder.layer.7.output.Dropout_dropout               -           -  \n",
            "92_bert.encoder.layer.7.output.BertLayerNorm_La...      1.536k       768.0  \n",
            "93_bert.encoder.layer.8.attention.self.Linear_q...    590.592k    589.824k  \n",
            "94_bert.encoder.layer.8.attention.self.Linear_key     590.592k    589.824k  \n",
            "95_bert.encoder.layer.8.attention.self.Linear_v...    590.592k    589.824k  \n",
            "96_bert.encoder.layer.8.attention.self.Dropout_...           -           -  \n",
            "97_bert.encoder.layer.8.attention.output.Linear...    590.592k    589.824k  \n",
            "98_bert.encoder.layer.8.attention.output.Dropou...           -           -  \n",
            "99_bert.encoder.layer.8.attention.output.BertLa...      1.536k       768.0  \n",
            "100_bert.encoder.layer.8.intermediate.Linear_dense   2.362368M   2.359296M  \n",
            "101_bert.encoder.layer.8.output.Linear_dense         2.360064M   2.359296M  \n",
            "102_bert.encoder.layer.8.output.Dropout_dropout              -           -  \n",
            "103_bert.encoder.layer.8.output.BertLayerNorm_L...      1.536k       768.0  \n",
            "104_bert.encoder.layer.9.attention.self.Linear_...    590.592k    589.824k  \n",
            "105_bert.encoder.layer.9.attention.self.Linear_key    590.592k    589.824k  \n",
            "106_bert.encoder.layer.9.attention.self.Linear_...    590.592k    589.824k  \n",
            "107_bert.encoder.layer.9.attention.self.Dropout...           -           -  \n",
            "108_bert.encoder.layer.9.attention.output.Linea...    590.592k    589.824k  \n",
            "109_bert.encoder.layer.9.attention.output.Dropo...           -           -  \n",
            "110_bert.encoder.layer.9.attention.output.BertL...      1.536k       768.0  \n",
            "111_bert.encoder.layer.9.intermediate.Linear_dense   2.362368M   2.359296M  \n",
            "112_bert.encoder.layer.9.output.Linear_dense         2.360064M   2.359296M  \n",
            "113_bert.encoder.layer.9.output.Dropout_dropout              -           -  \n",
            "114_bert.encoder.layer.9.output.BertLayerNorm_L...      1.536k       768.0  \n",
            "115_bert.encoder.layer.10.attention.self.Linear...    590.592k    589.824k  \n",
            "116_bert.encoder.layer.10.attention.self.Linear...    590.592k    589.824k  \n",
            "117_bert.encoder.layer.10.attention.self.Linear...    590.592k    589.824k  \n",
            "118_bert.encoder.layer.10.attention.self.Dropou...           -           -  \n",
            "119_bert.encoder.layer.10.attention.output.Line...    590.592k    589.824k  \n",
            "120_bert.encoder.layer.10.attention.output.Drop...           -           -  \n",
            "121_bert.encoder.layer.10.attention.output.Bert...      1.536k       768.0  \n",
            "122_bert.encoder.layer.10.intermediate.Linear_d...   2.362368M   2.359296M  \n",
            "123_bert.encoder.layer.10.output.Linear_dense        2.360064M   2.359296M  \n",
            "124_bert.encoder.layer.10.output.Dropout_dropout             -           -  \n",
            "125_bert.encoder.layer.10.output.BertLayerNorm_...      1.536k       768.0  \n",
            "126_bert.encoder.layer.11.attention.self.Linear...    590.592k    589.824k  \n",
            "127_bert.encoder.layer.11.attention.self.Linear...    590.592k    589.824k  \n",
            "128_bert.encoder.layer.11.attention.self.Linear...    590.592k    589.824k  \n",
            "129_bert.encoder.layer.11.attention.self.Dropou...           -           -  \n",
            "130_bert.encoder.layer.11.attention.output.Line...    590.592k    589.824k  \n",
            "131_bert.encoder.layer.11.attention.output.Drop...           -           -  \n",
            "132_bert.encoder.layer.11.attention.output.Bert...      1.536k       768.0  \n",
            "133_bert.encoder.layer.11.intermediate.Linear_d...   2.362368M   2.359296M  \n",
            "134_bert.encoder.layer.11.output.Linear_dense        2.360064M   2.359296M  \n",
            "135_bert.encoder.layer.11.output.Dropout_dropout             -           -  \n",
            "136_bert.encoder.layer.11.output.BertLayerNorm_...      1.536k       768.0  \n",
            "137_bert.pooler.Linear_dense                          590.592k    589.824k  \n",
            "138_bert.pooler.Tanh_activation                              -           -  \n",
            "139_dropout                                                  -           -  \n",
            "140_bert.embeddings.Embedding_word_embeddings                -  23.440896M  \n",
            "141_bert.embeddings.Embedding_position_embeddings            -    393.216k  \n",
            "142_bert.embeddings.Embedding_token_type_embedd...           -      1.536k  \n",
            "143_bert.embeddings.BertLayerNorm_LayerNorm                  -       768.0  \n",
            "144_bert.embeddings.Dropout_dropout                          -           -  \n",
            "145_bert.encoder.layer.0.attention.self.Linear_...           -    589.824k  \n",
            "146_bert.encoder.layer.0.attention.self.Linear_key           -    589.824k  \n",
            "147_bert.encoder.layer.0.attention.self.Linear_...           -    589.824k  \n",
            "148_bert.encoder.layer.0.attention.self.Dropout...           -           -  \n",
            "149_bert.encoder.layer.0.attention.output.Linea...           -    589.824k  \n",
            "150_bert.encoder.layer.0.attention.output.Dropo...           -           -  \n",
            "151_bert.encoder.layer.0.attention.output.BertL...           -       768.0  \n",
            "152_bert.encoder.layer.0.intermediate.Linear_dense           -   2.359296M  \n",
            "153_bert.encoder.layer.0.output.Linear_dense                 -   2.359296M  \n",
            "154_bert.encoder.layer.0.output.Dropout_dropout              -           -  \n",
            "155_bert.encoder.layer.0.output.BertLayerNorm_L...           -       768.0  \n",
            "156_bert.encoder.layer.1.attention.self.Linear_...           -    589.824k  \n",
            "157_bert.encoder.layer.1.attention.self.Linear_key           -    589.824k  \n",
            "158_bert.encoder.layer.1.attention.self.Linear_...           -    589.824k  \n",
            "159_bert.encoder.layer.1.attention.self.Dropout...           -           -  \n",
            "160_bert.encoder.layer.1.attention.output.Linea...           -    589.824k  \n",
            "161_bert.encoder.layer.1.attention.output.Dropo...           -           -  \n",
            "162_bert.encoder.layer.1.attention.output.BertL...           -       768.0  \n",
            "163_bert.encoder.layer.1.intermediate.Linear_dense           -   2.359296M  \n",
            "164_bert.encoder.layer.1.output.Linear_dense                 -   2.359296M  \n",
            "165_bert.encoder.layer.1.output.Dropout_dropout              -           -  \n",
            "166_bert.encoder.layer.1.output.BertLayerNorm_L...           -       768.0  \n",
            "167_bert.encoder.layer.2.attention.self.Linear_...           -    589.824k  \n",
            "168_bert.encoder.layer.2.attention.self.Linear_key           -    589.824k  \n",
            "169_bert.encoder.layer.2.attention.self.Linear_...           -    589.824k  \n",
            "170_bert.encoder.layer.2.attention.self.Dropout...           -           -  \n",
            "171_bert.encoder.layer.2.attention.output.Linea...           -    589.824k  \n",
            "172_bert.encoder.layer.2.attention.output.Dropo...           -           -  \n",
            "173_bert.encoder.layer.2.attention.output.BertL...           -       768.0  \n",
            "174_bert.encoder.layer.2.intermediate.Linear_dense           -   2.359296M  \n",
            "175_bert.encoder.layer.2.output.Linear_dense                 -   2.359296M  \n",
            "176_bert.encoder.layer.2.output.Dropout_dropout              -           -  \n",
            "177_bert.encoder.layer.2.output.BertLayerNorm_L...           -       768.0  \n",
            "178_bert.encoder.layer.3.attention.self.Linear_...           -    589.824k  \n",
            "179_bert.encoder.layer.3.attention.self.Linear_key           -    589.824k  \n",
            "180_bert.encoder.layer.3.attention.self.Linear_...           -    589.824k  \n",
            "181_bert.encoder.layer.3.attention.self.Dropout...           -           -  \n",
            "182_bert.encoder.layer.3.attention.output.Linea...           -    589.824k  \n",
            "183_bert.encoder.layer.3.attention.output.Dropo...           -           -  \n",
            "184_bert.encoder.layer.3.attention.output.BertL...           -       768.0  \n",
            "185_bert.encoder.layer.3.intermediate.Linear_dense           -   2.359296M  \n",
            "186_bert.encoder.layer.3.output.Linear_dense                 -   2.359296M  \n",
            "187_bert.encoder.layer.3.output.Dropout_dropout              -           -  \n",
            "188_bert.encoder.layer.3.output.BertLayerNorm_L...           -       768.0  \n",
            "189_bert.encoder.layer.4.attention.self.Linear_...           -    589.824k  \n",
            "190_bert.encoder.layer.4.attention.self.Linear_key           -    589.824k  \n",
            "191_bert.encoder.layer.4.attention.self.Linear_...           -    589.824k  \n",
            "192_bert.encoder.layer.4.attention.self.Dropout...           -           -  \n",
            "193_bert.encoder.layer.4.attention.output.Linea...           -    589.824k  \n",
            "194_bert.encoder.layer.4.attention.output.Dropo...           -           -  \n",
            "195_bert.encoder.layer.4.attention.output.BertL...           -       768.0  \n",
            "196_bert.encoder.layer.4.intermediate.Linear_dense           -   2.359296M  \n",
            "197_bert.encoder.layer.4.output.Linear_dense                 -   2.359296M  \n",
            "198_bert.encoder.layer.4.output.Dropout_dropout              -           -  \n",
            "199_bert.encoder.layer.4.output.BertLayerNorm_L...           -       768.0  \n",
            "200_bert.encoder.layer.5.attention.self.Linear_...           -    589.824k  \n",
            "201_bert.encoder.layer.5.attention.self.Linear_key           -    589.824k  \n",
            "202_bert.encoder.layer.5.attention.self.Linear_...           -    589.824k  \n",
            "203_bert.encoder.layer.5.attention.self.Dropout...           -           -  \n",
            "204_bert.encoder.layer.5.attention.output.Linea...           -    589.824k  \n",
            "205_bert.encoder.layer.5.attention.output.Dropo...           -           -  \n",
            "206_bert.encoder.layer.5.attention.output.BertL...           -       768.0  \n",
            "207_bert.encoder.layer.5.intermediate.Linear_dense           -   2.359296M  \n",
            "208_bert.encoder.layer.5.output.Linear_dense                 -   2.359296M  \n",
            "209_bert.encoder.layer.5.output.Dropout_dropout              -           -  \n",
            "210_bert.encoder.layer.5.output.BertLayerNorm_L...           -       768.0  \n",
            "211_bert.encoder.layer.6.attention.self.Linear_...           -    589.824k  \n",
            "212_bert.encoder.layer.6.attention.self.Linear_key           -    589.824k  \n",
            "213_bert.encoder.layer.6.attention.self.Linear_...           -    589.824k  \n",
            "214_bert.encoder.layer.6.attention.self.Dropout...           -           -  \n",
            "215_bert.encoder.layer.6.attention.output.Linea...           -    589.824k  \n",
            "216_bert.encoder.layer.6.attention.output.Dropo...           -           -  \n",
            "217_bert.encoder.layer.6.attention.output.BertL...           -       768.0  \n",
            "218_bert.encoder.layer.6.intermediate.Linear_dense           -   2.359296M  \n",
            "219_bert.encoder.layer.6.output.Linear_dense                 -   2.359296M  \n",
            "220_bert.encoder.layer.6.output.Dropout_dropout              -           -  \n",
            "221_bert.encoder.layer.6.output.BertLayerNorm_L...           -       768.0  \n",
            "222_bert.encoder.layer.7.attention.self.Linear_...           -    589.824k  \n",
            "223_bert.encoder.layer.7.attention.self.Linear_key           -    589.824k  \n",
            "224_bert.encoder.layer.7.attention.self.Linear_...           -    589.824k  \n",
            "225_bert.encoder.layer.7.attention.self.Dropout...           -           -  \n",
            "226_bert.encoder.layer.7.attention.output.Linea...           -    589.824k  \n",
            "227_bert.encoder.layer.7.attention.output.Dropo...           -           -  \n",
            "228_bert.encoder.layer.7.attention.output.BertL...           -       768.0  \n",
            "229_bert.encoder.layer.7.intermediate.Linear_dense           -   2.359296M  \n",
            "230_bert.encoder.layer.7.output.Linear_dense                 -   2.359296M  \n",
            "231_bert.encoder.layer.7.output.Dropout_dropout              -           -  \n",
            "232_bert.encoder.layer.7.output.BertLayerNorm_L...           -       768.0  \n",
            "233_bert.encoder.layer.8.attention.self.Linear_...           -    589.824k  \n",
            "234_bert.encoder.layer.8.attention.self.Linear_key           -    589.824k  \n",
            "235_bert.encoder.layer.8.attention.self.Linear_...           -    589.824k  \n",
            "236_bert.encoder.layer.8.attention.self.Dropout...           -           -  \n",
            "237_bert.encoder.layer.8.attention.output.Linea...           -    589.824k  \n",
            "238_bert.encoder.layer.8.attention.output.Dropo...           -           -  \n",
            "239_bert.encoder.layer.8.attention.output.BertL...           -       768.0  \n",
            "240_bert.encoder.layer.8.intermediate.Linear_dense           -   2.359296M  \n",
            "241_bert.encoder.layer.8.output.Linear_dense                 -   2.359296M  \n",
            "242_bert.encoder.layer.8.output.Dropout_dropout              -           -  \n",
            "243_bert.encoder.layer.8.output.BertLayerNorm_L...           -       768.0  \n",
            "244_bert.encoder.layer.9.attention.self.Linear_...           -    589.824k  \n",
            "245_bert.encoder.layer.9.attention.self.Linear_key           -    589.824k  \n",
            "246_bert.encoder.layer.9.attention.self.Linear_...           -    589.824k  \n",
            "247_bert.encoder.layer.9.attention.self.Dropout...           -           -  \n",
            "248_bert.encoder.layer.9.attention.output.Linea...           -    589.824k  \n",
            "249_bert.encoder.layer.9.attention.output.Dropo...           -           -  \n",
            "250_bert.encoder.layer.9.attention.output.BertL...           -       768.0  \n",
            "251_bert.encoder.layer.9.intermediate.Linear_dense           -   2.359296M  \n",
            "252_bert.encoder.layer.9.output.Linear_dense                 -   2.359296M  \n",
            "253_bert.encoder.layer.9.output.Dropout_dropout              -           -  \n",
            "254_bert.encoder.layer.9.output.BertLayerNorm_L...           -       768.0  \n",
            "255_bert.encoder.layer.10.attention.self.Linear...           -    589.824k  \n",
            "256_bert.encoder.layer.10.attention.self.Linear...           -    589.824k  \n",
            "257_bert.encoder.layer.10.attention.self.Linear...           -    589.824k  \n",
            "258_bert.encoder.layer.10.attention.self.Dropou...           -           -  \n",
            "259_bert.encoder.layer.10.attention.output.Line...           -    589.824k  \n",
            "260_bert.encoder.layer.10.attention.output.Drop...           -           -  \n",
            "261_bert.encoder.layer.10.attention.output.Bert...           -       768.0  \n",
            "262_bert.encoder.layer.10.intermediate.Linear_d...           -   2.359296M  \n",
            "263_bert.encoder.layer.10.output.Linear_dense                -   2.359296M  \n",
            "264_bert.encoder.layer.10.output.Dropout_dropout             -           -  \n",
            "265_bert.encoder.layer.10.output.BertLayerNorm_...           -       768.0  \n",
            "266_bert.encoder.layer.11.attention.self.Linear...           -    589.824k  \n",
            "267_bert.encoder.layer.11.attention.self.Linear...           -    589.824k  \n",
            "268_bert.encoder.layer.11.attention.self.Linear...           -    589.824k  \n",
            "269_bert.encoder.layer.11.attention.self.Dropou...           -           -  \n",
            "270_bert.encoder.layer.11.attention.output.Line...           -    589.824k  \n",
            "271_bert.encoder.layer.11.attention.output.Drop...           -           -  \n",
            "272_bert.encoder.layer.11.attention.output.Bert...           -       768.0  \n",
            "273_bert.encoder.layer.11.intermediate.Linear_d...           -   2.359296M  \n",
            "274_bert.encoder.layer.11.output.Linear_dense                -   2.359296M  \n",
            "275_bert.encoder.layer.11.output.Dropout_dropout             -           -  \n",
            "276_bert.encoder.layer.11.output.BertLayerNorm_...           -       768.0  \n",
            "277_bert.pooler.Linear_dense                                 -    589.824k  \n",
            "278_bert.pooler.Tanh_activation                              -           -  \n",
            "279_dropout                                                  -           -  \n",
            "280_bert.embeddings.Embedding_word_embeddings                -  23.440896M  \n",
            "281_bert.embeddings.Embedding_position_embeddings            -    393.216k  \n",
            "282_bert.embeddings.Embedding_token_type_embedd...           -      1.536k  \n",
            "283_bert.embeddings.BertLayerNorm_LayerNorm                  -       768.0  \n",
            "284_bert.embeddings.Dropout_dropout                          -           -  \n",
            "285_bert.encoder.layer.0.attention.self.Linear_...           -    589.824k  \n",
            "286_bert.encoder.layer.0.attention.self.Linear_key           -    589.824k  \n",
            "287_bert.encoder.layer.0.attention.self.Linear_...           -    589.824k  \n",
            "288_bert.encoder.layer.0.attention.self.Dropout...           -           -  \n",
            "289_bert.encoder.layer.0.attention.output.Linea...           -    589.824k  \n",
            "290_bert.encoder.layer.0.attention.output.Dropo...           -           -  \n",
            "291_bert.encoder.layer.0.attention.output.BertL...           -       768.0  \n",
            "292_bert.encoder.layer.0.intermediate.Linear_dense           -   2.359296M  \n",
            "293_bert.encoder.layer.0.output.Linear_dense                 -   2.359296M  \n",
            "294_bert.encoder.layer.0.output.Dropout_dropout              -           -  \n",
            "295_bert.encoder.layer.0.output.BertLayerNorm_L...           -       768.0  \n",
            "296_bert.encoder.layer.1.attention.self.Linear_...           -    589.824k  \n",
            "297_bert.encoder.layer.1.attention.self.Linear_key           -    589.824k  \n",
            "298_bert.encoder.layer.1.attention.self.Linear_...           -    589.824k  \n",
            "299_bert.encoder.layer.1.attention.self.Dropout...           -           -  \n",
            "300_bert.encoder.layer.1.attention.output.Linea...           -    589.824k  \n",
            "301_bert.encoder.layer.1.attention.output.Dropo...           -           -  \n",
            "302_bert.encoder.layer.1.attention.output.BertL...           -       768.0  \n",
            "303_bert.encoder.layer.1.intermediate.Linear_dense           -   2.359296M  \n",
            "304_bert.encoder.layer.1.output.Linear_dense                 -   2.359296M  \n",
            "305_bert.encoder.layer.1.output.Dropout_dropout              -           -  \n",
            "306_bert.encoder.layer.1.output.BertLayerNorm_L...           -       768.0  \n",
            "307_bert.encoder.layer.2.attention.self.Linear_...           -    589.824k  \n",
            "308_bert.encoder.layer.2.attention.self.Linear_key           -    589.824k  \n",
            "309_bert.encoder.layer.2.attention.self.Linear_...           -    589.824k  \n",
            "310_bert.encoder.layer.2.attention.self.Dropout...           -           -  \n",
            "311_bert.encoder.layer.2.attention.output.Linea...           -    589.824k  \n",
            "312_bert.encoder.layer.2.attention.output.Dropo...           -           -  \n",
            "313_bert.encoder.layer.2.attention.output.BertL...           -       768.0  \n",
            "314_bert.encoder.layer.2.intermediate.Linear_dense           -   2.359296M  \n",
            "315_bert.encoder.layer.2.output.Linear_dense                 -   2.359296M  \n",
            "316_bert.encoder.layer.2.output.Dropout_dropout              -           -  \n",
            "317_bert.encoder.layer.2.output.BertLayerNorm_L...           -       768.0  \n",
            "318_bert.encoder.layer.3.attention.self.Linear_...           -    589.824k  \n",
            "319_bert.encoder.layer.3.attention.self.Linear_key           -    589.824k  \n",
            "320_bert.encoder.layer.3.attention.self.Linear_...           -    589.824k  \n",
            "321_bert.encoder.layer.3.attention.self.Dropout...           -           -  \n",
            "322_bert.encoder.layer.3.attention.output.Linea...           -    589.824k  \n",
            "323_bert.encoder.layer.3.attention.output.Dropo...           -           -  \n",
            "324_bert.encoder.layer.3.attention.output.BertL...           -       768.0  \n",
            "325_bert.encoder.layer.3.intermediate.Linear_dense           -   2.359296M  \n",
            "326_bert.encoder.layer.3.output.Linear_dense                 -   2.359296M  \n",
            "327_bert.encoder.layer.3.output.Dropout_dropout              -           -  \n",
            "328_bert.encoder.layer.3.output.BertLayerNorm_L...           -       768.0  \n",
            "329_bert.encoder.layer.4.attention.self.Linear_...           -    589.824k  \n",
            "330_bert.encoder.layer.4.attention.self.Linear_key           -    589.824k  \n",
            "331_bert.encoder.layer.4.attention.self.Linear_...           -    589.824k  \n",
            "332_bert.encoder.layer.4.attention.self.Dropout...           -           -  \n",
            "333_bert.encoder.layer.4.attention.output.Linea...           -    589.824k  \n",
            "334_bert.encoder.layer.4.attention.output.Dropo...           -           -  \n",
            "335_bert.encoder.layer.4.attention.output.BertL...           -       768.0  \n",
            "336_bert.encoder.layer.4.intermediate.Linear_dense           -   2.359296M  \n",
            "337_bert.encoder.layer.4.output.Linear_dense                 -   2.359296M  \n",
            "338_bert.encoder.layer.4.output.Dropout_dropout              -           -  \n",
            "339_bert.encoder.layer.4.output.BertLayerNorm_L...           -       768.0  \n",
            "340_bert.encoder.layer.5.attention.self.Linear_...           -    589.824k  \n",
            "341_bert.encoder.layer.5.attention.self.Linear_key           -    589.824k  \n",
            "342_bert.encoder.layer.5.attention.self.Linear_...           -    589.824k  \n",
            "343_bert.encoder.layer.5.attention.self.Dropout...           -           -  \n",
            "344_bert.encoder.layer.5.attention.output.Linea...           -    589.824k  \n",
            "345_bert.encoder.layer.5.attention.output.Dropo...           -           -  \n",
            "346_bert.encoder.layer.5.attention.output.BertL...           -       768.0  \n",
            "347_bert.encoder.layer.5.intermediate.Linear_dense           -   2.359296M  \n",
            "348_bert.encoder.layer.5.output.Linear_dense                 -   2.359296M  \n",
            "349_bert.encoder.layer.5.output.Dropout_dropout              -           -  \n",
            "350_bert.encoder.layer.5.output.BertLayerNorm_L...           -       768.0  \n",
            "351_bert.encoder.layer.6.attention.self.Linear_...           -    589.824k  \n",
            "352_bert.encoder.layer.6.attention.self.Linear_key           -    589.824k  \n",
            "353_bert.encoder.layer.6.attention.self.Linear_...           -    589.824k  \n",
            "354_bert.encoder.layer.6.attention.self.Dropout...           -           -  \n",
            "355_bert.encoder.layer.6.attention.output.Linea...           -    589.824k  \n",
            "356_bert.encoder.layer.6.attention.output.Dropo...           -           -  \n",
            "357_bert.encoder.layer.6.attention.output.BertL...           -       768.0  \n",
            "358_bert.encoder.layer.6.intermediate.Linear_dense           -   2.359296M  \n",
            "359_bert.encoder.layer.6.output.Linear_dense                 -   2.359296M  \n",
            "360_bert.encoder.layer.6.output.Dropout_dropout              -           -  \n",
            "361_bert.encoder.layer.6.output.BertLayerNorm_L...           -       768.0  \n",
            "362_bert.encoder.layer.7.attention.self.Linear_...           -    589.824k  \n",
            "363_bert.encoder.layer.7.attention.self.Linear_key           -    589.824k  \n",
            "364_bert.encoder.layer.7.attention.self.Linear_...           -    589.824k  \n",
            "365_bert.encoder.layer.7.attention.self.Dropout...           -           -  \n",
            "366_bert.encoder.layer.7.attention.output.Linea...           -    589.824k  \n",
            "367_bert.encoder.layer.7.attention.output.Dropo...           -           -  \n",
            "368_bert.encoder.layer.7.attention.output.BertL...           -       768.0  \n",
            "369_bert.encoder.layer.7.intermediate.Linear_dense           -   2.359296M  \n",
            "370_bert.encoder.layer.7.output.Linear_dense                 -   2.359296M  \n",
            "371_bert.encoder.layer.7.output.Dropout_dropout              -           -  \n",
            "372_bert.encoder.layer.7.output.BertLayerNorm_L...           -       768.0  \n",
            "373_bert.encoder.layer.8.attention.self.Linear_...           -    589.824k  \n",
            "374_bert.encoder.layer.8.attention.self.Linear_key           -    589.824k  \n",
            "375_bert.encoder.layer.8.attention.self.Linear_...           -    589.824k  \n",
            "376_bert.encoder.layer.8.attention.self.Dropout...           -           -  \n",
            "377_bert.encoder.layer.8.attention.output.Linea...           -    589.824k  \n",
            "378_bert.encoder.layer.8.attention.output.Dropo...           -           -  \n",
            "379_bert.encoder.layer.8.attention.output.BertL...           -       768.0  \n",
            "380_bert.encoder.layer.8.intermediate.Linear_dense           -   2.359296M  \n",
            "381_bert.encoder.layer.8.output.Linear_dense                 -   2.359296M  \n",
            "382_bert.encoder.layer.8.output.Dropout_dropout              -           -  \n",
            "383_bert.encoder.layer.8.output.BertLayerNorm_L...           -       768.0  \n",
            "384_bert.encoder.layer.9.attention.self.Linear_...           -    589.824k  \n",
            "385_bert.encoder.layer.9.attention.self.Linear_key           -    589.824k  \n",
            "386_bert.encoder.layer.9.attention.self.Linear_...           -    589.824k  \n",
            "387_bert.encoder.layer.9.attention.self.Dropout...           -           -  \n",
            "388_bert.encoder.layer.9.attention.output.Linea...           -    589.824k  \n",
            "389_bert.encoder.layer.9.attention.output.Dropo...           -           -  \n",
            "390_bert.encoder.layer.9.attention.output.BertL...           -       768.0  \n",
            "391_bert.encoder.layer.9.intermediate.Linear_dense           -   2.359296M  \n",
            "392_bert.encoder.layer.9.output.Linear_dense                 -   2.359296M  \n",
            "393_bert.encoder.layer.9.output.Dropout_dropout              -           -  \n",
            "394_bert.encoder.layer.9.output.BertLayerNorm_L...           -       768.0  \n",
            "395_bert.encoder.layer.10.attention.self.Linear...           -    589.824k  \n",
            "396_bert.encoder.layer.10.attention.self.Linear...           -    589.824k  \n",
            "397_bert.encoder.layer.10.attention.self.Linear...           -    589.824k  \n",
            "398_bert.encoder.layer.10.attention.self.Dropou...           -           -  \n",
            "399_bert.encoder.layer.10.attention.output.Line...           -    589.824k  \n",
            "400_bert.encoder.layer.10.attention.output.Drop...           -           -  \n",
            "401_bert.encoder.layer.10.attention.output.Bert...           -       768.0  \n",
            "402_bert.encoder.layer.10.intermediate.Linear_d...           -   2.359296M  \n",
            "403_bert.encoder.layer.10.output.Linear_dense                -   2.359296M  \n",
            "404_bert.encoder.layer.10.output.Dropout_dropout             -           -  \n",
            "405_bert.encoder.layer.10.output.BertLayerNorm_...           -       768.0  \n",
            "406_bert.encoder.layer.11.attention.self.Linear...           -    589.824k  \n",
            "407_bert.encoder.layer.11.attention.self.Linear...           -    589.824k  \n",
            "408_bert.encoder.layer.11.attention.self.Linear...           -    589.824k  \n",
            "409_bert.encoder.layer.11.attention.self.Dropou...           -           -  \n",
            "410_bert.encoder.layer.11.attention.output.Line...           -    589.824k  \n",
            "411_bert.encoder.layer.11.attention.output.Drop...           -           -  \n",
            "412_bert.encoder.layer.11.attention.output.Bert...           -       768.0  \n",
            "413_bert.encoder.layer.11.intermediate.Linear_d...           -   2.359296M  \n",
            "414_bert.encoder.layer.11.output.Linear_dense                -   2.359296M  \n",
            "415_bert.encoder.layer.11.output.Dropout_dropout             -           -  \n",
            "416_bert.encoder.layer.11.output.BertLayerNorm_...           -       768.0  \n",
            "417_bert.pooler.Linear_dense                                 -    589.824k  \n",
            "418_bert.pooler.Tanh_activation                              -           -  \n",
            "419_dropout                                                  -           -  \n",
            "420_classifier                                           4.61k      4.608k  \n",
            "---------------------------------------------------------------------------------------------------------------------------\n",
            "                           Totals\n",
            "Total params           109.48685M\n",
            "Trainable params       109.48685M\n",
            "Non-trainable params          0.0\n",
            "Mult-Adds             328.142592M\n",
            "===========================================================================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ff81e70a-e5ca-4baf-b556-4b59330879f9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_bert.embeddings.Embedding_word_embeddings</th>\n",
              "      <td>[768, 30522]</td>\n",
              "      <td>[8, 64, 768]</td>\n",
              "      <td>23440896.0</td>\n",
              "      <td>23440896.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_bert.embeddings.Embedding_position_embeddings</th>\n",
              "      <td>[768, 512]</td>\n",
              "      <td>[8, 64, 768]</td>\n",
              "      <td>393216.0</td>\n",
              "      <td>393216.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_bert.embeddings.Embedding_token_type_embeddings</th>\n",
              "      <td>[768, 2]</td>\n",
              "      <td>[8, 64, 768]</td>\n",
              "      <td>1536.0</td>\n",
              "      <td>1536.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_bert.embeddings.BertLayerNorm_LayerNorm</th>\n",
              "      <td>[768]</td>\n",
              "      <td>[8, 64, 768]</td>\n",
              "      <td>1536.0</td>\n",
              "      <td>768.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_bert.embeddings.Dropout_dropout</th>\n",
              "      <td>-</td>\n",
              "      <td>[8, 64, 768]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>416_bert.encoder.layer.11.output.BertLayerNorm_LayerNorm</th>\n",
              "      <td>[768]</td>\n",
              "      <td>[8, 32, 768]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>768.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>417_bert.pooler.Linear_dense</th>\n",
              "      <td>[768, 768]</td>\n",
              "      <td>[8, 768]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>589824.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>418_bert.pooler.Tanh_activation</th>\n",
              "      <td>-</td>\n",
              "      <td>[8, 768]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>419_dropout</th>\n",
              "      <td>-</td>\n",
              "      <td>[8, 768]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420_classifier</th>\n",
              "      <td>[2304, 2]</td>\n",
              "      <td>[8, 2]</td>\n",
              "      <td>4610.0</td>\n",
              "      <td>4608.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>421 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ff81e70a-e5ca-4baf-b556-4b59330879f9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ff81e70a-e5ca-4baf-b556-4b59330879f9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ff81e70a-e5ca-4baf-b556-4b59330879f9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                    Kernel Shape  \\\n",
              "Layer                                                              \n",
              "0_bert.embeddings.Embedding_word_embeddings         [768, 30522]   \n",
              "1_bert.embeddings.Embedding_position_embeddings       [768, 512]   \n",
              "2_bert.embeddings.Embedding_token_type_embeddings       [768, 2]   \n",
              "3_bert.embeddings.BertLayerNorm_LayerNorm                  [768]   \n",
              "4_bert.embeddings.Dropout_dropout                              -   \n",
              "...                                                          ...   \n",
              "416_bert.encoder.layer.11.output.BertLayerNorm_...         [768]   \n",
              "417_bert.pooler.Linear_dense                          [768, 768]   \n",
              "418_bert.pooler.Tanh_activation                                -   \n",
              "419_dropout                                                    -   \n",
              "420_classifier                                         [2304, 2]   \n",
              "\n",
              "                                                    Output Shape      Params  \\\n",
              "Layer                                                                          \n",
              "0_bert.embeddings.Embedding_word_embeddings         [8, 64, 768]  23440896.0   \n",
              "1_bert.embeddings.Embedding_position_embeddings     [8, 64, 768]    393216.0   \n",
              "2_bert.embeddings.Embedding_token_type_embeddings   [8, 64, 768]      1536.0   \n",
              "3_bert.embeddings.BertLayerNorm_LayerNorm           [8, 64, 768]      1536.0   \n",
              "4_bert.embeddings.Dropout_dropout                   [8, 64, 768]         NaN   \n",
              "...                                                          ...         ...   \n",
              "416_bert.encoder.layer.11.output.BertLayerNorm_...  [8, 32, 768]         NaN   \n",
              "417_bert.pooler.Linear_dense                            [8, 768]         NaN   \n",
              "418_bert.pooler.Tanh_activation                         [8, 768]         NaN   \n",
              "419_dropout                                             [8, 768]         NaN   \n",
              "420_classifier                                            [8, 2]      4610.0   \n",
              "\n",
              "                                                     Mult-Adds  \n",
              "Layer                                                           \n",
              "0_bert.embeddings.Embedding_word_embeddings         23440896.0  \n",
              "1_bert.embeddings.Embedding_position_embeddings       393216.0  \n",
              "2_bert.embeddings.Embedding_token_type_embeddings       1536.0  \n",
              "3_bert.embeddings.BertLayerNorm_LayerNorm                768.0  \n",
              "4_bert.embeddings.Dropout_dropout                          NaN  \n",
              "...                                                        ...  \n",
              "416_bert.encoder.layer.11.output.BertLayerNorm_...       768.0  \n",
              "417_bert.pooler.Linear_dense                          589824.0  \n",
              "418_bert.pooler.Tanh_activation                            NaN  \n",
              "419_dropout                                                NaN  \n",
              "420_classifier                                          4608.0  \n",
              "\n",
              "[421 rows x 4 columns]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(model,x,y,z,m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBwunYpyugFg"
      },
      "source": [
        "# Training Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "WyCA5fvyVkb0"
      },
      "outputs": [],
      "source": [
        "gc.collect() # These commands help you when you face CUDA OOM error\n",
        "torch.cuda.empty_cache()\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "GnTLL-5gMBrY"
      },
      "outputs": [],
      "source": [
        "train_acc = []\n",
        "val_acc = []\n",
        "train_loss = []\n",
        "val_loss = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "oiuoAWxgPo_p"
      },
      "outputs": [],
      "source": [
        "def train(model, criterion, optimizer, scheduler):\n",
        "    since = Time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 100\n",
        "    best_acc = 0\n",
        "\n",
        "    scheduler.step()\n",
        "    model.train()  \n",
        "    running_loss = 0.0\n",
        "    fakeness_corrects = 0\n",
        "    \n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5) \n",
        "    \n",
        "    for data in train_loader:\n",
        "        inputs = data[0:4]\n",
        "        fakeness = data[-1]\n",
        "\n",
        "        inputs1 = inputs[0] # News statement input\n",
        "        inputs2 = inputs[1] # Justification input\n",
        "        inputs3 = inputs[2] # Meta data input\n",
        "        inputs4 = inputs[3] # Credit scores input\n",
        "\n",
        "        inputs1 = inputs1.to(device)\n",
        "        inputs2 = inputs2.to(device)\n",
        "        inputs3 = inputs3.to(device)\n",
        "        inputs4 = inputs4.to(device)\n",
        "\n",
        "        fakeness = fakeness.to(device)\n",
        "\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        \n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(inputs1, inputs2, inputs3, inputs4)\n",
        "            outputs = F.softmax(outputs,dim=1)\n",
        "            loss = criterion(outputs, torch.max(fakeness.float(), 1)[1])\n",
        "\n",
        "        running_loss += loss.item() * inputs1.size(0)\n",
        "\n",
        "\n",
        "        fakeness_corrects += torch.sum(torch.max(outputs, 1)[1] == torch.max(fakeness, 1)[1])\n",
        "        batch_bar.set_postfix(\n",
        "            acc=\"{:.04f}%\".format(100 * fakeness_corrects / (config['batch_size']*(i + 1))),\n",
        "            loss=\"{:.04f}\".format(float(running_loss / (i + 1))),\n",
        "            num_correct=fakeness_corrects.item(),\n",
        "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "        \n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "\n",
        "        batch_bar.update()\n",
        "\n",
        "    batch_bar.close()\n",
        "    epoch_loss = running_loss / len(X_train)\n",
        "\n",
        "\n",
        "    fakeness_acc = fakeness_corrects.double() / len(X_train)\n",
        "\n",
        "    print('Train total loss: {:.4f} '.format(epoch_loss))\n",
        "    print('Train fakeness_acc: {:.4f}'.format(fakeness_acc))\n",
        "\n",
        "    # Saving training acc and loss for each epoch\n",
        "    fakeness_acc1 = fakeness_acc.data\n",
        "    fakeness_acc1 = fakeness_acc1.cpu()\n",
        "    fakeness_acc1 = fakeness_acc1.numpy()\n",
        "    train_acc.append(fakeness_acc1)\n",
        "\n",
        "    train_loss.append(epoch_loss)\n",
        "\n",
        "    return train_acc, train_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "fbpr1VaQT8O1"
      },
      "outputs": [],
      "source": [
        "def validation(model, criterion, optimizer, scheduler):\n",
        "    since = Time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 100\n",
        "    best_acc = 0\n",
        "\n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val', ncols=5)\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    fakeness_corrects = 0\n",
        "\n",
        "    # Iterate over data.\n",
        "    for data in val_loader:\n",
        "        inputs = data[0:4]\n",
        "        fakeness = data[-1]\n",
        "        inputs1 = inputs[0] # News statement input\n",
        "        inputs2 = inputs[1] # Justification input\n",
        "        inputs3 = inputs[2] # Meta data input\n",
        "        inputs4 = inputs[3] # Credit scores input\n",
        "\n",
        "        inputs1 = inputs1.to(device)\n",
        "        inputs2 = inputs2.to(device)\n",
        "        inputs3 = inputs3.to(device)\n",
        "        inputs4 = inputs4.to(device)\n",
        "\n",
        "        fakeness = fakeness.to(device)\n",
        "\n",
        "        # forward\n",
        "\n",
        "        outputs = model(inputs1, inputs2, inputs3, inputs4)\n",
        "\n",
        "        outputs = F.softmax(outputs,dim=1)\n",
        "\n",
        "        loss = criterion(outputs, torch.max(fakeness.float(), 1)[1])\n",
        "\n",
        "        running_loss += loss.item() * inputs1.size(0)\n",
        "\n",
        "\n",
        "        fakeness_corrects += torch.sum(torch.max(outputs, 1)[1] == torch.max(fakeness, 1)[1])\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            acc=\"{:.04f}%\".format(100 * fakeness_corrects / (config['batch_size']*(i + 1))),\n",
        "            loss=\"{:.04f}\".format(float(running_loss / (i + 1))),\n",
        "            num_correct=fakeness_corrects.item(),\n",
        "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "\n",
        "        batch_bar.update()\n",
        "\n",
        "    batch_bar.close()\n",
        "\n",
        "\n",
        "    epoch_loss = running_loss / len(X_val)\n",
        "    fakeness_acc = fakeness_corrects.double() / len(X_val)\n",
        "\n",
        "    print('Validation total loss: {:.4f} '.format(epoch_loss ))\n",
        "    print('Validation fakeness_acc: {:.4f}'.format(fakeness_acc))\n",
        "    best_acc = fakeness_acc\n",
        "\n",
        "    # Saving val acc and loss for each epoch\n",
        "    fakeness_acc1 = fakeness_acc.data\n",
        "    fakeness_acc1 = fakeness_acc1.cpu()\n",
        "    fakeness_acc1 = fakeness_acc1.numpy()\n",
        "    val_acc.append(fakeness_acc1)\n",
        "\n",
        "    val_loss.append(epoch_loss)\n",
        "\n",
        "    print('Best val Acc: {:4f}'.format(float(best_acc)))\n",
        "\n",
        "    return val_acc, val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "4s52yBOvICPZ",
        "outputId": "3617088c-1a8e-4518-adb2-40eae98826af"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mngaddam\u001b[0m (\u001b[33mfake-news\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221209_204852-3fbo8775</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/fake-news/claim-verification/runs/3fbo8775\" target=\"_blank\">test1</a></strong> to <a href=\"https://wandb.ai/fake-news/claim-verification\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# run = wandb.init(\n",
        "#     name = \"test1\", ## Wandb creates random run names if you skip this field\n",
        "#     reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "#     # run_id = ### Insert specific run id here if you want to resume a previous run\n",
        "#     # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "#     project = \"claim-verification\", ### Project should be created in your wandb account \n",
        "#     config = config, ### Wandb Config for your run,\n",
        "#     entity=\"fake-news\"\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fLLj5KIMMOe"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "0nqLiAmkMMBc"
      },
      "outputs": [],
      "source": [
        "lrlast = .0001\n",
        "lrmain = .00001\n",
        "optim1 = torch.optim.Adam(\n",
        "    [\n",
        "        {\"params\":model.bert.parameters(),\"lr\": lrmain},\n",
        "        {\"params\":model.classifier.parameters(), \"lr\": lrlast},\n",
        "\n",
        "   ])\n",
        "\n",
        "#optim1 = optim.Adam(model.parameters(), lr=0.001)#,momentum=.9)\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim1\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "'''import focal_loss\n",
        "loss_args = {\"alpha\": 0.5, \"gamma\": 2.0}\n",
        "criterion = focal_loss.FocalLoss(*loss_args)'''\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 3 epochs\n",
        "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_ft, step_size=3, gamma=0.1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpYExu4vT4_g"
      },
      "source": [
        "### Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MWUEh273d1Wz",
        "outputId": "fbca494c-b7c1-4d50-9652-397763ed456b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train total loss: 0.5769 \n",
            "Train fakeness_acc: 0.7174\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation total loss: 0.5755 \n",
            "Validation fakeness_acc: 0.7111\n",
            "Best val Acc: 0.711059\n",
            "Saving model...\n",
            "\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train total loss: 0.5628 \n",
            "Train fakeness_acc: 0.7313\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation total loss: 0.5733 \n",
            "Validation fakeness_acc: 0.7142\n",
            "Best val Acc: 0.714174\n",
            "Saving model...\n",
            "\n",
            "Epoch 3/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train total loss: 0.5582 \n",
            "Train fakeness_acc: 0.7329\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation total loss: 0.5692 \n",
            "Validation fakeness_acc: 0.7173\n",
            "Best val Acc: 0.717290\n",
            "Saving model...\n",
            "\n",
            "Epoch 4/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train total loss: 0.5526 \n",
            "Train fakeness_acc: 0.7419\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation total loss: 0.5633 \n",
            "Validation fakeness_acc: 0.7196\n",
            "Best val Acc: 0.719626\n",
            "Saving model...\n",
            "\n",
            "Epoch 5/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train total loss: 0.5528 \n",
            "Train fakeness_acc: 0.7411\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation total loss: 0.5635 \n",
            "Validation fakeness_acc: 0.7188\n",
            "Best val Acc: 0.718847\n",
            "\n",
            "Epoch 6/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train total loss: 0.5513 \n",
            "Train fakeness_acc: 0.7448\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation total loss: 0.5634 \n",
            "Validation fakeness_acc: 0.7188\n",
            "Best val Acc: 0.718847\n",
            "\n",
            "Epoch 7/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train total loss: 0.5515 \n",
            "Train fakeness_acc: 0.7437\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation total loss: 0.5634 \n",
            "Validation fakeness_acc: 0.7188\n",
            "Best val Acc: 0.718847\n",
            "\n",
            "Epoch 8/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train total loss: 0.5515 \n",
            "Train fakeness_acc: 0.7457\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation total loss: 0.5634 \n",
            "Validation fakeness_acc: 0.7188\n",
            "Best val Acc: 0.718847\n",
            "\n",
            "Epoch 9/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train total loss: 0.5515 \n",
            "Train fakeness_acc: 0.7466\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation total loss: 0.5634 \n",
            "Validation fakeness_acc: 0.7188\n",
            "Best val Acc: 0.718847\n",
            "\n",
            "Epoch 10/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train total loss: 0.5521 \n",
            "Train fakeness_acc: 0.7475\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation total loss: 0.5634 \n",
            "Validation fakeness_acc: 0.7188\n",
            "Best val Acc: 0.718847\n",
            "\n",
            "Epoch 11/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train:  38%|███▊      | 484/1280 [02:23<03:54,  3.40it/s, acc=27.7230%, loss=1.7280, lr=0.0000, num_correct=2810]"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-8d1d3e62798e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#t0 = time.time()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-b61ecf0f90da>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;31m# optimizer.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Iterate over number of epochs to train and evaluate your model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "best_acc = 0.0 ### Monitor best accuracy in your run\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "    print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n",
        "    #t0 = time.time()\n",
        "\n",
        "    train_acc, train_loss = train(model, criterion, optimizer_ft, exp_lr_scheduler)\n",
        "    accuracy, val_loss = validation(model, criterion, optimizer_ft, exp_lr_scheduler)\n",
        "\n",
        "\n",
        "    # wandb.log({\"train loss\": float(train_loss[-1]), \"validation accuracy\": float(accuracy[-1]), \"val_loss\": float(val_loss[-1]), \"train acc\":float(train_acc[-1])})\n",
        "    #scheduler.step(accuracy) #ReduceLRonPlateau\n",
        "    exp_lr_scheduler.step() #StepLR\n",
        "\n",
        "    ### Log metrics at each epoch in your run - Optionally, you can log at each batch inside train/eval functions (explore wandb documentation/wandb recitation)\n",
        "\n",
        "\n",
        "    ### Save checkpoint at each epoch\n",
        "    ### Save checkpoint with information you want\n",
        "    if float(accuracy[-1])>best_acc:\n",
        "        best_acc = float(accuracy[-1])\n",
        "        print('Saving model...')\n",
        "        torch.save({'epoch': epoch,\n",
        "                  'model_state_dict': model.state_dict(),\n",
        "                  'optimizer_state_dict': optimizer_ft.state_dict(),\n",
        "                  'loss': train_loss,\n",
        "                  'acc': accuracy}, \n",
        "            './model_checkpoint.pth')\n",
        "          \n",
        "          ### Save checkpoint in wandb\n",
        "        # wandb.save('checkpoint.pth')\n",
        "\n",
        "    # Is your training time very high? Look into mixed precision training if your GPU (Tesla T4, V100, etc) can make use of it \n",
        "    # Refer - https://pytorch.org/docs/stable/notes/amp_examples.html\n",
        "    #print('Duration:',time.time() - t0)\n",
        "### Finish your wandb run\n",
        "# run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZox-KDDmqDz",
        "outputId": "386b5d3d-699c-45df-9a00-2b3e192e57d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 64]) torch.Size([8, 256]) torch.Size([8, 32])\n",
            "tensor([[ 2096,  1023,  1010,  2199,  2110,  5126,  2020,  2794,  2000,  1996,\n",
            "          2163, 26854,  1010,  5392,  2015,  6599, 19939,  2015,  3333,  2011,\n",
            "          2062,  2084,  1002,  1018,  4551,  1012,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0],\n",
            "        [ 2048,  2086,  3283, 11293,  2894,  2985,  1002,  2753,  1010,  2199,\n",
            "          1037,  2095,  2025, 11787,  1996,  2082,  2533,  2055,  3901,  1999,\n",
            "          1996,  2163,  3348, 25042, 15584,  1012,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0],\n",
            "        [ 1996,  2142,  2163,  2003,  2012,  3181,  2501, 26836,  1997,  3633,\n",
            "          2108, 10439,  2890, 22342,  2098,  2006,  1996,  3675,  2013,  3032,\n",
            "          2007,  9452,  7208,  2107,  2004,  4501,  2030,  7041,  2030,  7795,\n",
            "          1012,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0],\n",
            "        [ 4419,  2739,  2003,  7917,  1999,  2710,  2138,  2009, 23640,  3736,\n",
            "          2375,  2008, 28139, 15338,  2015,  2739,  6833,  2013,  4688,  2000,\n",
            "          2037,  7193,  1012,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0],\n",
            "        [ 2057,  5247,  1999,  4171,  7077, 19990,  6604,  1002,  1015,  1012,\n",
            "          1015, 23458,  1012,  2008,  2015,  2062,  2084,  2057,  5247,  2006,\n",
            "          2256,  3639,  5166,  1999,  1037,  2095,  1010,  2006, 27615,  2030,\n",
            "         19960,  5555,  3593,  1999,  1037,  2095,  1012,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0],\n",
            "        [ 2104,  8112, 16302,  1010,  1996, 25760,  2097,  2031,  3229,  2000,\n",
            "          1996,  2137,  7243,  5123,  2740,  2729,  2592,  1012,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0],\n",
            "        [ 2758,  2002,  2357,  2091,  1037, 19960,  5555,  3593,  4935,  2104,\n",
            "          8112, 16302,  1010,  2021,  2138,  1997,  4506,  2002,  2165,  1010,\n",
            "          2005,  1996,  2034,  2051,  1999,  5273,  2015,  2381,  3071,  2542,\n",
            "          1999,  5635,  2003,  3139,  2104, 19960,  5555,  3593,  1012,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0],\n",
            "        [ 2066,  2033,  1010,  3660,  5232,  4941,  1996,  2687,  5193,  3021,\n",
            "          1998,  1996,  1002,  1023,  4551,  1997,  5949,  3993,  5938,  1012,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0]])\n",
            "tensor([[ 2057,  3726,  2356,  ...,     0,     0,     0],\n",
            "        [ 1997, 24903, 10510,  ...,     0,     0,     0],\n",
            "        [ 6890,  2056,  1996,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [ 8112,  2003,  6149,  ...,     0,     0,     0],\n",
            "        [14455,  2056,  1000,  ...,     0,     0,     0],\n",
            "        [ 2009,  2036,  2089,  ...,     0,     0,     0]])\n",
            "tensor([[ 2110,  1011,  5166,  1010,  2110,  1011, 16156,  6877,  1011,  9482,\n",
            "          2110,  4387,  5392,  3951,  1037, 17178,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0],\n",
            "        [ 2336,  1010,  4126,  1010,  4735,  1011,  3425,  3533,  2063,  1011,\n",
            "         11409, 18939, 11012,  2569,  3353,  4905,  2236,  9763,  2479,  2110,\n",
            "          1011,  2880,  1037,  2160,  5446,  2837,  4994,     0,     0,     0,\n",
            "             0,     0],\n",
            "        [ 7041,  1010,  7521,  1010, 10130,  6174,  1011,  6890,  3099,  3146,\n",
            "          3951,  7928,  2006, 13229,  1005,  1055,  1000,  2110,  1997,  1996,\n",
            "          2586,  1000,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0],\n",
            "        [ 9615,  1010,  3097,  1011,  3343,  9130,  1011,  8466,  2591,  2865,\n",
            "         14739,  3904,  3904,  8466,  2006,  9130,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0],\n",
            "        [ 7773, 14537,  1011,  7890, 12129,  5205,  2047,  7035,  7672,  2019,\n",
            "          4357,  2007,  1996,  8368,  2604,  1997,  1005,  1996, 10013,  1012,\n",
            "          1005,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0],\n",
            "        [ 2740,  1011,  2729,  1010,  7773,  4388,  1011, 26519,  2160,  3484,\n",
            "          3003,  3448,  3951,  1037,  2723,  4613,  1012,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0],\n",
            "        [ 2740,  1011,  2729,  1010, 19960,  5555,  3593,  1010,  5635,  1010,\n",
            "          2270,  1011,  2740,  1010,  2110,  1011,  5166,  3660,  1011,  5232,\n",
            "          9184,  2221,  3237,  5273,  3951,  1037,  4613,     0,     0,     0,\n",
            "             0,     0],\n",
            "        [ 2976,  1011,  5166,  1010,  4471,  1011,  3698,  1010,  7773,  3958,\n",
            "          1011,  3168, 27698,  7389,  3678,  2266,  1997,  3519,  5273,  3951,\n",
            "          1037,  2557,  4748,  2005,  1996,  3660,  5232, 19100,  3049,     0,\n",
            "             0,     0]])\n",
            "tensor([[0.6429, 0.6429, 0.6429,  ..., 0.6429, 0.6429, 0.6429],\n",
            "        [0.5000, 0.5000, 0.5000,  ..., 0.5000, 0.5000, 0.5000],\n",
            "        [0.6510, 0.6510, 0.6510,  ..., 0.6510, 0.6510, 0.6510],\n",
            "        ...,\n",
            "        [0.7019, 0.7019, 0.7019,  ..., 0.7019, 0.7019, 0.7019],\n",
            "        [0.6093, 0.6093, 0.6093,  ..., 0.6093, 0.6093, 0.6093],\n",
            "        [0.8000, 0.8000, 0.8000,  ..., 0.8000, 0.8000, 0.8000]])\n",
            "tensor([[1, 0],\n",
            "        [1, 0],\n",
            "        [0, 1],\n",
            "        [0, 1],\n",
            "        [0, 1],\n",
            "        [0, 1],\n",
            "        [1, 0],\n",
            "        [0, 1]])\n",
            "5\n"
          ]
        }
      ],
      "source": [
        "# sanity check\n",
        "for data in test_loader:\n",
        "    x, y, z, m, n = data\n",
        "    print(x.shape, y.shape, z.shape)\n",
        "    print(x)\n",
        "    print(y)\n",
        "    print(z)\n",
        "    print(m)\n",
        "    print(n)\n",
        "    print(len(data))\n",
        "    x, y, z, m, n = x.to(device), y.to(device), z.to(device), m.to(device), n.to(device) \n",
        "    break "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "pbO4RiEhwl4z"
      },
      "outputs": [],
      "source": [
        "def test(model, dataLoader, criterion):\n",
        "\n",
        "    model.eval()   # Set model to evaluate mode\n",
        "    batch_bar = tqdm(total=len(dataLoader), dynamic_ncols=True, position=0, leave=False, desc='Test', ncols=5)\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    fakeness_corrects = 0\n",
        "\n",
        "    for data in dataLoader:\n",
        "        inputs = data[0:4]\n",
        "        fakeness = data[-1]\n",
        "        inputs1 = inputs[0] # News statement input\n",
        "        inputs2 = inputs[1] # Justification input\n",
        "        inputs3 = inputs[2] # Meta data input\n",
        "        inputs4 = inputs[3] # Credit scores input\n",
        "\n",
        "        inputs1 = inputs1.to(device)\n",
        "        inputs2 = inputs2.to(device)\n",
        "        inputs3 = inputs3.to(device)\n",
        "        inputs4 = inputs4.to(device)\n",
        "\n",
        "        fakeness = fakeness.to(device)\n",
        "\n",
        "        # forward\n",
        "        outputs = model(inputs1, inputs2, inputs3, inputs4)\n",
        "\n",
        "        outputs = F.softmax(outputs,dim=1)\n",
        "\n",
        "        loss = criterion(outputs, torch.max(fakeness.float(), 1)[1])\n",
        "\n",
        "        running_loss += loss.item() * inputs1.size(0)\n",
        "\n",
        "\n",
        "        fakeness_corrects += torch.sum(torch.max(outputs, 1)[1] == torch.max(fakeness, 1)[1])\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            acc=\"{:.04f}%\".format(100 * fakeness_corrects / (config['batch_size']*(i + 1))),\n",
        "            loss=\"{:.04f}\".format(float(running_loss / (i + 1))),\n",
        "            num_correct=fakeness_corrects.item())\n",
        "\n",
        "        batch_bar.update()\n",
        "\n",
        "    batch_bar.close()\n",
        "\n",
        "\n",
        "    epoch_loss = running_loss / len(X_val)\n",
        "    fakeness_acc = fakeness_corrects.double() / len(X_val)\n",
        "\n",
        "    print('Test total loss: {:.4f} '.format(epoch_loss ))\n",
        "    print('Test fakeness_acc: {:.4f}'.format(fakeness_acc))\n",
        "    best_acc = fakeness_acc\n",
        "\n",
        "    # Saving test acc and loss for each epoch\n",
        "    fakeness_acc1 = fakeness_acc.data\n",
        "    fakeness_acc1 = fakeness_acc1.cpu()\n",
        "    fakeness_acc1 = fakeness_acc1.numpy()\n",
        "    val_acc.append(fakeness_acc1)\n",
        "\n",
        "    val_loss.append(epoch_loss)\n",
        "\n",
        "    print('Best Test Acc: {:4f}'.format(float(best_acc)))\n",
        "\n",
        "    return val_acc, val_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhJvYBBWoyWm",
        "outputId": "e8f16292-fa6a-4ded-b1a9-63db184c6b85"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                                  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test total loss: 0.5414 \n",
            "Test fakeness_acc: 0.7375\n",
            "Best Test Acc: 0.737539\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        }
      ],
      "source": [
        "test_acc, test_loss = test(model,test_loader,criterion)\n",
        "# print(test_acc)\n",
        "# print(test_loss)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "fb4a7f07a05e69ccd70b8610395539af4ea9eff6155ae8b28a9ee451516f169c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
