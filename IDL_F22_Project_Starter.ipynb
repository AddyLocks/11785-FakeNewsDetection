{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR4qfYrVoO4v"
      },
      "source": [
        "# Installs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd5aNaLVoR_g"
      },
      "source": [
        "## wandb\n",
        "\n",
        "You will need to fetch your api key from wandb.ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mA9qZoIDcx-h"
      },
      "outputs": [],
      "source": [
        "!pip install wandb -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONgAWhqdoYy-"
      },
      "source": [
        "## Misc\n",
        "\n",
        "This may take a while"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS7a7xeEoaV9",
        "outputId": "caf83316-9864-491e-c4c3-38ad705a3b54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchsummaryX in /usr/local/lib/python3.7/dist-packages (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.21.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.12.1+cu113)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torchsummaryX) (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torchsummaryX) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torchsummaryX) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torchsummaryX) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchsummaryX) (4.1.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: slugify in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.12.1+cu113)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (2022.6.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch_pretrained_bert) (1.26.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (4.1.1)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (0.6.0)\n",
            "Requirement already satisfied: botocore<1.30.0,>=1.29.5 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch_pretrained_bert) (1.29.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.30.0,>=1.29.5->boto3->pytorch_pretrained_bert) (1.25.11)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.30.0,>=1.29.5->boto3->pytorch_pretrained_bert) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.5->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_pretrained_bert) (2022.9.24)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchsummaryX\n",
        "!pip install slugify\n",
        "!pip install pytorch_pretrained_bert"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWVONJxCobPc"
      },
      "source": [
        "## imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78ZTCIXoof2f",
        "outputId": "b1825b2c-3459-438a-bd75-4311cb334bac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import time as Time\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
        "from pytorch_pretrained_bert import BertConfig\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummaryX import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import datetime\n",
        "import wandb\n",
        "import copy\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9v5ewZDMpYA"
      },
      "source": [
        "# Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Cp-716IMZRd"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiS6pZ3Vveb0"
      },
      "outputs": [],
      "source": [
        "!mkdir '/content/glove'\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -qo 'glove.6B.zip' -d '/content/glove'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnIMttYa5EGc"
      },
      "source": [
        "# Setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NeEA5A_y5HCw"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"num_labels\" : 2, # True or False Classification\n",
        "    \"lr\" : 2e-3,\n",
        "    \"epochs\" : 50,\n",
        "    'batch_size' : 8,\n",
        "    #'LR scheduler': 'CosineAnnealingLR',\n",
        "    'LR scheduler': 'ReduceLROnPlateau',\n",
        "                'scheduler factor': 0.5,\n",
        "    'scheduler threshold': 0.01,\n",
        "            'scheduler patience': 5,\n",
        "   #'scheduler Tmax': 0,\n",
        "    } # Feel free to add more items here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n37yR-fl5Yxg"
      },
      "source": [
        "# Read Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gCf4pk9y5bH3"
      },
      "outputs": [],
      "source": [
        "# remeber to change the path if you are not on google colab and directly uploading\n",
        "train_path = '/content/train2.tsv'\n",
        "test_path = '/content/test2.tsv'\n",
        "val_path = '/content/val2.tsv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "P9zWt35f5pDj"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(train_path, sep=\"\\t\", header=None)\n",
        "test_df = pd.read_csv(test_path, sep=\"\\t\", header=None)\n",
        "val_df = pd.read_csv(val_path, sep=\"\\t\", header=None)\n",
        "\n",
        "# Fill nan (empty boxes) with 0\n",
        "train_df = train_df.fillna(0)\n",
        "test_df = test_df.fillna(0)\n",
        "val_df = val_df.fillna(0)\n",
        "\n",
        "train = train_df.values\n",
        "test = test_df.values\n",
        "val = val_df.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "A3TkMhc78nKV"
      },
      "outputs": [],
      "source": [
        "# label: ground truth results from politifacts\n",
        "labels = {'train':[train[i][2] for i in range(len(train))], 'test':[test[i][2] for i in range(len(test))], 'val':[val[i][2] for i in range(len(val))]}\n",
        "# Short Statement\n",
        "statements = {'train':[train[i][3] for i in range(len(train))], 'test':[test[i][3] for i in range(len(test))], 'val':[val[i][3] for i in range(len(val))]}\n",
        "# Topic of Statement\n",
        "subjects = {'train':[train[i][4] for i in range(len(train))], 'test':[test[i][4] for i in range(len(test))], 'val':[val[i][4] for i in range(len(val))]}\n",
        "# Speaker\n",
        "speakers = {'train':[train[i][5] for i in range(len(train))], 'test':[test[i][5] for i in range(len(test))], 'val':[val[i][5] for i in range(len(val))]}\n",
        "# Speaker job or title\n",
        "jobs = {'train':[train[i][6] for i in range(len(train))], 'test':[test[i][6] for i in range(len(test))], 'val':[val[i][6] for i in range(len(val))]}\n",
        "# State of Relevance\n",
        "states = {'train':[train[i][7] for i in range(len(train))], 'test':[test[i][7] for i in range(len(test))], 'val':[val[i][7] for i in range(len(val))]}\n",
        "# party affiliation\n",
        "affiliations = {'train':[train[i][8] for i in range(len(train))], 'test':[test[i][8] for i in range(len(test))], 'val':[val[i][8] for i in range(len(val))]}\n",
        "# total history of speaker(count of barely true, false, half true, mostly true, pants on fire respectively)\n",
        "credits = {'train':[train[i][9:14] for i in range(len(train))], 'test':[test[i][9:14] for i in range(len(test))], 'val':[val[i][9:14] for i in range(len(val))]}\n",
        "# venue of statement\n",
        "contexts = {'train':[train[i][14] for i in range(len(train))], 'test':[test[i][14] for i in range(len(test))], 'val':[val[i][14] for i in range(len(val))]}\n",
        "# verdict justification from politifacts\n",
        "justification = {'train':[train[i][15] for i in range(len(train))], 'test':[test[i][15] for i in range(len(test))], 'val':[val[i][15] for i in range(len(val))]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0yBDQOrR93wo"
      },
      "outputs": [],
      "source": [
        "# currently only do 2 way classfication & simplify 6 way label into true and false\n",
        "# convert label to 2 hot based on verdict label\n",
        "if config[\"num_labels\"] ==2:\n",
        "  def onehot(label):\n",
        "    label_onehot = [0]*len(label)\n",
        "    for i in range(len(label)):\n",
        "      if label[i] =='true' or label[i] =='mostly-true' or label[i] =='half-true':\n",
        "        label_onehot[i] = [1,0]\n",
        "      elif label[i] =='barely-true' or label[i] =='false' or label[i] =='pants-fire':\n",
        "        label_onehot[i] = [0,1]\n",
        "      else:\n",
        "        print('Unexpected Label. Set vector to [0]')\n",
        "    return label_onehot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "N9Qc774Z_lVc"
      },
      "outputs": [],
      "source": [
        "# Convert to one hot\n",
        "label_onehot = {'train':onehot(labels['train']), 'test':onehot(labels['test']), 'val':onehot(labels['val'])}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWIGKC5TBWjz",
        "outputId": "1fb43b12-85b9-4225-e892-ba36f5c56a27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abortion\n",
            "abortion dwayne-bohac State representative Texas republican a mailer\n"
          ]
        }
      ],
      "source": [
        "# Meta data\n",
        "metadata = {'train':[0]*len(train), 'val':[0]*len(val), 'test':[0]*len(test)}\n",
        "\n",
        "for i in range(len(train)):\n",
        "    subject = subjects['train'][i]\n",
        "    if subject == 0:\n",
        "        subject = 'None'\n",
        "\n",
        "    speaker = speakers['train'][i]\n",
        "    if speaker == 0:\n",
        "        speaker = 'None'\n",
        "\n",
        "    job = jobs['train'][i]\n",
        "    if job == 0:\n",
        "        job = 'None'\n",
        "\n",
        "    state = states['train'][i]\n",
        "    if state == 0:\n",
        "        state = 'None'\n",
        "\n",
        "    affiliation = affiliations['train'][i]\n",
        "    if affiliation == 0:\n",
        "        affiliation = 'None'\n",
        "\n",
        "    context = contexts['train'][i]\n",
        "    if context == 0 :\n",
        "        context = 'None'\n",
        "    if i == 0:\n",
        "      print(subject)\n",
        "    meta = subject + ' ' + speaker + ' ' + job + ' ' + state + ' ' + affiliation + ' ' + context\n",
        "    if i == 0:\n",
        "      print(meta)\n",
        "    metadata['train'][i] = meta\n",
        "\n",
        "for i in range(len(val)):\n",
        "    subject = subjects['val'][i]\n",
        "    if subject == 0:\n",
        "        subject = 'None'\n",
        "\n",
        "    speaker = speakers['val'][i]\n",
        "    if speaker == 0:\n",
        "        speaker = 'None'\n",
        "\n",
        "    job = jobs['val'][i]\n",
        "    if job == 0:\n",
        "        job = 'None'\n",
        "\n",
        "    state = states['val'][i]\n",
        "    if state == 0:\n",
        "        state = 'None'\n",
        "\n",
        "    affiliation = affiliations['val'][i]\n",
        "    if affiliation == 0:\n",
        "        affiliation = 'None'\n",
        "\n",
        "    context = contexts['val'][i]\n",
        "    if context == 0 :\n",
        "        context = 'None'\n",
        "\n",
        "    meta = subject + ' ' + speaker + ' ' + job + ' ' + state + ' ' + affiliation + ' ' + context\n",
        "\n",
        "    metadata['val'][i] = meta\n",
        "\n",
        "for i in range(len(test)):\n",
        "    subject = subjects['test'][i]\n",
        "    if subject == 0:\n",
        "        subject = 'None'\n",
        "\n",
        "    speaker = speakers['test'][i]\n",
        "    if speaker == 0:\n",
        "        speaker = 'None'\n",
        "\n",
        "    job = jobs['test'][i]\n",
        "    if job == 0:\n",
        "        job = 'None'\n",
        "\n",
        "    state = states['test'][i]\n",
        "    if state == 0:\n",
        "        state = 'None'\n",
        "\n",
        "    affiliation = affiliations['test'][i]\n",
        "    if affiliation == 0:\n",
        "        affiliation = 'None'\n",
        "\n",
        "    context = contexts['test'][i]\n",
        "    if context == 0 :\n",
        "        context = 'None'\n",
        "\n",
        "    meta = subject + ' ' + speaker + ' ' + job + ' ' + state + ' ' + affiliation + ' ' + context\n",
        "\n",
        "    metadata['test'][i] = meta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jF4_ku_TDV_G"
      },
      "outputs": [],
      "source": [
        "# Credit score calculation\n",
        "# barely true weighs 0.75, false weighs 0.9, half true weigh 0.5, mostly true weigh 0.2, pants on fire weigh 1\n",
        "credit_score = {'train':[0]*len(train), 'val':[0]*len(val), 'test':[0]*len(test)}\n",
        "for i in range(len(train)):\n",
        "    credit = credits['train'][i]\n",
        "    if sum(credit) == 0:\n",
        "        score = 0.5\n",
        "    else:\n",
        "        score = (credit[3]*0.2 + credit[2]*0.5 + credit[0]*0.75 + credit[1]*0.9 + credit[4]*1)/(sum(credit))\n",
        "    credit_score['train'][i] = [score for i in range(2304)]\n",
        "\n",
        "for i in range(len(val)):\n",
        "    credit = credits['val'][i]\n",
        "    if sum(credit) == 0:\n",
        "        score = 0.5\n",
        "    else:\n",
        "        score = (credit[3]*0.2 + credit[2]*0.5 + credit[0]*0.75 + credit[1]*0.9 + credit[4]*1)/(sum(credit))\n",
        "    credit_score['val'][i] = [score for i in range(2304)]\n",
        "\n",
        "for i in range(len(test)):\n",
        "    credit = credits['test'][i]\n",
        "    if sum(credit) == 0:\n",
        "        score = 0.5\n",
        "    else:\n",
        "        score = (credit[3]*0.2 + credit[2]*0.5 + credit[0]*0.75 + credit[1]*0.9 + credit[4]*1)/(sum(credit))\n",
        "    credit_score['test'][i] = [score for i in range(2304)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ORNHnSFroP0"
      },
      "source": [
        "# Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xRVPUbWMFzBk"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Loading the statements\n",
        "X_train = statements['train']\n",
        "y_train = label_onehot['train']\n",
        "\n",
        "X_val = statements['val']\n",
        "y_val = label_onehot['val']\n",
        "\n",
        "\n",
        "X_test = statements['test']\n",
        "y_test = label_onehot['test']\n",
        "\n",
        "# Loading the justification\n",
        "X_train_just = justification['train']\n",
        "\n",
        "X_val_just = justification['val']\n",
        "\n",
        "\n",
        "X_test_just = justification['test']\n",
        "\n",
        "\n",
        "# Loading the meta data\n",
        "X_train_meta = metadata['train']\n",
        "X_val_meta = metadata['val']\n",
        "X_test_meta = metadata['test']\n",
        "\n",
        "# Loading Credit scores\n",
        "\n",
        "X_train_credit = credit_score['train']\n",
        "X_val_credit = credit_score['val']\n",
        "X_test_credit = credit_score['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zrLbmyvZGTAJ"
      },
      "outputs": [],
      "source": [
        "max_seq_length_stat = 64\n",
        "max_seq_length_just = 256\n",
        "max_seq_length_meta = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agmNBKf4JrLV"
      },
      "source": [
        "### Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "afd0_vlbJmr_"
      },
      "outputs": [],
      "source": [
        "class TextDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, xy_list ,transform=None): \n",
        "        '''\n",
        "        Initializes the dataset.\n",
        "\n",
        "        '''\n",
        "\n",
        "        # Load the xy list\n",
        "\n",
        "        self.x_y_list = xy_list\n",
        "        self.length = len(xy_list[0])\n",
        "        for i in range(self.length):\n",
        "        #   Load in each statement and tokenize\n",
        "            #print(self.x_y_list[0][i])\n",
        "            tokenized_stat = tokenizer.tokenize(self.x_y_list[0][i])\n",
        "            if len(tokenized_stat) > max_seq_length_stat:\n",
        "              # clip if the statement is too long\n",
        "                tokenized_stat = tokenized_stat[:max_seq_length_stat]\n",
        "\n",
        "            # convert statement to ids\n",
        "            ids_stat  = tokenizer.convert_tokens_to_ids(tokenized_stat)\n",
        "            # pad the statement to given length\n",
        "            padding = [0] * (max_seq_length_stat - len(ids_stat))\n",
        "\n",
        "            ids_stat += padding\n",
        "            # sanity check\n",
        "            assert len(ids_stat) == max_seq_length_stat\n",
        "            \n",
        "            #if i == 1:\n",
        "            #  print(ids_stat)\n",
        "            ids_stat = torch.tensor(ids_stat)\n",
        "            \n",
        "            if self.x_y_list[1][i] == 0:\n",
        "                self.x_y_list[1][i] = 'No justification'\n",
        "\n",
        "            #print(self.x_y_list[1][i])\n",
        "            tokenized_just = tokenizer.tokenize(self.x_y_list[1][i])\n",
        "            if len(tokenized_just) > max_seq_length_just:\n",
        "              # clip if the statement is too long\n",
        "                tokenized_just = tokenized_just[:max_seq_length_just]\n",
        "\n",
        "            # convert statement to ids\n",
        "            ids_just  = tokenizer.convert_tokens_to_ids(tokenized_just)\n",
        "            # pad the statement to given length\n",
        "            padding = [0] * (max_seq_length_just - len(ids_just))\n",
        "\n",
        "            ids_just += padding\n",
        "            # sanity check\n",
        "            assert len(ids_just) == max_seq_length_just\n",
        "\n",
        "            #if i == 1:\n",
        "            #  print(ids_just)\n",
        "            ids_just = torch.tensor(ids_just)\n",
        "\n",
        "            #print(self.x_y_list[2][i])\n",
        "            tokenized_meta = tokenizer.tokenize(self.x_y_list[2][i])\n",
        "            if len(tokenized_meta) > max_seq_length_meta:\n",
        "              # clip if the statement is too long\n",
        "                tokenized_meta = tokenized_meta[:max_seq_length_meta]\n",
        "\n",
        "            # convert statement to ids\n",
        "            ids_meta  = tokenizer.convert_tokens_to_ids(tokenized_meta)\n",
        "            # pad the statement to given length\n",
        "            padding = [0] * (max_seq_length_meta - len(ids_meta))\n",
        "\n",
        "            ids_meta += padding\n",
        "            # sanity check\n",
        "            assert len(ids_meta) == max_seq_length_meta\n",
        "\n",
        "            ids_meta = torch.tensor(ids_meta)\n",
        "            \n",
        "            credit_scr = torch.tensor(self.x_y_list[3][i]) # Credit score\n",
        "\n",
        "            #if i == 1:\n",
        "            #  print(credit_scr)\n",
        "\n",
        "            label = torch.from_numpy(np.array(self.x_y_list[4][i]))\n",
        "\n",
        "            #if i == 1:\n",
        "            #  print(label)\n",
        "\n",
        "            self.x_y_list[0][i] = ids_stat\n",
        "            self.x_y_list[1][i] = ids_just\n",
        "            self.x_y_list[2][i] = ids_meta\n",
        "            self.x_y_list[3][i] = credit_scr\n",
        "            self.x_y_list[4][i] = label\n",
        "\n",
        "    def __len__(self):\n",
        "        \n",
        "        '''\n",
        "        TODO: What do we return here?\n",
        "        '''\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        '''\n",
        "        TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n",
        "\n",
        "        If you didn't do the loading and processing of the data in __init__,\n",
        "        do that here.\n",
        "\n",
        "        Once done, return a tuple of features and labels.\n",
        "        '''\n",
        "        \n",
        "        ids_stat = self.x_y_list[0][ind] \n",
        "        ids_just = self.x_y_list[1][ind]\n",
        "        ids_meta = self.x_y_list[2][ind] \n",
        "        credit_scr = self.x_y_list[3][ind] \n",
        "        label = self.x_y_list[4][ind]\n",
        "\n",
        "        return ids_stat, ids_just, ids_meta, credit_scr, label\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt-veYcdL6Fe"
      },
      "source": [
        "### Data - Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4icymeX1ImUN"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = config['batch_size'] # Increase if your device can handle it\n",
        "\n",
        "transforms = [] # set of tranformations\n",
        "# You may pass this as a parameter to the dataset class above\n",
        "# This will help modularize your implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmuPk9J6L8dz"
      },
      "source": [
        "### Data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "3_kG0gU2x4hH",
        "outputId": "815d4c14-04da-4579-fdad-dd8be1837393"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'abortion dwayne-bohac State representative Texas republican a mailer'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# get me RAMMM!!!! \n",
        "import gc \n",
        "gc.collect()\n",
        "X_train_meta[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mzoYfTKu14s",
        "outputId": "2fae7833-9768-4b14-b41e-6daef7c20a86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size:  8\n",
            "Train dataset samples = 10240, batches = 1280\n",
            "Val dataset samples = 1284, batches = 161\n",
            "Test dataset samples = 1267, batches = 159\n"
          ]
        }
      ],
      "source": [
        "# Create objects for the dataset class\n",
        "#train_data = TextDataset([X_train[:100], X_train_just[:100], X_train_meta[:100], X_train_credit[:100], y_train[:100]])\n",
        "train_data = TextDataset([X_train, X_train_just, X_train_meta, X_train_credit, y_train])\n",
        "val_data = TextDataset([X_val, X_val_just, X_val_meta, X_val_credit, y_val]) \n",
        "test_data = TextDataset([X_test, X_test_just, X_test_meta, X_test_credit, y_test]) \n",
        "\n",
        "# Do NOT forget to pass in the collate function as parameter while creating the dataloader\n",
        "train_loader = torch.utils.data.DataLoader(train_data, num_workers= 4,\n",
        "                                           batch_size=BATCH_SIZE, pin_memory= True,\n",
        "                                           shuffle= True)\n",
        "val_loader = torch.utils.data.DataLoader(val_data, num_workers= 2,\n",
        "                                           batch_size=BATCH_SIZE, pin_memory= True,\n",
        "                                           shuffle= True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, num_workers= 2,\n",
        "                                           batch_size=BATCH_SIZE, pin_memory= True,\n",
        "                                           shuffle= True)\n",
        "\n",
        "print(\"Batch size: \", BATCH_SIZE)\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXMtwyviKaxK",
        "outputId": "be550c47-1aec-49b9-b484-1611e2022a7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 64]) torch.Size([8, 256]) torch.Size([8, 32])\n"
          ]
        }
      ],
      "source": [
        "# sanity check\n",
        "for data in train_loader:\n",
        "    x, y, z, m, n = data\n",
        "    print(x.shape, y.shape, z.shape)\n",
        "    x, y, z, m, n = x.to(device), y.to(device), z.to(device), m.to(device), n.to(device) \n",
        "    break "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly4mjUUUuJhy"
      },
      "source": [
        "# Model Config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLad4pChcuvX"
      },
      "source": [
        "## Basic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [],
      "source": [
        "config_bert = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)"
      ],
      "metadata": {
        "id": "cmocsazoDQF_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [],
      "source": [
        "class BertLayerNorm(nn.Module):\n",
        "        def __init__(self, hidden_size, eps=1e-12):\n",
        "            \"\"\"Construct a layernorm module in the TF style (epsilon inside the square root).\n",
        "            \"\"\"\n",
        "            super(BertLayerNorm, self).__init__()\n",
        "            self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "            self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
        "            self.variance_epsilon = eps\n",
        "\n",
        "        def forward(self, x):\n",
        "            u = x.mean(-1, keepdim=True)\n",
        "            s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "            x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
        "            return self.weight * x + self.bias"
      ],
      "metadata": {
        "id": "CukOseHrDQF_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "EQhvHr71GJfq"
      },
      "outputs": [],
      "source": [
        "class BertForSequenceClassification(nn.Module):\n",
        "    def __init__(self, num_labels=2): # Change number of labels here.\n",
        "        super(BertForSequenceClassification, self).__init__()\n",
        "        self.num_labels = num_labels\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(config_bert.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config_bert.hidden_size*3, num_labels)\n",
        "        #self.fc1 = nn.Linear(config_bert.hidden_size*2, 512)\n",
        "        nn.init.xavier_normal_(self.classifier.weight)\n",
        "\n",
        "    '''def forward_once(self, x):\n",
        "        # Forward pass\n",
        "        output = self.cnn1(x)\n",
        "        output = output.view(output.size()[0], -1)\n",
        "        output = self.fc1(output)\n",
        "        return output'''\n",
        "\n",
        "    def forward_once(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
        "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        #logits = self.classifier(pooled_output)\n",
        "\n",
        "        return pooled_output\n",
        "\n",
        "    def forward(self, input_ids1, input_ids2, input_ids3, credit_sc):\n",
        "        # forward pass of input 1\n",
        "        output1 = self.forward_once(input_ids1, token_type_ids=None, attention_mask=None, labels=None)\n",
        "        # forward pass of input 2\n",
        "        output2 = self.forward_once(input_ids2, token_type_ids=None, attention_mask=None, labels=None)\n",
        "\n",
        "        output3 = self.forward_once(input_ids3, token_type_ids=None, attention_mask=None, labels=None)\n",
        "\n",
        "        out = torch.cat((output1, output2, output3), 1)\n",
        "        #print(out.shape)\n",
        "\n",
        "        # Multiply the credit score with the output after concatnation\n",
        "\n",
        "        out = torch.add(credit_sc, out)\n",
        "\n",
        "        #out = self.fc1(out)\n",
        "        logits = self.classifier(out)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def freeze_bert_encoder(self):\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def unfreeze_bert_encoder(self):\n",
        "        for param in self.bert.parameters():\n",
        "            param.requires_grad = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUThsowyQdN7"
      },
      "source": [
        "## INIT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "CGoiXd70tb5z"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "model = BertForSequenceClassification().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(model,x,y,z,m)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F1jmeZiwHJn8",
        "outputId": "d66977fe-facd-40fb-a20a-5440215ceade"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===========================================================================================================================\n",
            "                                                    Kernel Shape  \\\n",
            "Layer                                                              \n",
            "0_bert.embeddings.Embedding_word_embeddings         [768, 30522]   \n",
            "1_bert.embeddings.Embedding_position_embeddings       [768, 512]   \n",
            "2_bert.embeddings.Embedding_token_type_embeddings       [768, 2]   \n",
            "3_bert.embeddings.BertLayerNorm_LayerNorm                  [768]   \n",
            "4_bert.embeddings.Dropout_dropout                              -   \n",
            "5_bert.encoder.layer.0.attention.self.Linear_query    [768, 768]   \n",
            "6_bert.encoder.layer.0.attention.self.Linear_key      [768, 768]   \n",
            "7_bert.encoder.layer.0.attention.self.Linear_value    [768, 768]   \n",
            "8_bert.encoder.layer.0.attention.self.Dropout_d...             -   \n",
            "9_bert.encoder.layer.0.attention.output.Linear_...    [768, 768]   \n",
            "10_bert.encoder.layer.0.attention.output.Dropou...             -   \n",
            "11_bert.encoder.layer.0.attention.output.BertLa...         [768]   \n",
            "12_bert.encoder.layer.0.intermediate.Linear_dense    [768, 3072]   \n",
            "13_bert.encoder.layer.0.output.Linear_dense          [3072, 768]   \n",
            "14_bert.encoder.layer.0.output.Dropout_dropout                 -   \n",
            "15_bert.encoder.layer.0.output.BertLayerNorm_La...         [768]   \n",
            "16_bert.encoder.layer.1.attention.self.Linear_q...    [768, 768]   \n",
            "17_bert.encoder.layer.1.attention.self.Linear_key     [768, 768]   \n",
            "18_bert.encoder.layer.1.attention.self.Linear_v...    [768, 768]   \n",
            "19_bert.encoder.layer.1.attention.self.Dropout_...             -   \n",
            "20_bert.encoder.layer.1.attention.output.Linear...    [768, 768]   \n",
            "21_bert.encoder.layer.1.attention.output.Dropou...             -   \n",
            "22_bert.encoder.layer.1.attention.output.BertLa...         [768]   \n",
            "23_bert.encoder.layer.1.intermediate.Linear_dense    [768, 3072]   \n",
            "24_bert.encoder.layer.1.output.Linear_dense          [3072, 768]   \n",
            "25_bert.encoder.layer.1.output.Dropout_dropout                 -   \n",
            "26_bert.encoder.layer.1.output.BertLayerNorm_La...         [768]   \n",
            "27_bert.encoder.layer.2.attention.self.Linear_q...    [768, 768]   \n",
            "28_bert.encoder.layer.2.attention.self.Linear_key     [768, 768]   \n",
            "29_bert.encoder.layer.2.attention.self.Linear_v...    [768, 768]   \n",
            "30_bert.encoder.layer.2.attention.self.Dropout_...             -   \n",
            "31_bert.encoder.layer.2.attention.output.Linear...    [768, 768]   \n",
            "32_bert.encoder.layer.2.attention.output.Dropou...             -   \n",
            "33_bert.encoder.layer.2.attention.output.BertLa...         [768]   \n",
            "34_bert.encoder.layer.2.intermediate.Linear_dense    [768, 3072]   \n",
            "35_bert.encoder.layer.2.output.Linear_dense          [3072, 768]   \n",
            "36_bert.encoder.layer.2.output.Dropout_dropout                 -   \n",
            "37_bert.encoder.layer.2.output.BertLayerNorm_La...         [768]   \n",
            "38_bert.encoder.layer.3.attention.self.Linear_q...    [768, 768]   \n",
            "39_bert.encoder.layer.3.attention.self.Linear_key     [768, 768]   \n",
            "40_bert.encoder.layer.3.attention.self.Linear_v...    [768, 768]   \n",
            "41_bert.encoder.layer.3.attention.self.Dropout_...             -   \n",
            "42_bert.encoder.layer.3.attention.output.Linear...    [768, 768]   \n",
            "43_bert.encoder.layer.3.attention.output.Dropou...             -   \n",
            "44_bert.encoder.layer.3.attention.output.BertLa...         [768]   \n",
            "45_bert.encoder.layer.3.intermediate.Linear_dense    [768, 3072]   \n",
            "46_bert.encoder.layer.3.output.Linear_dense          [3072, 768]   \n",
            "47_bert.encoder.layer.3.output.Dropout_dropout                 -   \n",
            "48_bert.encoder.layer.3.output.BertLayerNorm_La...         [768]   \n",
            "49_bert.encoder.layer.4.attention.self.Linear_q...    [768, 768]   \n",
            "50_bert.encoder.layer.4.attention.self.Linear_key     [768, 768]   \n",
            "51_bert.encoder.layer.4.attention.self.Linear_v...    [768, 768]   \n",
            "52_bert.encoder.layer.4.attention.self.Dropout_...             -   \n",
            "53_bert.encoder.layer.4.attention.output.Linear...    [768, 768]   \n",
            "54_bert.encoder.layer.4.attention.output.Dropou...             -   \n",
            "55_bert.encoder.layer.4.attention.output.BertLa...         [768]   \n",
            "56_bert.encoder.layer.4.intermediate.Linear_dense    [768, 3072]   \n",
            "57_bert.encoder.layer.4.output.Linear_dense          [3072, 768]   \n",
            "58_bert.encoder.layer.4.output.Dropout_dropout                 -   \n",
            "59_bert.encoder.layer.4.output.BertLayerNorm_La...         [768]   \n",
            "60_bert.encoder.layer.5.attention.self.Linear_q...    [768, 768]   \n",
            "61_bert.encoder.layer.5.attention.self.Linear_key     [768, 768]   \n",
            "62_bert.encoder.layer.5.attention.self.Linear_v...    [768, 768]   \n",
            "63_bert.encoder.layer.5.attention.self.Dropout_...             -   \n",
            "64_bert.encoder.layer.5.attention.output.Linear...    [768, 768]   \n",
            "65_bert.encoder.layer.5.attention.output.Dropou...             -   \n",
            "66_bert.encoder.layer.5.attention.output.BertLa...         [768]   \n",
            "67_bert.encoder.layer.5.intermediate.Linear_dense    [768, 3072]   \n",
            "68_bert.encoder.layer.5.output.Linear_dense          [3072, 768]   \n",
            "69_bert.encoder.layer.5.output.Dropout_dropout                 -   \n",
            "70_bert.encoder.layer.5.output.BertLayerNorm_La...         [768]   \n",
            "71_bert.encoder.layer.6.attention.self.Linear_q...    [768, 768]   \n",
            "72_bert.encoder.layer.6.attention.self.Linear_key     [768, 768]   \n",
            "73_bert.encoder.layer.6.attention.self.Linear_v...    [768, 768]   \n",
            "74_bert.encoder.layer.6.attention.self.Dropout_...             -   \n",
            "75_bert.encoder.layer.6.attention.output.Linear...    [768, 768]   \n",
            "76_bert.encoder.layer.6.attention.output.Dropou...             -   \n",
            "77_bert.encoder.layer.6.attention.output.BertLa...         [768]   \n",
            "78_bert.encoder.layer.6.intermediate.Linear_dense    [768, 3072]   \n",
            "79_bert.encoder.layer.6.output.Linear_dense          [3072, 768]   \n",
            "80_bert.encoder.layer.6.output.Dropout_dropout                 -   \n",
            "81_bert.encoder.layer.6.output.BertLayerNorm_La...         [768]   \n",
            "82_bert.encoder.layer.7.attention.self.Linear_q...    [768, 768]   \n",
            "83_bert.encoder.layer.7.attention.self.Linear_key     [768, 768]   \n",
            "84_bert.encoder.layer.7.attention.self.Linear_v...    [768, 768]   \n",
            "85_bert.encoder.layer.7.attention.self.Dropout_...             -   \n",
            "86_bert.encoder.layer.7.attention.output.Linear...    [768, 768]   \n",
            "87_bert.encoder.layer.7.attention.output.Dropou...             -   \n",
            "88_bert.encoder.layer.7.attention.output.BertLa...         [768]   \n",
            "89_bert.encoder.layer.7.intermediate.Linear_dense    [768, 3072]   \n",
            "90_bert.encoder.layer.7.output.Linear_dense          [3072, 768]   \n",
            "91_bert.encoder.layer.7.output.Dropout_dropout                 -   \n",
            "92_bert.encoder.layer.7.output.BertLayerNorm_La...         [768]   \n",
            "93_bert.encoder.layer.8.attention.self.Linear_q...    [768, 768]   \n",
            "94_bert.encoder.layer.8.attention.self.Linear_key     [768, 768]   \n",
            "95_bert.encoder.layer.8.attention.self.Linear_v...    [768, 768]   \n",
            "96_bert.encoder.layer.8.attention.self.Dropout_...             -   \n",
            "97_bert.encoder.layer.8.attention.output.Linear...    [768, 768]   \n",
            "98_bert.encoder.layer.8.attention.output.Dropou...             -   \n",
            "99_bert.encoder.layer.8.attention.output.BertLa...         [768]   \n",
            "100_bert.encoder.layer.8.intermediate.Linear_dense   [768, 3072]   \n",
            "101_bert.encoder.layer.8.output.Linear_dense         [3072, 768]   \n",
            "102_bert.encoder.layer.8.output.Dropout_dropout                -   \n",
            "103_bert.encoder.layer.8.output.BertLayerNorm_L...         [768]   \n",
            "104_bert.encoder.layer.9.attention.self.Linear_...    [768, 768]   \n",
            "105_bert.encoder.layer.9.attention.self.Linear_key    [768, 768]   \n",
            "106_bert.encoder.layer.9.attention.self.Linear_...    [768, 768]   \n",
            "107_bert.encoder.layer.9.attention.self.Dropout...             -   \n",
            "108_bert.encoder.layer.9.attention.output.Linea...    [768, 768]   \n",
            "109_bert.encoder.layer.9.attention.output.Dropo...             -   \n",
            "110_bert.encoder.layer.9.attention.output.BertL...         [768]   \n",
            "111_bert.encoder.layer.9.intermediate.Linear_dense   [768, 3072]   \n",
            "112_bert.encoder.layer.9.output.Linear_dense         [3072, 768]   \n",
            "113_bert.encoder.layer.9.output.Dropout_dropout                -   \n",
            "114_bert.encoder.layer.9.output.BertLayerNorm_L...         [768]   \n",
            "115_bert.encoder.layer.10.attention.self.Linear...    [768, 768]   \n",
            "116_bert.encoder.layer.10.attention.self.Linear...    [768, 768]   \n",
            "117_bert.encoder.layer.10.attention.self.Linear...    [768, 768]   \n",
            "118_bert.encoder.layer.10.attention.self.Dropou...             -   \n",
            "119_bert.encoder.layer.10.attention.output.Line...    [768, 768]   \n",
            "120_bert.encoder.layer.10.attention.output.Drop...             -   \n",
            "121_bert.encoder.layer.10.attention.output.Bert...         [768]   \n",
            "122_bert.encoder.layer.10.intermediate.Linear_d...   [768, 3072]   \n",
            "123_bert.encoder.layer.10.output.Linear_dense        [3072, 768]   \n",
            "124_bert.encoder.layer.10.output.Dropout_dropout               -   \n",
            "125_bert.encoder.layer.10.output.BertLayerNorm_...         [768]   \n",
            "126_bert.encoder.layer.11.attention.self.Linear...    [768, 768]   \n",
            "127_bert.encoder.layer.11.attention.self.Linear...    [768, 768]   \n",
            "128_bert.encoder.layer.11.attention.self.Linear...    [768, 768]   \n",
            "129_bert.encoder.layer.11.attention.self.Dropou...             -   \n",
            "130_bert.encoder.layer.11.attention.output.Line...    [768, 768]   \n",
            "131_bert.encoder.layer.11.attention.output.Drop...             -   \n",
            "132_bert.encoder.layer.11.attention.output.Bert...         [768]   \n",
            "133_bert.encoder.layer.11.intermediate.Linear_d...   [768, 3072]   \n",
            "134_bert.encoder.layer.11.output.Linear_dense        [3072, 768]   \n",
            "135_bert.encoder.layer.11.output.Dropout_dropout               -   \n",
            "136_bert.encoder.layer.11.output.BertLayerNorm_...         [768]   \n",
            "137_bert.pooler.Linear_dense                          [768, 768]   \n",
            "138_bert.pooler.Tanh_activation                                -   \n",
            "139_dropout                                                    -   \n",
            "140_bert.embeddings.Embedding_word_embeddings       [768, 30522]   \n",
            "141_bert.embeddings.Embedding_position_embeddings     [768, 512]   \n",
            "142_bert.embeddings.Embedding_token_type_embedd...      [768, 2]   \n",
            "143_bert.embeddings.BertLayerNorm_LayerNorm                [768]   \n",
            "144_bert.embeddings.Dropout_dropout                            -   \n",
            "145_bert.encoder.layer.0.attention.self.Linear_...    [768, 768]   \n",
            "146_bert.encoder.layer.0.attention.self.Linear_key    [768, 768]   \n",
            "147_bert.encoder.layer.0.attention.self.Linear_...    [768, 768]   \n",
            "148_bert.encoder.layer.0.attention.self.Dropout...             -   \n",
            "149_bert.encoder.layer.0.attention.output.Linea...    [768, 768]   \n",
            "150_bert.encoder.layer.0.attention.output.Dropo...             -   \n",
            "151_bert.encoder.layer.0.attention.output.BertL...         [768]   \n",
            "152_bert.encoder.layer.0.intermediate.Linear_dense   [768, 3072]   \n",
            "153_bert.encoder.layer.0.output.Linear_dense         [3072, 768]   \n",
            "154_bert.encoder.layer.0.output.Dropout_dropout                -   \n",
            "155_bert.encoder.layer.0.output.BertLayerNorm_L...         [768]   \n",
            "156_bert.encoder.layer.1.attention.self.Linear_...    [768, 768]   \n",
            "157_bert.encoder.layer.1.attention.self.Linear_key    [768, 768]   \n",
            "158_bert.encoder.layer.1.attention.self.Linear_...    [768, 768]   \n",
            "159_bert.encoder.layer.1.attention.self.Dropout...             -   \n",
            "160_bert.encoder.layer.1.attention.output.Linea...    [768, 768]   \n",
            "161_bert.encoder.layer.1.attention.output.Dropo...             -   \n",
            "162_bert.encoder.layer.1.attention.output.BertL...         [768]   \n",
            "163_bert.encoder.layer.1.intermediate.Linear_dense   [768, 3072]   \n",
            "164_bert.encoder.layer.1.output.Linear_dense         [3072, 768]   \n",
            "165_bert.encoder.layer.1.output.Dropout_dropout                -   \n",
            "166_bert.encoder.layer.1.output.BertLayerNorm_L...         [768]   \n",
            "167_bert.encoder.layer.2.attention.self.Linear_...    [768, 768]   \n",
            "168_bert.encoder.layer.2.attention.self.Linear_key    [768, 768]   \n",
            "169_bert.encoder.layer.2.attention.self.Linear_...    [768, 768]   \n",
            "170_bert.encoder.layer.2.attention.self.Dropout...             -   \n",
            "171_bert.encoder.layer.2.attention.output.Linea...    [768, 768]   \n",
            "172_bert.encoder.layer.2.attention.output.Dropo...             -   \n",
            "173_bert.encoder.layer.2.attention.output.BertL...         [768]   \n",
            "174_bert.encoder.layer.2.intermediate.Linear_dense   [768, 3072]   \n",
            "175_bert.encoder.layer.2.output.Linear_dense         [3072, 768]   \n",
            "176_bert.encoder.layer.2.output.Dropout_dropout                -   \n",
            "177_bert.encoder.layer.2.output.BertLayerNorm_L...         [768]   \n",
            "178_bert.encoder.layer.3.attention.self.Linear_...    [768, 768]   \n",
            "179_bert.encoder.layer.3.attention.self.Linear_key    [768, 768]   \n",
            "180_bert.encoder.layer.3.attention.self.Linear_...    [768, 768]   \n",
            "181_bert.encoder.layer.3.attention.self.Dropout...             -   \n",
            "182_bert.encoder.layer.3.attention.output.Linea...    [768, 768]   \n",
            "183_bert.encoder.layer.3.attention.output.Dropo...             -   \n",
            "184_bert.encoder.layer.3.attention.output.BertL...         [768]   \n",
            "185_bert.encoder.layer.3.intermediate.Linear_dense   [768, 3072]   \n",
            "186_bert.encoder.layer.3.output.Linear_dense         [3072, 768]   \n",
            "187_bert.encoder.layer.3.output.Dropout_dropout                -   \n",
            "188_bert.encoder.layer.3.output.BertLayerNorm_L...         [768]   \n",
            "189_bert.encoder.layer.4.attention.self.Linear_...    [768, 768]   \n",
            "190_bert.encoder.layer.4.attention.self.Linear_key    [768, 768]   \n",
            "191_bert.encoder.layer.4.attention.self.Linear_...    [768, 768]   \n",
            "192_bert.encoder.layer.4.attention.self.Dropout...             -   \n",
            "193_bert.encoder.layer.4.attention.output.Linea...    [768, 768]   \n",
            "194_bert.encoder.layer.4.attention.output.Dropo...             -   \n",
            "195_bert.encoder.layer.4.attention.output.BertL...         [768]   \n",
            "196_bert.encoder.layer.4.intermediate.Linear_dense   [768, 3072]   \n",
            "197_bert.encoder.layer.4.output.Linear_dense         [3072, 768]   \n",
            "198_bert.encoder.layer.4.output.Dropout_dropout                -   \n",
            "199_bert.encoder.layer.4.output.BertLayerNorm_L...         [768]   \n",
            "200_bert.encoder.layer.5.attention.self.Linear_...    [768, 768]   \n",
            "201_bert.encoder.layer.5.attention.self.Linear_key    [768, 768]   \n",
            "202_bert.encoder.layer.5.attention.self.Linear_...    [768, 768]   \n",
            "203_bert.encoder.layer.5.attention.self.Dropout...             -   \n",
            "204_bert.encoder.layer.5.attention.output.Linea...    [768, 768]   \n",
            "205_bert.encoder.layer.5.attention.output.Dropo...             -   \n",
            "206_bert.encoder.layer.5.attention.output.BertL...         [768]   \n",
            "207_bert.encoder.layer.5.intermediate.Linear_dense   [768, 3072]   \n",
            "208_bert.encoder.layer.5.output.Linear_dense         [3072, 768]   \n",
            "209_bert.encoder.layer.5.output.Dropout_dropout                -   \n",
            "210_bert.encoder.layer.5.output.BertLayerNorm_L...         [768]   \n",
            "211_bert.encoder.layer.6.attention.self.Linear_...    [768, 768]   \n",
            "212_bert.encoder.layer.6.attention.self.Linear_key    [768, 768]   \n",
            "213_bert.encoder.layer.6.attention.self.Linear_...    [768, 768]   \n",
            "214_bert.encoder.layer.6.attention.self.Dropout...             -   \n",
            "215_bert.encoder.layer.6.attention.output.Linea...    [768, 768]   \n",
            "216_bert.encoder.layer.6.attention.output.Dropo...             -   \n",
            "217_bert.encoder.layer.6.attention.output.BertL...         [768]   \n",
            "218_bert.encoder.layer.6.intermediate.Linear_dense   [768, 3072]   \n",
            "219_bert.encoder.layer.6.output.Linear_dense         [3072, 768]   \n",
            "220_bert.encoder.layer.6.output.Dropout_dropout                -   \n",
            "221_bert.encoder.layer.6.output.BertLayerNorm_L...         [768]   \n",
            "222_bert.encoder.layer.7.attention.self.Linear_...    [768, 768]   \n",
            "223_bert.encoder.layer.7.attention.self.Linear_key    [768, 768]   \n",
            "224_bert.encoder.layer.7.attention.self.Linear_...    [768, 768]   \n",
            "225_bert.encoder.layer.7.attention.self.Dropout...             -   \n",
            "226_bert.encoder.layer.7.attention.output.Linea...    [768, 768]   \n",
            "227_bert.encoder.layer.7.attention.output.Dropo...             -   \n",
            "228_bert.encoder.layer.7.attention.output.BertL...         [768]   \n",
            "229_bert.encoder.layer.7.intermediate.Linear_dense   [768, 3072]   \n",
            "230_bert.encoder.layer.7.output.Linear_dense         [3072, 768]   \n",
            "231_bert.encoder.layer.7.output.Dropout_dropout                -   \n",
            "232_bert.encoder.layer.7.output.BertLayerNorm_L...         [768]   \n",
            "233_bert.encoder.layer.8.attention.self.Linear_...    [768, 768]   \n",
            "234_bert.encoder.layer.8.attention.self.Linear_key    [768, 768]   \n",
            "235_bert.encoder.layer.8.attention.self.Linear_...    [768, 768]   \n",
            "236_bert.encoder.layer.8.attention.self.Dropout...             -   \n",
            "237_bert.encoder.layer.8.attention.output.Linea...    [768, 768]   \n",
            "238_bert.encoder.layer.8.attention.output.Dropo...             -   \n",
            "239_bert.encoder.layer.8.attention.output.BertL...         [768]   \n",
            "240_bert.encoder.layer.8.intermediate.Linear_dense   [768, 3072]   \n",
            "241_bert.encoder.layer.8.output.Linear_dense         [3072, 768]   \n",
            "242_bert.encoder.layer.8.output.Dropout_dropout                -   \n",
            "243_bert.encoder.layer.8.output.BertLayerNorm_L...         [768]   \n",
            "244_bert.encoder.layer.9.attention.self.Linear_...    [768, 768]   \n",
            "245_bert.encoder.layer.9.attention.self.Linear_key    [768, 768]   \n",
            "246_bert.encoder.layer.9.attention.self.Linear_...    [768, 768]   \n",
            "247_bert.encoder.layer.9.attention.self.Dropout...             -   \n",
            "248_bert.encoder.layer.9.attention.output.Linea...    [768, 768]   \n",
            "249_bert.encoder.layer.9.attention.output.Dropo...             -   \n",
            "250_bert.encoder.layer.9.attention.output.BertL...         [768]   \n",
            "251_bert.encoder.layer.9.intermediate.Linear_dense   [768, 3072]   \n",
            "252_bert.encoder.layer.9.output.Linear_dense         [3072, 768]   \n",
            "253_bert.encoder.layer.9.output.Dropout_dropout                -   \n",
            "254_bert.encoder.layer.9.output.BertLayerNorm_L...         [768]   \n",
            "255_bert.encoder.layer.10.attention.self.Linear...    [768, 768]   \n",
            "256_bert.encoder.layer.10.attention.self.Linear...    [768, 768]   \n",
            "257_bert.encoder.layer.10.attention.self.Linear...    [768, 768]   \n",
            "258_bert.encoder.layer.10.attention.self.Dropou...             -   \n",
            "259_bert.encoder.layer.10.attention.output.Line...    [768, 768]   \n",
            "260_bert.encoder.layer.10.attention.output.Drop...             -   \n",
            "261_bert.encoder.layer.10.attention.output.Bert...         [768]   \n",
            "262_bert.encoder.layer.10.intermediate.Linear_d...   [768, 3072]   \n",
            "263_bert.encoder.layer.10.output.Linear_dense        [3072, 768]   \n",
            "264_bert.encoder.layer.10.output.Dropout_dropout               -   \n",
            "265_bert.encoder.layer.10.output.BertLayerNorm_...         [768]   \n",
            "266_bert.encoder.layer.11.attention.self.Linear...    [768, 768]   \n",
            "267_bert.encoder.layer.11.attention.self.Linear...    [768, 768]   \n",
            "268_bert.encoder.layer.11.attention.self.Linear...    [768, 768]   \n",
            "269_bert.encoder.layer.11.attention.self.Dropou...             -   \n",
            "270_bert.encoder.layer.11.attention.output.Line...    [768, 768]   \n",
            "271_bert.encoder.layer.11.attention.output.Drop...             -   \n",
            "272_bert.encoder.layer.11.attention.output.Bert...         [768]   \n",
            "273_bert.encoder.layer.11.intermediate.Linear_d...   [768, 3072]   \n",
            "274_bert.encoder.layer.11.output.Linear_dense        [3072, 768]   \n",
            "275_bert.encoder.layer.11.output.Dropout_dropout               -   \n",
            "276_bert.encoder.layer.11.output.BertLayerNorm_...         [768]   \n",
            "277_bert.pooler.Linear_dense                          [768, 768]   \n",
            "278_bert.pooler.Tanh_activation                                -   \n",
            "279_dropout                                                    -   \n",
            "280_bert.embeddings.Embedding_word_embeddings       [768, 30522]   \n",
            "281_bert.embeddings.Embedding_position_embeddings     [768, 512]   \n",
            "282_bert.embeddings.Embedding_token_type_embedd...      [768, 2]   \n",
            "283_bert.embeddings.BertLayerNorm_LayerNorm                [768]   \n",
            "284_bert.embeddings.Dropout_dropout                            -   \n",
            "285_bert.encoder.layer.0.attention.self.Linear_...    [768, 768]   \n",
            "286_bert.encoder.layer.0.attention.self.Linear_key    [768, 768]   \n",
            "287_bert.encoder.layer.0.attention.self.Linear_...    [768, 768]   \n",
            "288_bert.encoder.layer.0.attention.self.Dropout...             -   \n",
            "289_bert.encoder.layer.0.attention.output.Linea...    [768, 768]   \n",
            "290_bert.encoder.layer.0.attention.output.Dropo...             -   \n",
            "291_bert.encoder.layer.0.attention.output.BertL...         [768]   \n",
            "292_bert.encoder.layer.0.intermediate.Linear_dense   [768, 3072]   \n",
            "293_bert.encoder.layer.0.output.Linear_dense         [3072, 768]   \n",
            "294_bert.encoder.layer.0.output.Dropout_dropout                -   \n",
            "295_bert.encoder.layer.0.output.BertLayerNorm_L...         [768]   \n",
            "296_bert.encoder.layer.1.attention.self.Linear_...    [768, 768]   \n",
            "297_bert.encoder.layer.1.attention.self.Linear_key    [768, 768]   \n",
            "298_bert.encoder.layer.1.attention.self.Linear_...    [768, 768]   \n",
            "299_bert.encoder.layer.1.attention.self.Dropout...             -   \n",
            "300_bert.encoder.layer.1.attention.output.Linea...    [768, 768]   \n",
            "301_bert.encoder.layer.1.attention.output.Dropo...             -   \n",
            "302_bert.encoder.layer.1.attention.output.BertL...         [768]   \n",
            "303_bert.encoder.layer.1.intermediate.Linear_dense   [768, 3072]   \n",
            "304_bert.encoder.layer.1.output.Linear_dense         [3072, 768]   \n",
            "305_bert.encoder.layer.1.output.Dropout_dropout                -   \n",
            "306_bert.encoder.layer.1.output.BertLayerNorm_L...         [768]   \n",
            "307_bert.encoder.layer.2.attention.self.Linear_...    [768, 768]   \n",
            "308_bert.encoder.layer.2.attention.self.Linear_key    [768, 768]   \n",
            "309_bert.encoder.layer.2.attention.self.Linear_...    [768, 768]   \n",
            "310_bert.encoder.layer.2.attention.self.Dropout...             -   \n",
            "311_bert.encoder.layer.2.attention.output.Linea...    [768, 768]   \n",
            "312_bert.encoder.layer.2.attention.output.Dropo...             -   \n",
            "313_bert.encoder.layer.2.attention.output.BertL...         [768]   \n",
            "314_bert.encoder.layer.2.intermediate.Linear_dense   [768, 3072]   \n",
            "315_bert.encoder.layer.2.output.Linear_dense         [3072, 768]   \n",
            "316_bert.encoder.layer.2.output.Dropout_dropout                -   \n",
            "317_bert.encoder.layer.2.output.BertLayerNorm_L...         [768]   \n",
            "318_bert.encoder.layer.3.attention.self.Linear_...    [768, 768]   \n",
            "319_bert.encoder.layer.3.attention.self.Linear_key    [768, 768]   \n",
            "320_bert.encoder.layer.3.attention.self.Linear_...    [768, 768]   \n",
            "321_bert.encoder.layer.3.attention.self.Dropout...             -   \n",
            "322_bert.encoder.layer.3.attention.output.Linea...    [768, 768]   \n",
            "323_bert.encoder.layer.3.attention.output.Dropo...             -   \n",
            "324_bert.encoder.layer.3.attention.output.BertL...         [768]   \n",
            "325_bert.encoder.layer.3.intermediate.Linear_dense   [768, 3072]   \n",
            "326_bert.encoder.layer.3.output.Linear_dense         [3072, 768]   \n",
            "327_bert.encoder.layer.3.output.Dropout_dropout                -   \n",
            "328_bert.encoder.layer.3.output.BertLayerNorm_L...         [768]   \n",
            "329_bert.encoder.layer.4.attention.self.Linear_...    [768, 768]   \n",
            "330_bert.encoder.layer.4.attention.self.Linear_key    [768, 768]   \n",
            "331_bert.encoder.layer.4.attention.self.Linear_...    [768, 768]   \n",
            "332_bert.encoder.layer.4.attention.self.Dropout...             -   \n",
            "333_bert.encoder.layer.4.attention.output.Linea...    [768, 768]   \n",
            "334_bert.encoder.layer.4.attention.output.Dropo...             -   \n",
            "335_bert.encoder.layer.4.attention.output.BertL...         [768]   \n",
            "336_bert.encoder.layer.4.intermediate.Linear_dense   [768, 3072]   \n",
            "337_bert.encoder.layer.4.output.Linear_dense         [3072, 768]   \n",
            "338_bert.encoder.layer.4.output.Dropout_dropout                -   \n",
            "339_bert.encoder.layer.4.output.BertLayerNorm_L...         [768]   \n",
            "340_bert.encoder.layer.5.attention.self.Linear_...    [768, 768]   \n",
            "341_bert.encoder.layer.5.attention.self.Linear_key    [768, 768]   \n",
            "342_bert.encoder.layer.5.attention.self.Linear_...    [768, 768]   \n",
            "343_bert.encoder.layer.5.attention.self.Dropout...             -   \n",
            "344_bert.encoder.layer.5.attention.output.Linea...    [768, 768]   \n",
            "345_bert.encoder.layer.5.attention.output.Dropo...             -   \n",
            "346_bert.encoder.layer.5.attention.output.BertL...         [768]   \n",
            "347_bert.encoder.layer.5.intermediate.Linear_dense   [768, 3072]   \n",
            "348_bert.encoder.layer.5.output.Linear_dense         [3072, 768]   \n",
            "349_bert.encoder.layer.5.output.Dropout_dropout                -   \n",
            "350_bert.encoder.layer.5.output.BertLayerNorm_L...         [768]   \n",
            "351_bert.encoder.layer.6.attention.self.Linear_...    [768, 768]   \n",
            "352_bert.encoder.layer.6.attention.self.Linear_key    [768, 768]   \n",
            "353_bert.encoder.layer.6.attention.self.Linear_...    [768, 768]   \n",
            "354_bert.encoder.layer.6.attention.self.Dropout...             -   \n",
            "355_bert.encoder.layer.6.attention.output.Linea...    [768, 768]   \n",
            "356_bert.encoder.layer.6.attention.output.Dropo...             -   \n",
            "357_bert.encoder.layer.6.attention.output.BertL...         [768]   \n",
            "358_bert.encoder.layer.6.intermediate.Linear_dense   [768, 3072]   \n",
            "359_bert.encoder.layer.6.output.Linear_dense         [3072, 768]   \n",
            "360_bert.encoder.layer.6.output.Dropout_dropout                -   \n",
            "361_bert.encoder.layer.6.output.BertLayerNorm_L...         [768]   \n",
            "362_bert.encoder.layer.7.attention.self.Linear_...    [768, 768]   \n",
            "363_bert.encoder.layer.7.attention.self.Linear_key    [768, 768]   \n",
            "364_bert.encoder.layer.7.attention.self.Linear_...    [768, 768]   \n",
            "365_bert.encoder.layer.7.attention.self.Dropout...             -   \n",
            "366_bert.encoder.layer.7.attention.output.Linea...    [768, 768]   \n",
            "367_bert.encoder.layer.7.attention.output.Dropo...             -   \n",
            "368_bert.encoder.layer.7.attention.output.BertL...         [768]   \n",
            "369_bert.encoder.layer.7.intermediate.Linear_dense   [768, 3072]   \n",
            "370_bert.encoder.layer.7.output.Linear_dense         [3072, 768]   \n",
            "371_bert.encoder.layer.7.output.Dropout_dropout                -   \n",
            "372_bert.encoder.layer.7.output.BertLayerNorm_L...         [768]   \n",
            "373_bert.encoder.layer.8.attention.self.Linear_...    [768, 768]   \n",
            "374_bert.encoder.layer.8.attention.self.Linear_key    [768, 768]   \n",
            "375_bert.encoder.layer.8.attention.self.Linear_...    [768, 768]   \n",
            "376_bert.encoder.layer.8.attention.self.Dropout...             -   \n",
            "377_bert.encoder.layer.8.attention.output.Linea...    [768, 768]   \n",
            "378_bert.encoder.layer.8.attention.output.Dropo...             -   \n",
            "379_bert.encoder.layer.8.attention.output.BertL...         [768]   \n",
            "380_bert.encoder.layer.8.intermediate.Linear_dense   [768, 3072]   \n",
            "381_bert.encoder.layer.8.output.Linear_dense         [3072, 768]   \n",
            "382_bert.encoder.layer.8.output.Dropout_dropout                -   \n",
            "383_bert.encoder.layer.8.output.BertLayerNorm_L...         [768]   \n",
            "384_bert.encoder.layer.9.attention.self.Linear_...    [768, 768]   \n",
            "385_bert.encoder.layer.9.attention.self.Linear_key    [768, 768]   \n",
            "386_bert.encoder.layer.9.attention.self.Linear_...    [768, 768]   \n",
            "387_bert.encoder.layer.9.attention.self.Dropout...             -   \n",
            "388_bert.encoder.layer.9.attention.output.Linea...    [768, 768]   \n",
            "389_bert.encoder.layer.9.attention.output.Dropo...             -   \n",
            "390_bert.encoder.layer.9.attention.output.BertL...         [768]   \n",
            "391_bert.encoder.layer.9.intermediate.Linear_dense   [768, 3072]   \n",
            "392_bert.encoder.layer.9.output.Linear_dense         [3072, 768]   \n",
            "393_bert.encoder.layer.9.output.Dropout_dropout                -   \n",
            "394_bert.encoder.layer.9.output.BertLayerNorm_L...         [768]   \n",
            "395_bert.encoder.layer.10.attention.self.Linear...    [768, 768]   \n",
            "396_bert.encoder.layer.10.attention.self.Linear...    [768, 768]   \n",
            "397_bert.encoder.layer.10.attention.self.Linear...    [768, 768]   \n",
            "398_bert.encoder.layer.10.attention.self.Dropou...             -   \n",
            "399_bert.encoder.layer.10.attention.output.Line...    [768, 768]   \n",
            "400_bert.encoder.layer.10.attention.output.Drop...             -   \n",
            "401_bert.encoder.layer.10.attention.output.Bert...         [768]   \n",
            "402_bert.encoder.layer.10.intermediate.Linear_d...   [768, 3072]   \n",
            "403_bert.encoder.layer.10.output.Linear_dense        [3072, 768]   \n",
            "404_bert.encoder.layer.10.output.Dropout_dropout               -   \n",
            "405_bert.encoder.layer.10.output.BertLayerNorm_...         [768]   \n",
            "406_bert.encoder.layer.11.attention.self.Linear...    [768, 768]   \n",
            "407_bert.encoder.layer.11.attention.self.Linear...    [768, 768]   \n",
            "408_bert.encoder.layer.11.attention.self.Linear...    [768, 768]   \n",
            "409_bert.encoder.layer.11.attention.self.Dropou...             -   \n",
            "410_bert.encoder.layer.11.attention.output.Line...    [768, 768]   \n",
            "411_bert.encoder.layer.11.attention.output.Drop...             -   \n",
            "412_bert.encoder.layer.11.attention.output.Bert...         [768]   \n",
            "413_bert.encoder.layer.11.intermediate.Linear_d...   [768, 3072]   \n",
            "414_bert.encoder.layer.11.output.Linear_dense        [3072, 768]   \n",
            "415_bert.encoder.layer.11.output.Dropout_dropout               -   \n",
            "416_bert.encoder.layer.11.output.BertLayerNorm_...         [768]   \n",
            "417_bert.pooler.Linear_dense                          [768, 768]   \n",
            "418_bert.pooler.Tanh_activation                                -   \n",
            "419_dropout                                                    -   \n",
            "420_classifier                                         [2304, 2]   \n",
            "\n",
            "                                                         Output Shape  \\\n",
            "Layer                                                                   \n",
            "0_bert.embeddings.Embedding_word_embeddings              [8, 64, 768]   \n",
            "1_bert.embeddings.Embedding_position_embeddings          [8, 64, 768]   \n",
            "2_bert.embeddings.Embedding_token_type_embeddings        [8, 64, 768]   \n",
            "3_bert.embeddings.BertLayerNorm_LayerNorm                [8, 64, 768]   \n",
            "4_bert.embeddings.Dropout_dropout                        [8, 64, 768]   \n",
            "5_bert.encoder.layer.0.attention.self.Linear_query       [8, 64, 768]   \n",
            "6_bert.encoder.layer.0.attention.self.Linear_key         [8, 64, 768]   \n",
            "7_bert.encoder.layer.0.attention.self.Linear_value       [8, 64, 768]   \n",
            "8_bert.encoder.layer.0.attention.self.Dropout_d...    [8, 12, 64, 64]   \n",
            "9_bert.encoder.layer.0.attention.output.Linear_...       [8, 64, 768]   \n",
            "10_bert.encoder.layer.0.attention.output.Dropou...       [8, 64, 768]   \n",
            "11_bert.encoder.layer.0.attention.output.BertLa...       [8, 64, 768]   \n",
            "12_bert.encoder.layer.0.intermediate.Linear_dense       [8, 64, 3072]   \n",
            "13_bert.encoder.layer.0.output.Linear_dense              [8, 64, 768]   \n",
            "14_bert.encoder.layer.0.output.Dropout_dropout           [8, 64, 768]   \n",
            "15_bert.encoder.layer.0.output.BertLayerNorm_La...       [8, 64, 768]   \n",
            "16_bert.encoder.layer.1.attention.self.Linear_q...       [8, 64, 768]   \n",
            "17_bert.encoder.layer.1.attention.self.Linear_key        [8, 64, 768]   \n",
            "18_bert.encoder.layer.1.attention.self.Linear_v...       [8, 64, 768]   \n",
            "19_bert.encoder.layer.1.attention.self.Dropout_...    [8, 12, 64, 64]   \n",
            "20_bert.encoder.layer.1.attention.output.Linear...       [8, 64, 768]   \n",
            "21_bert.encoder.layer.1.attention.output.Dropou...       [8, 64, 768]   \n",
            "22_bert.encoder.layer.1.attention.output.BertLa...       [8, 64, 768]   \n",
            "23_bert.encoder.layer.1.intermediate.Linear_dense       [8, 64, 3072]   \n",
            "24_bert.encoder.layer.1.output.Linear_dense              [8, 64, 768]   \n",
            "25_bert.encoder.layer.1.output.Dropout_dropout           [8, 64, 768]   \n",
            "26_bert.encoder.layer.1.output.BertLayerNorm_La...       [8, 64, 768]   \n",
            "27_bert.encoder.layer.2.attention.self.Linear_q...       [8, 64, 768]   \n",
            "28_bert.encoder.layer.2.attention.self.Linear_key        [8, 64, 768]   \n",
            "29_bert.encoder.layer.2.attention.self.Linear_v...       [8, 64, 768]   \n",
            "30_bert.encoder.layer.2.attention.self.Dropout_...    [8, 12, 64, 64]   \n",
            "31_bert.encoder.layer.2.attention.output.Linear...       [8, 64, 768]   \n",
            "32_bert.encoder.layer.2.attention.output.Dropou...       [8, 64, 768]   \n",
            "33_bert.encoder.layer.2.attention.output.BertLa...       [8, 64, 768]   \n",
            "34_bert.encoder.layer.2.intermediate.Linear_dense       [8, 64, 3072]   \n",
            "35_bert.encoder.layer.2.output.Linear_dense              [8, 64, 768]   \n",
            "36_bert.encoder.layer.2.output.Dropout_dropout           [8, 64, 768]   \n",
            "37_bert.encoder.layer.2.output.BertLayerNorm_La...       [8, 64, 768]   \n",
            "38_bert.encoder.layer.3.attention.self.Linear_q...       [8, 64, 768]   \n",
            "39_bert.encoder.layer.3.attention.self.Linear_key        [8, 64, 768]   \n",
            "40_bert.encoder.layer.3.attention.self.Linear_v...       [8, 64, 768]   \n",
            "41_bert.encoder.layer.3.attention.self.Dropout_...    [8, 12, 64, 64]   \n",
            "42_bert.encoder.layer.3.attention.output.Linear...       [8, 64, 768]   \n",
            "43_bert.encoder.layer.3.attention.output.Dropou...       [8, 64, 768]   \n",
            "44_bert.encoder.layer.3.attention.output.BertLa...       [8, 64, 768]   \n",
            "45_bert.encoder.layer.3.intermediate.Linear_dense       [8, 64, 3072]   \n",
            "46_bert.encoder.layer.3.output.Linear_dense              [8, 64, 768]   \n",
            "47_bert.encoder.layer.3.output.Dropout_dropout           [8, 64, 768]   \n",
            "48_bert.encoder.layer.3.output.BertLayerNorm_La...       [8, 64, 768]   \n",
            "49_bert.encoder.layer.4.attention.self.Linear_q...       [8, 64, 768]   \n",
            "50_bert.encoder.layer.4.attention.self.Linear_key        [8, 64, 768]   \n",
            "51_bert.encoder.layer.4.attention.self.Linear_v...       [8, 64, 768]   \n",
            "52_bert.encoder.layer.4.attention.self.Dropout_...    [8, 12, 64, 64]   \n",
            "53_bert.encoder.layer.4.attention.output.Linear...       [8, 64, 768]   \n",
            "54_bert.encoder.layer.4.attention.output.Dropou...       [8, 64, 768]   \n",
            "55_bert.encoder.layer.4.attention.output.BertLa...       [8, 64, 768]   \n",
            "56_bert.encoder.layer.4.intermediate.Linear_dense       [8, 64, 3072]   \n",
            "57_bert.encoder.layer.4.output.Linear_dense              [8, 64, 768]   \n",
            "58_bert.encoder.layer.4.output.Dropout_dropout           [8, 64, 768]   \n",
            "59_bert.encoder.layer.4.output.BertLayerNorm_La...       [8, 64, 768]   \n",
            "60_bert.encoder.layer.5.attention.self.Linear_q...       [8, 64, 768]   \n",
            "61_bert.encoder.layer.5.attention.self.Linear_key        [8, 64, 768]   \n",
            "62_bert.encoder.layer.5.attention.self.Linear_v...       [8, 64, 768]   \n",
            "63_bert.encoder.layer.5.attention.self.Dropout_...    [8, 12, 64, 64]   \n",
            "64_bert.encoder.layer.5.attention.output.Linear...       [8, 64, 768]   \n",
            "65_bert.encoder.layer.5.attention.output.Dropou...       [8, 64, 768]   \n",
            "66_bert.encoder.layer.5.attention.output.BertLa...       [8, 64, 768]   \n",
            "67_bert.encoder.layer.5.intermediate.Linear_dense       [8, 64, 3072]   \n",
            "68_bert.encoder.layer.5.output.Linear_dense              [8, 64, 768]   \n",
            "69_bert.encoder.layer.5.output.Dropout_dropout           [8, 64, 768]   \n",
            "70_bert.encoder.layer.5.output.BertLayerNorm_La...       [8, 64, 768]   \n",
            "71_bert.encoder.layer.6.attention.self.Linear_q...       [8, 64, 768]   \n",
            "72_bert.encoder.layer.6.attention.self.Linear_key        [8, 64, 768]   \n",
            "73_bert.encoder.layer.6.attention.self.Linear_v...       [8, 64, 768]   \n",
            "74_bert.encoder.layer.6.attention.self.Dropout_...    [8, 12, 64, 64]   \n",
            "75_bert.encoder.layer.6.attention.output.Linear...       [8, 64, 768]   \n",
            "76_bert.encoder.layer.6.attention.output.Dropou...       [8, 64, 768]   \n",
            "77_bert.encoder.layer.6.attention.output.BertLa...       [8, 64, 768]   \n",
            "78_bert.encoder.layer.6.intermediate.Linear_dense       [8, 64, 3072]   \n",
            "79_bert.encoder.layer.6.output.Linear_dense              [8, 64, 768]   \n",
            "80_bert.encoder.layer.6.output.Dropout_dropout           [8, 64, 768]   \n",
            "81_bert.encoder.layer.6.output.BertLayerNorm_La...       [8, 64, 768]   \n",
            "82_bert.encoder.layer.7.attention.self.Linear_q...       [8, 64, 768]   \n",
            "83_bert.encoder.layer.7.attention.self.Linear_key        [8, 64, 768]   \n",
            "84_bert.encoder.layer.7.attention.self.Linear_v...       [8, 64, 768]   \n",
            "85_bert.encoder.layer.7.attention.self.Dropout_...    [8, 12, 64, 64]   \n",
            "86_bert.encoder.layer.7.attention.output.Linear...       [8, 64, 768]   \n",
            "87_bert.encoder.layer.7.attention.output.Dropou...       [8, 64, 768]   \n",
            "88_bert.encoder.layer.7.attention.output.BertLa...       [8, 64, 768]   \n",
            "89_bert.encoder.layer.7.intermediate.Linear_dense       [8, 64, 3072]   \n",
            "90_bert.encoder.layer.7.output.Linear_dense              [8, 64, 768]   \n",
            "91_bert.encoder.layer.7.output.Dropout_dropout           [8, 64, 768]   \n",
            "92_bert.encoder.layer.7.output.BertLayerNorm_La...       [8, 64, 768]   \n",
            "93_bert.encoder.layer.8.attention.self.Linear_q...       [8, 64, 768]   \n",
            "94_bert.encoder.layer.8.attention.self.Linear_key        [8, 64, 768]   \n",
            "95_bert.encoder.layer.8.attention.self.Linear_v...       [8, 64, 768]   \n",
            "96_bert.encoder.layer.8.attention.self.Dropout_...    [8, 12, 64, 64]   \n",
            "97_bert.encoder.layer.8.attention.output.Linear...       [8, 64, 768]   \n",
            "98_bert.encoder.layer.8.attention.output.Dropou...       [8, 64, 768]   \n",
            "99_bert.encoder.layer.8.attention.output.BertLa...       [8, 64, 768]   \n",
            "100_bert.encoder.layer.8.intermediate.Linear_dense      [8, 64, 3072]   \n",
            "101_bert.encoder.layer.8.output.Linear_dense             [8, 64, 768]   \n",
            "102_bert.encoder.layer.8.output.Dropout_dropout          [8, 64, 768]   \n",
            "103_bert.encoder.layer.8.output.BertLayerNorm_L...       [8, 64, 768]   \n",
            "104_bert.encoder.layer.9.attention.self.Linear_...       [8, 64, 768]   \n",
            "105_bert.encoder.layer.9.attention.self.Linear_key       [8, 64, 768]   \n",
            "106_bert.encoder.layer.9.attention.self.Linear_...       [8, 64, 768]   \n",
            "107_bert.encoder.layer.9.attention.self.Dropout...    [8, 12, 64, 64]   \n",
            "108_bert.encoder.layer.9.attention.output.Linea...       [8, 64, 768]   \n",
            "109_bert.encoder.layer.9.attention.output.Dropo...       [8, 64, 768]   \n",
            "110_bert.encoder.layer.9.attention.output.BertL...       [8, 64, 768]   \n",
            "111_bert.encoder.layer.9.intermediate.Linear_dense      [8, 64, 3072]   \n",
            "112_bert.encoder.layer.9.output.Linear_dense             [8, 64, 768]   \n",
            "113_bert.encoder.layer.9.output.Dropout_dropout          [8, 64, 768]   \n",
            "114_bert.encoder.layer.9.output.BertLayerNorm_L...       [8, 64, 768]   \n",
            "115_bert.encoder.layer.10.attention.self.Linear...       [8, 64, 768]   \n",
            "116_bert.encoder.layer.10.attention.self.Linear...       [8, 64, 768]   \n",
            "117_bert.encoder.layer.10.attention.self.Linear...       [8, 64, 768]   \n",
            "118_bert.encoder.layer.10.attention.self.Dropou...    [8, 12, 64, 64]   \n",
            "119_bert.encoder.layer.10.attention.output.Line...       [8, 64, 768]   \n",
            "120_bert.encoder.layer.10.attention.output.Drop...       [8, 64, 768]   \n",
            "121_bert.encoder.layer.10.attention.output.Bert...       [8, 64, 768]   \n",
            "122_bert.encoder.layer.10.intermediate.Linear_d...      [8, 64, 3072]   \n",
            "123_bert.encoder.layer.10.output.Linear_dense            [8, 64, 768]   \n",
            "124_bert.encoder.layer.10.output.Dropout_dropout         [8, 64, 768]   \n",
            "125_bert.encoder.layer.10.output.BertLayerNorm_...       [8, 64, 768]   \n",
            "126_bert.encoder.layer.11.attention.self.Linear...       [8, 64, 768]   \n",
            "127_bert.encoder.layer.11.attention.self.Linear...       [8, 64, 768]   \n",
            "128_bert.encoder.layer.11.attention.self.Linear...       [8, 64, 768]   \n",
            "129_bert.encoder.layer.11.attention.self.Dropou...    [8, 12, 64, 64]   \n",
            "130_bert.encoder.layer.11.attention.output.Line...       [8, 64, 768]   \n",
            "131_bert.encoder.layer.11.attention.output.Drop...       [8, 64, 768]   \n",
            "132_bert.encoder.layer.11.attention.output.Bert...       [8, 64, 768]   \n",
            "133_bert.encoder.layer.11.intermediate.Linear_d...      [8, 64, 3072]   \n",
            "134_bert.encoder.layer.11.output.Linear_dense            [8, 64, 768]   \n",
            "135_bert.encoder.layer.11.output.Dropout_dropout         [8, 64, 768]   \n",
            "136_bert.encoder.layer.11.output.BertLayerNorm_...       [8, 64, 768]   \n",
            "137_bert.pooler.Linear_dense                                 [8, 768]   \n",
            "138_bert.pooler.Tanh_activation                              [8, 768]   \n",
            "139_dropout                                                  [8, 768]   \n",
            "140_bert.embeddings.Embedding_word_embeddings           [8, 256, 768]   \n",
            "141_bert.embeddings.Embedding_position_embeddings       [8, 256, 768]   \n",
            "142_bert.embeddings.Embedding_token_type_embedd...      [8, 256, 768]   \n",
            "143_bert.embeddings.BertLayerNorm_LayerNorm             [8, 256, 768]   \n",
            "144_bert.embeddings.Dropout_dropout                     [8, 256, 768]   \n",
            "145_bert.encoder.layer.0.attention.self.Linear_...      [8, 256, 768]   \n",
            "146_bert.encoder.layer.0.attention.self.Linear_key      [8, 256, 768]   \n",
            "147_bert.encoder.layer.0.attention.self.Linear_...      [8, 256, 768]   \n",
            "148_bert.encoder.layer.0.attention.self.Dropout...  [8, 12, 256, 256]   \n",
            "149_bert.encoder.layer.0.attention.output.Linea...      [8, 256, 768]   \n",
            "150_bert.encoder.layer.0.attention.output.Dropo...      [8, 256, 768]   \n",
            "151_bert.encoder.layer.0.attention.output.BertL...      [8, 256, 768]   \n",
            "152_bert.encoder.layer.0.intermediate.Linear_dense     [8, 256, 3072]   \n",
            "153_bert.encoder.layer.0.output.Linear_dense            [8, 256, 768]   \n",
            "154_bert.encoder.layer.0.output.Dropout_dropout         [8, 256, 768]   \n",
            "155_bert.encoder.layer.0.output.BertLayerNorm_L...      [8, 256, 768]   \n",
            "156_bert.encoder.layer.1.attention.self.Linear_...      [8, 256, 768]   \n",
            "157_bert.encoder.layer.1.attention.self.Linear_key      [8, 256, 768]   \n",
            "158_bert.encoder.layer.1.attention.self.Linear_...      [8, 256, 768]   \n",
            "159_bert.encoder.layer.1.attention.self.Dropout...  [8, 12, 256, 256]   \n",
            "160_bert.encoder.layer.1.attention.output.Linea...      [8, 256, 768]   \n",
            "161_bert.encoder.layer.1.attention.output.Dropo...      [8, 256, 768]   \n",
            "162_bert.encoder.layer.1.attention.output.BertL...      [8, 256, 768]   \n",
            "163_bert.encoder.layer.1.intermediate.Linear_dense     [8, 256, 3072]   \n",
            "164_bert.encoder.layer.1.output.Linear_dense            [8, 256, 768]   \n",
            "165_bert.encoder.layer.1.output.Dropout_dropout         [8, 256, 768]   \n",
            "166_bert.encoder.layer.1.output.BertLayerNorm_L...      [8, 256, 768]   \n",
            "167_bert.encoder.layer.2.attention.self.Linear_...      [8, 256, 768]   \n",
            "168_bert.encoder.layer.2.attention.self.Linear_key      [8, 256, 768]   \n",
            "169_bert.encoder.layer.2.attention.self.Linear_...      [8, 256, 768]   \n",
            "170_bert.encoder.layer.2.attention.self.Dropout...  [8, 12, 256, 256]   \n",
            "171_bert.encoder.layer.2.attention.output.Linea...      [8, 256, 768]   \n",
            "172_bert.encoder.layer.2.attention.output.Dropo...      [8, 256, 768]   \n",
            "173_bert.encoder.layer.2.attention.output.BertL...      [8, 256, 768]   \n",
            "174_bert.encoder.layer.2.intermediate.Linear_dense     [8, 256, 3072]   \n",
            "175_bert.encoder.layer.2.output.Linear_dense            [8, 256, 768]   \n",
            "176_bert.encoder.layer.2.output.Dropout_dropout         [8, 256, 768]   \n",
            "177_bert.encoder.layer.2.output.BertLayerNorm_L...      [8, 256, 768]   \n",
            "178_bert.encoder.layer.3.attention.self.Linear_...      [8, 256, 768]   \n",
            "179_bert.encoder.layer.3.attention.self.Linear_key      [8, 256, 768]   \n",
            "180_bert.encoder.layer.3.attention.self.Linear_...      [8, 256, 768]   \n",
            "181_bert.encoder.layer.3.attention.self.Dropout...  [8, 12, 256, 256]   \n",
            "182_bert.encoder.layer.3.attention.output.Linea...      [8, 256, 768]   \n",
            "183_bert.encoder.layer.3.attention.output.Dropo...      [8, 256, 768]   \n",
            "184_bert.encoder.layer.3.attention.output.BertL...      [8, 256, 768]   \n",
            "185_bert.encoder.layer.3.intermediate.Linear_dense     [8, 256, 3072]   \n",
            "186_bert.encoder.layer.3.output.Linear_dense            [8, 256, 768]   \n",
            "187_bert.encoder.layer.3.output.Dropout_dropout         [8, 256, 768]   \n",
            "188_bert.encoder.layer.3.output.BertLayerNorm_L...      [8, 256, 768]   \n",
            "189_bert.encoder.layer.4.attention.self.Linear_...      [8, 256, 768]   \n",
            "190_bert.encoder.layer.4.attention.self.Linear_key      [8, 256, 768]   \n",
            "191_bert.encoder.layer.4.attention.self.Linear_...      [8, 256, 768]   \n",
            "192_bert.encoder.layer.4.attention.self.Dropout...  [8, 12, 256, 256]   \n",
            "193_bert.encoder.layer.4.attention.output.Linea...      [8, 256, 768]   \n",
            "194_bert.encoder.layer.4.attention.output.Dropo...      [8, 256, 768]   \n",
            "195_bert.encoder.layer.4.attention.output.BertL...      [8, 256, 768]   \n",
            "196_bert.encoder.layer.4.intermediate.Linear_dense     [8, 256, 3072]   \n",
            "197_bert.encoder.layer.4.output.Linear_dense            [8, 256, 768]   \n",
            "198_bert.encoder.layer.4.output.Dropout_dropout         [8, 256, 768]   \n",
            "199_bert.encoder.layer.4.output.BertLayerNorm_L...      [8, 256, 768]   \n",
            "200_bert.encoder.layer.5.attention.self.Linear_...      [8, 256, 768]   \n",
            "201_bert.encoder.layer.5.attention.self.Linear_key      [8, 256, 768]   \n",
            "202_bert.encoder.layer.5.attention.self.Linear_...      [8, 256, 768]   \n",
            "203_bert.encoder.layer.5.attention.self.Dropout...  [8, 12, 256, 256]   \n",
            "204_bert.encoder.layer.5.attention.output.Linea...      [8, 256, 768]   \n",
            "205_bert.encoder.layer.5.attention.output.Dropo...      [8, 256, 768]   \n",
            "206_bert.encoder.layer.5.attention.output.BertL...      [8, 256, 768]   \n",
            "207_bert.encoder.layer.5.intermediate.Linear_dense     [8, 256, 3072]   \n",
            "208_bert.encoder.layer.5.output.Linear_dense            [8, 256, 768]   \n",
            "209_bert.encoder.layer.5.output.Dropout_dropout         [8, 256, 768]   \n",
            "210_bert.encoder.layer.5.output.BertLayerNorm_L...      [8, 256, 768]   \n",
            "211_bert.encoder.layer.6.attention.self.Linear_...      [8, 256, 768]   \n",
            "212_bert.encoder.layer.6.attention.self.Linear_key      [8, 256, 768]   \n",
            "213_bert.encoder.layer.6.attention.self.Linear_...      [8, 256, 768]   \n",
            "214_bert.encoder.layer.6.attention.self.Dropout...  [8, 12, 256, 256]   \n",
            "215_bert.encoder.layer.6.attention.output.Linea...      [8, 256, 768]   \n",
            "216_bert.encoder.layer.6.attention.output.Dropo...      [8, 256, 768]   \n",
            "217_bert.encoder.layer.6.attention.output.BertL...      [8, 256, 768]   \n",
            "218_bert.encoder.layer.6.intermediate.Linear_dense     [8, 256, 3072]   \n",
            "219_bert.encoder.layer.6.output.Linear_dense            [8, 256, 768]   \n",
            "220_bert.encoder.layer.6.output.Dropout_dropout         [8, 256, 768]   \n",
            "221_bert.encoder.layer.6.output.BertLayerNorm_L...      [8, 256, 768]   \n",
            "222_bert.encoder.layer.7.attention.self.Linear_...      [8, 256, 768]   \n",
            "223_bert.encoder.layer.7.attention.self.Linear_key      [8, 256, 768]   \n",
            "224_bert.encoder.layer.7.attention.self.Linear_...      [8, 256, 768]   \n",
            "225_bert.encoder.layer.7.attention.self.Dropout...  [8, 12, 256, 256]   \n",
            "226_bert.encoder.layer.7.attention.output.Linea...      [8, 256, 768]   \n",
            "227_bert.encoder.layer.7.attention.output.Dropo...      [8, 256, 768]   \n",
            "228_bert.encoder.layer.7.attention.output.BertL...      [8, 256, 768]   \n",
            "229_bert.encoder.layer.7.intermediate.Linear_dense     [8, 256, 3072]   \n",
            "230_bert.encoder.layer.7.output.Linear_dense            [8, 256, 768]   \n",
            "231_bert.encoder.layer.7.output.Dropout_dropout         [8, 256, 768]   \n",
            "232_bert.encoder.layer.7.output.BertLayerNorm_L...      [8, 256, 768]   \n",
            "233_bert.encoder.layer.8.attention.self.Linear_...      [8, 256, 768]   \n",
            "234_bert.encoder.layer.8.attention.self.Linear_key      [8, 256, 768]   \n",
            "235_bert.encoder.layer.8.attention.self.Linear_...      [8, 256, 768]   \n",
            "236_bert.encoder.layer.8.attention.self.Dropout...  [8, 12, 256, 256]   \n",
            "237_bert.encoder.layer.8.attention.output.Linea...      [8, 256, 768]   \n",
            "238_bert.encoder.layer.8.attention.output.Dropo...      [8, 256, 768]   \n",
            "239_bert.encoder.layer.8.attention.output.BertL...      [8, 256, 768]   \n",
            "240_bert.encoder.layer.8.intermediate.Linear_dense     [8, 256, 3072]   \n",
            "241_bert.encoder.layer.8.output.Linear_dense            [8, 256, 768]   \n",
            "242_bert.encoder.layer.8.output.Dropout_dropout         [8, 256, 768]   \n",
            "243_bert.encoder.layer.8.output.BertLayerNorm_L...      [8, 256, 768]   \n",
            "244_bert.encoder.layer.9.attention.self.Linear_...      [8, 256, 768]   \n",
            "245_bert.encoder.layer.9.attention.self.Linear_key      [8, 256, 768]   \n",
            "246_bert.encoder.layer.9.attention.self.Linear_...      [8, 256, 768]   \n",
            "247_bert.encoder.layer.9.attention.self.Dropout...  [8, 12, 256, 256]   \n",
            "248_bert.encoder.layer.9.attention.output.Linea...      [8, 256, 768]   \n",
            "249_bert.encoder.layer.9.attention.output.Dropo...      [8, 256, 768]   \n",
            "250_bert.encoder.layer.9.attention.output.BertL...      [8, 256, 768]   \n",
            "251_bert.encoder.layer.9.intermediate.Linear_dense     [8, 256, 3072]   \n",
            "252_bert.encoder.layer.9.output.Linear_dense            [8, 256, 768]   \n",
            "253_bert.encoder.layer.9.output.Dropout_dropout         [8, 256, 768]   \n",
            "254_bert.encoder.layer.9.output.BertLayerNorm_L...      [8, 256, 768]   \n",
            "255_bert.encoder.layer.10.attention.self.Linear...      [8, 256, 768]   \n",
            "256_bert.encoder.layer.10.attention.self.Linear...      [8, 256, 768]   \n",
            "257_bert.encoder.layer.10.attention.self.Linear...      [8, 256, 768]   \n",
            "258_bert.encoder.layer.10.attention.self.Dropou...  [8, 12, 256, 256]   \n",
            "259_bert.encoder.layer.10.attention.output.Line...      [8, 256, 768]   \n",
            "260_bert.encoder.layer.10.attention.output.Drop...      [8, 256, 768]   \n",
            "261_bert.encoder.layer.10.attention.output.Bert...      [8, 256, 768]   \n",
            "262_bert.encoder.layer.10.intermediate.Linear_d...     [8, 256, 3072]   \n",
            "263_bert.encoder.layer.10.output.Linear_dense           [8, 256, 768]   \n",
            "264_bert.encoder.layer.10.output.Dropout_dropout        [8, 256, 768]   \n",
            "265_bert.encoder.layer.10.output.BertLayerNorm_...      [8, 256, 768]   \n",
            "266_bert.encoder.layer.11.attention.self.Linear...      [8, 256, 768]   \n",
            "267_bert.encoder.layer.11.attention.self.Linear...      [8, 256, 768]   \n",
            "268_bert.encoder.layer.11.attention.self.Linear...      [8, 256, 768]   \n",
            "269_bert.encoder.layer.11.attention.self.Dropou...  [8, 12, 256, 256]   \n",
            "270_bert.encoder.layer.11.attention.output.Line...      [8, 256, 768]   \n",
            "271_bert.encoder.layer.11.attention.output.Drop...      [8, 256, 768]   \n",
            "272_bert.encoder.layer.11.attention.output.Bert...      [8, 256, 768]   \n",
            "273_bert.encoder.layer.11.intermediate.Linear_d...     [8, 256, 3072]   \n",
            "274_bert.encoder.layer.11.output.Linear_dense           [8, 256, 768]   \n",
            "275_bert.encoder.layer.11.output.Dropout_dropout        [8, 256, 768]   \n",
            "276_bert.encoder.layer.11.output.BertLayerNorm_...      [8, 256, 768]   \n",
            "277_bert.pooler.Linear_dense                                 [8, 768]   \n",
            "278_bert.pooler.Tanh_activation                              [8, 768]   \n",
            "279_dropout                                                  [8, 768]   \n",
            "280_bert.embeddings.Embedding_word_embeddings            [8, 32, 768]   \n",
            "281_bert.embeddings.Embedding_position_embeddings        [8, 32, 768]   \n",
            "282_bert.embeddings.Embedding_token_type_embedd...       [8, 32, 768]   \n",
            "283_bert.embeddings.BertLayerNorm_LayerNorm              [8, 32, 768]   \n",
            "284_bert.embeddings.Dropout_dropout                      [8, 32, 768]   \n",
            "285_bert.encoder.layer.0.attention.self.Linear_...       [8, 32, 768]   \n",
            "286_bert.encoder.layer.0.attention.self.Linear_key       [8, 32, 768]   \n",
            "287_bert.encoder.layer.0.attention.self.Linear_...       [8, 32, 768]   \n",
            "288_bert.encoder.layer.0.attention.self.Dropout...    [8, 12, 32, 32]   \n",
            "289_bert.encoder.layer.0.attention.output.Linea...       [8, 32, 768]   \n",
            "290_bert.encoder.layer.0.attention.output.Dropo...       [8, 32, 768]   \n",
            "291_bert.encoder.layer.0.attention.output.BertL...       [8, 32, 768]   \n",
            "292_bert.encoder.layer.0.intermediate.Linear_dense      [8, 32, 3072]   \n",
            "293_bert.encoder.layer.0.output.Linear_dense             [8, 32, 768]   \n",
            "294_bert.encoder.layer.0.output.Dropout_dropout          [8, 32, 768]   \n",
            "295_bert.encoder.layer.0.output.BertLayerNorm_L...       [8, 32, 768]   \n",
            "296_bert.encoder.layer.1.attention.self.Linear_...       [8, 32, 768]   \n",
            "297_bert.encoder.layer.1.attention.self.Linear_key       [8, 32, 768]   \n",
            "298_bert.encoder.layer.1.attention.self.Linear_...       [8, 32, 768]   \n",
            "299_bert.encoder.layer.1.attention.self.Dropout...    [8, 12, 32, 32]   \n",
            "300_bert.encoder.layer.1.attention.output.Linea...       [8, 32, 768]   \n",
            "301_bert.encoder.layer.1.attention.output.Dropo...       [8, 32, 768]   \n",
            "302_bert.encoder.layer.1.attention.output.BertL...       [8, 32, 768]   \n",
            "303_bert.encoder.layer.1.intermediate.Linear_dense      [8, 32, 3072]   \n",
            "304_bert.encoder.layer.1.output.Linear_dense             [8, 32, 768]   \n",
            "305_bert.encoder.layer.1.output.Dropout_dropout          [8, 32, 768]   \n",
            "306_bert.encoder.layer.1.output.BertLayerNorm_L...       [8, 32, 768]   \n",
            "307_bert.encoder.layer.2.attention.self.Linear_...       [8, 32, 768]   \n",
            "308_bert.encoder.layer.2.attention.self.Linear_key       [8, 32, 768]   \n",
            "309_bert.encoder.layer.2.attention.self.Linear_...       [8, 32, 768]   \n",
            "310_bert.encoder.layer.2.attention.self.Dropout...    [8, 12, 32, 32]   \n",
            "311_bert.encoder.layer.2.attention.output.Linea...       [8, 32, 768]   \n",
            "312_bert.encoder.layer.2.attention.output.Dropo...       [8, 32, 768]   \n",
            "313_bert.encoder.layer.2.attention.output.BertL...       [8, 32, 768]   \n",
            "314_bert.encoder.layer.2.intermediate.Linear_dense      [8, 32, 3072]   \n",
            "315_bert.encoder.layer.2.output.Linear_dense             [8, 32, 768]   \n",
            "316_bert.encoder.layer.2.output.Dropout_dropout          [8, 32, 768]   \n",
            "317_bert.encoder.layer.2.output.BertLayerNorm_L...       [8, 32, 768]   \n",
            "318_bert.encoder.layer.3.attention.self.Linear_...       [8, 32, 768]   \n",
            "319_bert.encoder.layer.3.attention.self.Linear_key       [8, 32, 768]   \n",
            "320_bert.encoder.layer.3.attention.self.Linear_...       [8, 32, 768]   \n",
            "321_bert.encoder.layer.3.attention.self.Dropout...    [8, 12, 32, 32]   \n",
            "322_bert.encoder.layer.3.attention.output.Linea...       [8, 32, 768]   \n",
            "323_bert.encoder.layer.3.attention.output.Dropo...       [8, 32, 768]   \n",
            "324_bert.encoder.layer.3.attention.output.BertL...       [8, 32, 768]   \n",
            "325_bert.encoder.layer.3.intermediate.Linear_dense      [8, 32, 3072]   \n",
            "326_bert.encoder.layer.3.output.Linear_dense             [8, 32, 768]   \n",
            "327_bert.encoder.layer.3.output.Dropout_dropout          [8, 32, 768]   \n",
            "328_bert.encoder.layer.3.output.BertLayerNorm_L...       [8, 32, 768]   \n",
            "329_bert.encoder.layer.4.attention.self.Linear_...       [8, 32, 768]   \n",
            "330_bert.encoder.layer.4.attention.self.Linear_key       [8, 32, 768]   \n",
            "331_bert.encoder.layer.4.attention.self.Linear_...       [8, 32, 768]   \n",
            "332_bert.encoder.layer.4.attention.self.Dropout...    [8, 12, 32, 32]   \n",
            "333_bert.encoder.layer.4.attention.output.Linea...       [8, 32, 768]   \n",
            "334_bert.encoder.layer.4.attention.output.Dropo...       [8, 32, 768]   \n",
            "335_bert.encoder.layer.4.attention.output.BertL...       [8, 32, 768]   \n",
            "336_bert.encoder.layer.4.intermediate.Linear_dense      [8, 32, 3072]   \n",
            "337_bert.encoder.layer.4.output.Linear_dense             [8, 32, 768]   \n",
            "338_bert.encoder.layer.4.output.Dropout_dropout          [8, 32, 768]   \n",
            "339_bert.encoder.layer.4.output.BertLayerNorm_L...       [8, 32, 768]   \n",
            "340_bert.encoder.layer.5.attention.self.Linear_...       [8, 32, 768]   \n",
            "341_bert.encoder.layer.5.attention.self.Linear_key       [8, 32, 768]   \n",
            "342_bert.encoder.layer.5.attention.self.Linear_...       [8, 32, 768]   \n",
            "343_bert.encoder.layer.5.attention.self.Dropout...    [8, 12, 32, 32]   \n",
            "344_bert.encoder.layer.5.attention.output.Linea...       [8, 32, 768]   \n",
            "345_bert.encoder.layer.5.attention.output.Dropo...       [8, 32, 768]   \n",
            "346_bert.encoder.layer.5.attention.output.BertL...       [8, 32, 768]   \n",
            "347_bert.encoder.layer.5.intermediate.Linear_dense      [8, 32, 3072]   \n",
            "348_bert.encoder.layer.5.output.Linear_dense             [8, 32, 768]   \n",
            "349_bert.encoder.layer.5.output.Dropout_dropout          [8, 32, 768]   \n",
            "350_bert.encoder.layer.5.output.BertLayerNorm_L...       [8, 32, 768]   \n",
            "351_bert.encoder.layer.6.attention.self.Linear_...       [8, 32, 768]   \n",
            "352_bert.encoder.layer.6.attention.self.Linear_key       [8, 32, 768]   \n",
            "353_bert.encoder.layer.6.attention.self.Linear_...       [8, 32, 768]   \n",
            "354_bert.encoder.layer.6.attention.self.Dropout...    [8, 12, 32, 32]   \n",
            "355_bert.encoder.layer.6.attention.output.Linea...       [8, 32, 768]   \n",
            "356_bert.encoder.layer.6.attention.output.Dropo...       [8, 32, 768]   \n",
            "357_bert.encoder.layer.6.attention.output.BertL...       [8, 32, 768]   \n",
            "358_bert.encoder.layer.6.intermediate.Linear_dense      [8, 32, 3072]   \n",
            "359_bert.encoder.layer.6.output.Linear_dense             [8, 32, 768]   \n",
            "360_bert.encoder.layer.6.output.Dropout_dropout          [8, 32, 768]   \n",
            "361_bert.encoder.layer.6.output.BertLayerNorm_L...       [8, 32, 768]   \n",
            "362_bert.encoder.layer.7.attention.self.Linear_...       [8, 32, 768]   \n",
            "363_bert.encoder.layer.7.attention.self.Linear_key       [8, 32, 768]   \n",
            "364_bert.encoder.layer.7.attention.self.Linear_...       [8, 32, 768]   \n",
            "365_bert.encoder.layer.7.attention.self.Dropout...    [8, 12, 32, 32]   \n",
            "366_bert.encoder.layer.7.attention.output.Linea...       [8, 32, 768]   \n",
            "367_bert.encoder.layer.7.attention.output.Dropo...       [8, 32, 768]   \n",
            "368_bert.encoder.layer.7.attention.output.BertL...       [8, 32, 768]   \n",
            "369_bert.encoder.layer.7.intermediate.Linear_dense      [8, 32, 3072]   \n",
            "370_bert.encoder.layer.7.output.Linear_dense             [8, 32, 768]   \n",
            "371_bert.encoder.layer.7.output.Dropout_dropout          [8, 32, 768]   \n",
            "372_bert.encoder.layer.7.output.BertLayerNorm_L...       [8, 32, 768]   \n",
            "373_bert.encoder.layer.8.attention.self.Linear_...       [8, 32, 768]   \n",
            "374_bert.encoder.layer.8.attention.self.Linear_key       [8, 32, 768]   \n",
            "375_bert.encoder.layer.8.attention.self.Linear_...       [8, 32, 768]   \n",
            "376_bert.encoder.layer.8.attention.self.Dropout...    [8, 12, 32, 32]   \n",
            "377_bert.encoder.layer.8.attention.output.Linea...       [8, 32, 768]   \n",
            "378_bert.encoder.layer.8.attention.output.Dropo...       [8, 32, 768]   \n",
            "379_bert.encoder.layer.8.attention.output.BertL...       [8, 32, 768]   \n",
            "380_bert.encoder.layer.8.intermediate.Linear_dense      [8, 32, 3072]   \n",
            "381_bert.encoder.layer.8.output.Linear_dense             [8, 32, 768]   \n",
            "382_bert.encoder.layer.8.output.Dropout_dropout          [8, 32, 768]   \n",
            "383_bert.encoder.layer.8.output.BertLayerNorm_L...       [8, 32, 768]   \n",
            "384_bert.encoder.layer.9.attention.self.Linear_...       [8, 32, 768]   \n",
            "385_bert.encoder.layer.9.attention.self.Linear_key       [8, 32, 768]   \n",
            "386_bert.encoder.layer.9.attention.self.Linear_...       [8, 32, 768]   \n",
            "387_bert.encoder.layer.9.attention.self.Dropout...    [8, 12, 32, 32]   \n",
            "388_bert.encoder.layer.9.attention.output.Linea...       [8, 32, 768]   \n",
            "389_bert.encoder.layer.9.attention.output.Dropo...       [8, 32, 768]   \n",
            "390_bert.encoder.layer.9.attention.output.BertL...       [8, 32, 768]   \n",
            "391_bert.encoder.layer.9.intermediate.Linear_dense      [8, 32, 3072]   \n",
            "392_bert.encoder.layer.9.output.Linear_dense             [8, 32, 768]   \n",
            "393_bert.encoder.layer.9.output.Dropout_dropout          [8, 32, 768]   \n",
            "394_bert.encoder.layer.9.output.BertLayerNorm_L...       [8, 32, 768]   \n",
            "395_bert.encoder.layer.10.attention.self.Linear...       [8, 32, 768]   \n",
            "396_bert.encoder.layer.10.attention.self.Linear...       [8, 32, 768]   \n",
            "397_bert.encoder.layer.10.attention.self.Linear...       [8, 32, 768]   \n",
            "398_bert.encoder.layer.10.attention.self.Dropou...    [8, 12, 32, 32]   \n",
            "399_bert.encoder.layer.10.attention.output.Line...       [8, 32, 768]   \n",
            "400_bert.encoder.layer.10.attention.output.Drop...       [8, 32, 768]   \n",
            "401_bert.encoder.layer.10.attention.output.Bert...       [8, 32, 768]   \n",
            "402_bert.encoder.layer.10.intermediate.Linear_d...      [8, 32, 3072]   \n",
            "403_bert.encoder.layer.10.output.Linear_dense            [8, 32, 768]   \n",
            "404_bert.encoder.layer.10.output.Dropout_dropout         [8, 32, 768]   \n",
            "405_bert.encoder.layer.10.output.BertLayerNorm_...       [8, 32, 768]   \n",
            "406_bert.encoder.layer.11.attention.self.Linear...       [8, 32, 768]   \n",
            "407_bert.encoder.layer.11.attention.self.Linear...       [8, 32, 768]   \n",
            "408_bert.encoder.layer.11.attention.self.Linear...       [8, 32, 768]   \n",
            "409_bert.encoder.layer.11.attention.self.Dropou...    [8, 12, 32, 32]   \n",
            "410_bert.encoder.layer.11.attention.output.Line...       [8, 32, 768]   \n",
            "411_bert.encoder.layer.11.attention.output.Drop...       [8, 32, 768]   \n",
            "412_bert.encoder.layer.11.attention.output.Bert...       [8, 32, 768]   \n",
            "413_bert.encoder.layer.11.intermediate.Linear_d...      [8, 32, 3072]   \n",
            "414_bert.encoder.layer.11.output.Linear_dense            [8, 32, 768]   \n",
            "415_bert.encoder.layer.11.output.Dropout_dropout         [8, 32, 768]   \n",
            "416_bert.encoder.layer.11.output.BertLayerNorm_...       [8, 32, 768]   \n",
            "417_bert.pooler.Linear_dense                                 [8, 768]   \n",
            "418_bert.pooler.Tanh_activation                              [8, 768]   \n",
            "419_dropout                                                  [8, 768]   \n",
            "420_classifier                                                 [8, 2]   \n",
            "\n",
            "                                                        Params   Mult-Adds  \n",
            "Layer                                                                       \n",
            "0_bert.embeddings.Embedding_word_embeddings         23.440896M  23.440896M  \n",
            "1_bert.embeddings.Embedding_position_embeddings       393.216k    393.216k  \n",
            "2_bert.embeddings.Embedding_token_type_embeddings       1.536k      1.536k  \n",
            "3_bert.embeddings.BertLayerNorm_LayerNorm               1.536k       768.0  \n",
            "4_bert.embeddings.Dropout_dropout                            -           -  \n",
            "5_bert.encoder.layer.0.attention.self.Linear_query    590.592k    589.824k  \n",
            "6_bert.encoder.layer.0.attention.self.Linear_key      590.592k    589.824k  \n",
            "7_bert.encoder.layer.0.attention.self.Linear_value    590.592k    589.824k  \n",
            "8_bert.encoder.layer.0.attention.self.Dropout_d...           -           -  \n",
            "9_bert.encoder.layer.0.attention.output.Linear_...    590.592k    589.824k  \n",
            "10_bert.encoder.layer.0.attention.output.Dropou...           -           -  \n",
            "11_bert.encoder.layer.0.attention.output.BertLa...      1.536k       768.0  \n",
            "12_bert.encoder.layer.0.intermediate.Linear_dense    2.362368M   2.359296M  \n",
            "13_bert.encoder.layer.0.output.Linear_dense          2.360064M   2.359296M  \n",
            "14_bert.encoder.layer.0.output.Dropout_dropout               -           -  \n",
            "15_bert.encoder.layer.0.output.BertLayerNorm_La...      1.536k       768.0  \n",
            "16_bert.encoder.layer.1.attention.self.Linear_q...    590.592k    589.824k  \n",
            "17_bert.encoder.layer.1.attention.self.Linear_key     590.592k    589.824k  \n",
            "18_bert.encoder.layer.1.attention.self.Linear_v...    590.592k    589.824k  \n",
            "19_bert.encoder.layer.1.attention.self.Dropout_...           -           -  \n",
            "20_bert.encoder.layer.1.attention.output.Linear...    590.592k    589.824k  \n",
            "21_bert.encoder.layer.1.attention.output.Dropou...           -           -  \n",
            "22_bert.encoder.layer.1.attention.output.BertLa...      1.536k       768.0  \n",
            "23_bert.encoder.layer.1.intermediate.Linear_dense    2.362368M   2.359296M  \n",
            "24_bert.encoder.layer.1.output.Linear_dense          2.360064M   2.359296M  \n",
            "25_bert.encoder.layer.1.output.Dropout_dropout               -           -  \n",
            "26_bert.encoder.layer.1.output.BertLayerNorm_La...      1.536k       768.0  \n",
            "27_bert.encoder.layer.2.attention.self.Linear_q...    590.592k    589.824k  \n",
            "28_bert.encoder.layer.2.attention.self.Linear_key     590.592k    589.824k  \n",
            "29_bert.encoder.layer.2.attention.self.Linear_v...    590.592k    589.824k  \n",
            "30_bert.encoder.layer.2.attention.self.Dropout_...           -           -  \n",
            "31_bert.encoder.layer.2.attention.output.Linear...    590.592k    589.824k  \n",
            "32_bert.encoder.layer.2.attention.output.Dropou...           -           -  \n",
            "33_bert.encoder.layer.2.attention.output.BertLa...      1.536k       768.0  \n",
            "34_bert.encoder.layer.2.intermediate.Linear_dense    2.362368M   2.359296M  \n",
            "35_bert.encoder.layer.2.output.Linear_dense          2.360064M   2.359296M  \n",
            "36_bert.encoder.layer.2.output.Dropout_dropout               -           -  \n",
            "37_bert.encoder.layer.2.output.BertLayerNorm_La...      1.536k       768.0  \n",
            "38_bert.encoder.layer.3.attention.self.Linear_q...    590.592k    589.824k  \n",
            "39_bert.encoder.layer.3.attention.self.Linear_key     590.592k    589.824k  \n",
            "40_bert.encoder.layer.3.attention.self.Linear_v...    590.592k    589.824k  \n",
            "41_bert.encoder.layer.3.attention.self.Dropout_...           -           -  \n",
            "42_bert.encoder.layer.3.attention.output.Linear...    590.592k    589.824k  \n",
            "43_bert.encoder.layer.3.attention.output.Dropou...           -           -  \n",
            "44_bert.encoder.layer.3.attention.output.BertLa...      1.536k       768.0  \n",
            "45_bert.encoder.layer.3.intermediate.Linear_dense    2.362368M   2.359296M  \n",
            "46_bert.encoder.layer.3.output.Linear_dense          2.360064M   2.359296M  \n",
            "47_bert.encoder.layer.3.output.Dropout_dropout               -           -  \n",
            "48_bert.encoder.layer.3.output.BertLayerNorm_La...      1.536k       768.0  \n",
            "49_bert.encoder.layer.4.attention.self.Linear_q...    590.592k    589.824k  \n",
            "50_bert.encoder.layer.4.attention.self.Linear_key     590.592k    589.824k  \n",
            "51_bert.encoder.layer.4.attention.self.Linear_v...    590.592k    589.824k  \n",
            "52_bert.encoder.layer.4.attention.self.Dropout_...           -           -  \n",
            "53_bert.encoder.layer.4.attention.output.Linear...    590.592k    589.824k  \n",
            "54_bert.encoder.layer.4.attention.output.Dropou...           -           -  \n",
            "55_bert.encoder.layer.4.attention.output.BertLa...      1.536k       768.0  \n",
            "56_bert.encoder.layer.4.intermediate.Linear_dense    2.362368M   2.359296M  \n",
            "57_bert.encoder.layer.4.output.Linear_dense          2.360064M   2.359296M  \n",
            "58_bert.encoder.layer.4.output.Dropout_dropout               -           -  \n",
            "59_bert.encoder.layer.4.output.BertLayerNorm_La...      1.536k       768.0  \n",
            "60_bert.encoder.layer.5.attention.self.Linear_q...    590.592k    589.824k  \n",
            "61_bert.encoder.layer.5.attention.self.Linear_key     590.592k    589.824k  \n",
            "62_bert.encoder.layer.5.attention.self.Linear_v...    590.592k    589.824k  \n",
            "63_bert.encoder.layer.5.attention.self.Dropout_...           -           -  \n",
            "64_bert.encoder.layer.5.attention.output.Linear...    590.592k    589.824k  \n",
            "65_bert.encoder.layer.5.attention.output.Dropou...           -           -  \n",
            "66_bert.encoder.layer.5.attention.output.BertLa...      1.536k       768.0  \n",
            "67_bert.encoder.layer.5.intermediate.Linear_dense    2.362368M   2.359296M  \n",
            "68_bert.encoder.layer.5.output.Linear_dense          2.360064M   2.359296M  \n",
            "69_bert.encoder.layer.5.output.Dropout_dropout               -           -  \n",
            "70_bert.encoder.layer.5.output.BertLayerNorm_La...      1.536k       768.0  \n",
            "71_bert.encoder.layer.6.attention.self.Linear_q...    590.592k    589.824k  \n",
            "72_bert.encoder.layer.6.attention.self.Linear_key     590.592k    589.824k  \n",
            "73_bert.encoder.layer.6.attention.self.Linear_v...    590.592k    589.824k  \n",
            "74_bert.encoder.layer.6.attention.self.Dropout_...           -           -  \n",
            "75_bert.encoder.layer.6.attention.output.Linear...    590.592k    589.824k  \n",
            "76_bert.encoder.layer.6.attention.output.Dropou...           -           -  \n",
            "77_bert.encoder.layer.6.attention.output.BertLa...      1.536k       768.0  \n",
            "78_bert.encoder.layer.6.intermediate.Linear_dense    2.362368M   2.359296M  \n",
            "79_bert.encoder.layer.6.output.Linear_dense          2.360064M   2.359296M  \n",
            "80_bert.encoder.layer.6.output.Dropout_dropout               -           -  \n",
            "81_bert.encoder.layer.6.output.BertLayerNorm_La...      1.536k       768.0  \n",
            "82_bert.encoder.layer.7.attention.self.Linear_q...    590.592k    589.824k  \n",
            "83_bert.encoder.layer.7.attention.self.Linear_key     590.592k    589.824k  \n",
            "84_bert.encoder.layer.7.attention.self.Linear_v...    590.592k    589.824k  \n",
            "85_bert.encoder.layer.7.attention.self.Dropout_...           -           -  \n",
            "86_bert.encoder.layer.7.attention.output.Linear...    590.592k    589.824k  \n",
            "87_bert.encoder.layer.7.attention.output.Dropou...           -           -  \n",
            "88_bert.encoder.layer.7.attention.output.BertLa...      1.536k       768.0  \n",
            "89_bert.encoder.layer.7.intermediate.Linear_dense    2.362368M   2.359296M  \n",
            "90_bert.encoder.layer.7.output.Linear_dense          2.360064M   2.359296M  \n",
            "91_bert.encoder.layer.7.output.Dropout_dropout               -           -  \n",
            "92_bert.encoder.layer.7.output.BertLayerNorm_La...      1.536k       768.0  \n",
            "93_bert.encoder.layer.8.attention.self.Linear_q...    590.592k    589.824k  \n",
            "94_bert.encoder.layer.8.attention.self.Linear_key     590.592k    589.824k  \n",
            "95_bert.encoder.layer.8.attention.self.Linear_v...    590.592k    589.824k  \n",
            "96_bert.encoder.layer.8.attention.self.Dropout_...           -           -  \n",
            "97_bert.encoder.layer.8.attention.output.Linear...    590.592k    589.824k  \n",
            "98_bert.encoder.layer.8.attention.output.Dropou...           -           -  \n",
            "99_bert.encoder.layer.8.attention.output.BertLa...      1.536k       768.0  \n",
            "100_bert.encoder.layer.8.intermediate.Linear_dense   2.362368M   2.359296M  \n",
            "101_bert.encoder.layer.8.output.Linear_dense         2.360064M   2.359296M  \n",
            "102_bert.encoder.layer.8.output.Dropout_dropout              -           -  \n",
            "103_bert.encoder.layer.8.output.BertLayerNorm_L...      1.536k       768.0  \n",
            "104_bert.encoder.layer.9.attention.self.Linear_...    590.592k    589.824k  \n",
            "105_bert.encoder.layer.9.attention.self.Linear_key    590.592k    589.824k  \n",
            "106_bert.encoder.layer.9.attention.self.Linear_...    590.592k    589.824k  \n",
            "107_bert.encoder.layer.9.attention.self.Dropout...           -           -  \n",
            "108_bert.encoder.layer.9.attention.output.Linea...    590.592k    589.824k  \n",
            "109_bert.encoder.layer.9.attention.output.Dropo...           -           -  \n",
            "110_bert.encoder.layer.9.attention.output.BertL...      1.536k       768.0  \n",
            "111_bert.encoder.layer.9.intermediate.Linear_dense   2.362368M   2.359296M  \n",
            "112_bert.encoder.layer.9.output.Linear_dense         2.360064M   2.359296M  \n",
            "113_bert.encoder.layer.9.output.Dropout_dropout              -           -  \n",
            "114_bert.encoder.layer.9.output.BertLayerNorm_L...      1.536k       768.0  \n",
            "115_bert.encoder.layer.10.attention.self.Linear...    590.592k    589.824k  \n",
            "116_bert.encoder.layer.10.attention.self.Linear...    590.592k    589.824k  \n",
            "117_bert.encoder.layer.10.attention.self.Linear...    590.592k    589.824k  \n",
            "118_bert.encoder.layer.10.attention.self.Dropou...           -           -  \n",
            "119_bert.encoder.layer.10.attention.output.Line...    590.592k    589.824k  \n",
            "120_bert.encoder.layer.10.attention.output.Drop...           -           -  \n",
            "121_bert.encoder.layer.10.attention.output.Bert...      1.536k       768.0  \n",
            "122_bert.encoder.layer.10.intermediate.Linear_d...   2.362368M   2.359296M  \n",
            "123_bert.encoder.layer.10.output.Linear_dense        2.360064M   2.359296M  \n",
            "124_bert.encoder.layer.10.output.Dropout_dropout             -           -  \n",
            "125_bert.encoder.layer.10.output.BertLayerNorm_...      1.536k       768.0  \n",
            "126_bert.encoder.layer.11.attention.self.Linear...    590.592k    589.824k  \n",
            "127_bert.encoder.layer.11.attention.self.Linear...    590.592k    589.824k  \n",
            "128_bert.encoder.layer.11.attention.self.Linear...    590.592k    589.824k  \n",
            "129_bert.encoder.layer.11.attention.self.Dropou...           -           -  \n",
            "130_bert.encoder.layer.11.attention.output.Line...    590.592k    589.824k  \n",
            "131_bert.encoder.layer.11.attention.output.Drop...           -           -  \n",
            "132_bert.encoder.layer.11.attention.output.Bert...      1.536k       768.0  \n",
            "133_bert.encoder.layer.11.intermediate.Linear_d...   2.362368M   2.359296M  \n",
            "134_bert.encoder.layer.11.output.Linear_dense        2.360064M   2.359296M  \n",
            "135_bert.encoder.layer.11.output.Dropout_dropout             -           -  \n",
            "136_bert.encoder.layer.11.output.BertLayerNorm_...      1.536k       768.0  \n",
            "137_bert.pooler.Linear_dense                          590.592k    589.824k  \n",
            "138_bert.pooler.Tanh_activation                              -           -  \n",
            "139_dropout                                                  -           -  \n",
            "140_bert.embeddings.Embedding_word_embeddings                -  23.440896M  \n",
            "141_bert.embeddings.Embedding_position_embeddings            -    393.216k  \n",
            "142_bert.embeddings.Embedding_token_type_embedd...           -      1.536k  \n",
            "143_bert.embeddings.BertLayerNorm_LayerNorm                  -       768.0  \n",
            "144_bert.embeddings.Dropout_dropout                          -           -  \n",
            "145_bert.encoder.layer.0.attention.self.Linear_...           -    589.824k  \n",
            "146_bert.encoder.layer.0.attention.self.Linear_key           -    589.824k  \n",
            "147_bert.encoder.layer.0.attention.self.Linear_...           -    589.824k  \n",
            "148_bert.encoder.layer.0.attention.self.Dropout...           -           -  \n",
            "149_bert.encoder.layer.0.attention.output.Linea...           -    589.824k  \n",
            "150_bert.encoder.layer.0.attention.output.Dropo...           -           -  \n",
            "151_bert.encoder.layer.0.attention.output.BertL...           -       768.0  \n",
            "152_bert.encoder.layer.0.intermediate.Linear_dense           -   2.359296M  \n",
            "153_bert.encoder.layer.0.output.Linear_dense                 -   2.359296M  \n",
            "154_bert.encoder.layer.0.output.Dropout_dropout              -           -  \n",
            "155_bert.encoder.layer.0.output.BertLayerNorm_L...           -       768.0  \n",
            "156_bert.encoder.layer.1.attention.self.Linear_...           -    589.824k  \n",
            "157_bert.encoder.layer.1.attention.self.Linear_key           -    589.824k  \n",
            "158_bert.encoder.layer.1.attention.self.Linear_...           -    589.824k  \n",
            "159_bert.encoder.layer.1.attention.self.Dropout...           -           -  \n",
            "160_bert.encoder.layer.1.attention.output.Linea...           -    589.824k  \n",
            "161_bert.encoder.layer.1.attention.output.Dropo...           -           -  \n",
            "162_bert.encoder.layer.1.attention.output.BertL...           -       768.0  \n",
            "163_bert.encoder.layer.1.intermediate.Linear_dense           -   2.359296M  \n",
            "164_bert.encoder.layer.1.output.Linear_dense                 -   2.359296M  \n",
            "165_bert.encoder.layer.1.output.Dropout_dropout              -           -  \n",
            "166_bert.encoder.layer.1.output.BertLayerNorm_L...           -       768.0  \n",
            "167_bert.encoder.layer.2.attention.self.Linear_...           -    589.824k  \n",
            "168_bert.encoder.layer.2.attention.self.Linear_key           -    589.824k  \n",
            "169_bert.encoder.layer.2.attention.self.Linear_...           -    589.824k  \n",
            "170_bert.encoder.layer.2.attention.self.Dropout...           -           -  \n",
            "171_bert.encoder.layer.2.attention.output.Linea...           -    589.824k  \n",
            "172_bert.encoder.layer.2.attention.output.Dropo...           -           -  \n",
            "173_bert.encoder.layer.2.attention.output.BertL...           -       768.0  \n",
            "174_bert.encoder.layer.2.intermediate.Linear_dense           -   2.359296M  \n",
            "175_bert.encoder.layer.2.output.Linear_dense                 -   2.359296M  \n",
            "176_bert.encoder.layer.2.output.Dropout_dropout              -           -  \n",
            "177_bert.encoder.layer.2.output.BertLayerNorm_L...           -       768.0  \n",
            "178_bert.encoder.layer.3.attention.self.Linear_...           -    589.824k  \n",
            "179_bert.encoder.layer.3.attention.self.Linear_key           -    589.824k  \n",
            "180_bert.encoder.layer.3.attention.self.Linear_...           -    589.824k  \n",
            "181_bert.encoder.layer.3.attention.self.Dropout...           -           -  \n",
            "182_bert.encoder.layer.3.attention.output.Linea...           -    589.824k  \n",
            "183_bert.encoder.layer.3.attention.output.Dropo...           -           -  \n",
            "184_bert.encoder.layer.3.attention.output.BertL...           -       768.0  \n",
            "185_bert.encoder.layer.3.intermediate.Linear_dense           -   2.359296M  \n",
            "186_bert.encoder.layer.3.output.Linear_dense                 -   2.359296M  \n",
            "187_bert.encoder.layer.3.output.Dropout_dropout              -           -  \n",
            "188_bert.encoder.layer.3.output.BertLayerNorm_L...           -       768.0  \n",
            "189_bert.encoder.layer.4.attention.self.Linear_...           -    589.824k  \n",
            "190_bert.encoder.layer.4.attention.self.Linear_key           -    589.824k  \n",
            "191_bert.encoder.layer.4.attention.self.Linear_...           -    589.824k  \n",
            "192_bert.encoder.layer.4.attention.self.Dropout...           -           -  \n",
            "193_bert.encoder.layer.4.attention.output.Linea...           -    589.824k  \n",
            "194_bert.encoder.layer.4.attention.output.Dropo...           -           -  \n",
            "195_bert.encoder.layer.4.attention.output.BertL...           -       768.0  \n",
            "196_bert.encoder.layer.4.intermediate.Linear_dense           -   2.359296M  \n",
            "197_bert.encoder.layer.4.output.Linear_dense                 -   2.359296M  \n",
            "198_bert.encoder.layer.4.output.Dropout_dropout              -           -  \n",
            "199_bert.encoder.layer.4.output.BertLayerNorm_L...           -       768.0  \n",
            "200_bert.encoder.layer.5.attention.self.Linear_...           -    589.824k  \n",
            "201_bert.encoder.layer.5.attention.self.Linear_key           -    589.824k  \n",
            "202_bert.encoder.layer.5.attention.self.Linear_...           -    589.824k  \n",
            "203_bert.encoder.layer.5.attention.self.Dropout...           -           -  \n",
            "204_bert.encoder.layer.5.attention.output.Linea...           -    589.824k  \n",
            "205_bert.encoder.layer.5.attention.output.Dropo...           -           -  \n",
            "206_bert.encoder.layer.5.attention.output.BertL...           -       768.0  \n",
            "207_bert.encoder.layer.5.intermediate.Linear_dense           -   2.359296M  \n",
            "208_bert.encoder.layer.5.output.Linear_dense                 -   2.359296M  \n",
            "209_bert.encoder.layer.5.output.Dropout_dropout              -           -  \n",
            "210_bert.encoder.layer.5.output.BertLayerNorm_L...           -       768.0  \n",
            "211_bert.encoder.layer.6.attention.self.Linear_...           -    589.824k  \n",
            "212_bert.encoder.layer.6.attention.self.Linear_key           -    589.824k  \n",
            "213_bert.encoder.layer.6.attention.self.Linear_...           -    589.824k  \n",
            "214_bert.encoder.layer.6.attention.self.Dropout...           -           -  \n",
            "215_bert.encoder.layer.6.attention.output.Linea...           -    589.824k  \n",
            "216_bert.encoder.layer.6.attention.output.Dropo...           -           -  \n",
            "217_bert.encoder.layer.6.attention.output.BertL...           -       768.0  \n",
            "218_bert.encoder.layer.6.intermediate.Linear_dense           -   2.359296M  \n",
            "219_bert.encoder.layer.6.output.Linear_dense                 -   2.359296M  \n",
            "220_bert.encoder.layer.6.output.Dropout_dropout              -           -  \n",
            "221_bert.encoder.layer.6.output.BertLayerNorm_L...           -       768.0  \n",
            "222_bert.encoder.layer.7.attention.self.Linear_...           -    589.824k  \n",
            "223_bert.encoder.layer.7.attention.self.Linear_key           -    589.824k  \n",
            "224_bert.encoder.layer.7.attention.self.Linear_...           -    589.824k  \n",
            "225_bert.encoder.layer.7.attention.self.Dropout...           -           -  \n",
            "226_bert.encoder.layer.7.attention.output.Linea...           -    589.824k  \n",
            "227_bert.encoder.layer.7.attention.output.Dropo...           -           -  \n",
            "228_bert.encoder.layer.7.attention.output.BertL...           -       768.0  \n",
            "229_bert.encoder.layer.7.intermediate.Linear_dense           -   2.359296M  \n",
            "230_bert.encoder.layer.7.output.Linear_dense                 -   2.359296M  \n",
            "231_bert.encoder.layer.7.output.Dropout_dropout              -           -  \n",
            "232_bert.encoder.layer.7.output.BertLayerNorm_L...           -       768.0  \n",
            "233_bert.encoder.layer.8.attention.self.Linear_...           -    589.824k  \n",
            "234_bert.encoder.layer.8.attention.self.Linear_key           -    589.824k  \n",
            "235_bert.encoder.layer.8.attention.self.Linear_...           -    589.824k  \n",
            "236_bert.encoder.layer.8.attention.self.Dropout...           -           -  \n",
            "237_bert.encoder.layer.8.attention.output.Linea...           -    589.824k  \n",
            "238_bert.encoder.layer.8.attention.output.Dropo...           -           -  \n",
            "239_bert.encoder.layer.8.attention.output.BertL...           -       768.0  \n",
            "240_bert.encoder.layer.8.intermediate.Linear_dense           -   2.359296M  \n",
            "241_bert.encoder.layer.8.output.Linear_dense                 -   2.359296M  \n",
            "242_bert.encoder.layer.8.output.Dropout_dropout              -           -  \n",
            "243_bert.encoder.layer.8.output.BertLayerNorm_L...           -       768.0  \n",
            "244_bert.encoder.layer.9.attention.self.Linear_...           -    589.824k  \n",
            "245_bert.encoder.layer.9.attention.self.Linear_key           -    589.824k  \n",
            "246_bert.encoder.layer.9.attention.self.Linear_...           -    589.824k  \n",
            "247_bert.encoder.layer.9.attention.self.Dropout...           -           -  \n",
            "248_bert.encoder.layer.9.attention.output.Linea...           -    589.824k  \n",
            "249_bert.encoder.layer.9.attention.output.Dropo...           -           -  \n",
            "250_bert.encoder.layer.9.attention.output.BertL...           -       768.0  \n",
            "251_bert.encoder.layer.9.intermediate.Linear_dense           -   2.359296M  \n",
            "252_bert.encoder.layer.9.output.Linear_dense                 -   2.359296M  \n",
            "253_bert.encoder.layer.9.output.Dropout_dropout              -           -  \n",
            "254_bert.encoder.layer.9.output.BertLayerNorm_L...           -       768.0  \n",
            "255_bert.encoder.layer.10.attention.self.Linear...           -    589.824k  \n",
            "256_bert.encoder.layer.10.attention.self.Linear...           -    589.824k  \n",
            "257_bert.encoder.layer.10.attention.self.Linear...           -    589.824k  \n",
            "258_bert.encoder.layer.10.attention.self.Dropou...           -           -  \n",
            "259_bert.encoder.layer.10.attention.output.Line...           -    589.824k  \n",
            "260_bert.encoder.layer.10.attention.output.Drop...           -           -  \n",
            "261_bert.encoder.layer.10.attention.output.Bert...           -       768.0  \n",
            "262_bert.encoder.layer.10.intermediate.Linear_d...           -   2.359296M  \n",
            "263_bert.encoder.layer.10.output.Linear_dense                -   2.359296M  \n",
            "264_bert.encoder.layer.10.output.Dropout_dropout             -           -  \n",
            "265_bert.encoder.layer.10.output.BertLayerNorm_...           -       768.0  \n",
            "266_bert.encoder.layer.11.attention.self.Linear...           -    589.824k  \n",
            "267_bert.encoder.layer.11.attention.self.Linear...           -    589.824k  \n",
            "268_bert.encoder.layer.11.attention.self.Linear...           -    589.824k  \n",
            "269_bert.encoder.layer.11.attention.self.Dropou...           -           -  \n",
            "270_bert.encoder.layer.11.attention.output.Line...           -    589.824k  \n",
            "271_bert.encoder.layer.11.attention.output.Drop...           -           -  \n",
            "272_bert.encoder.layer.11.attention.output.Bert...           -       768.0  \n",
            "273_bert.encoder.layer.11.intermediate.Linear_d...           -   2.359296M  \n",
            "274_bert.encoder.layer.11.output.Linear_dense                -   2.359296M  \n",
            "275_bert.encoder.layer.11.output.Dropout_dropout             -           -  \n",
            "276_bert.encoder.layer.11.output.BertLayerNorm_...           -       768.0  \n",
            "277_bert.pooler.Linear_dense                                 -    589.824k  \n",
            "278_bert.pooler.Tanh_activation                              -           -  \n",
            "279_dropout                                                  -           -  \n",
            "280_bert.embeddings.Embedding_word_embeddings                -  23.440896M  \n",
            "281_bert.embeddings.Embedding_position_embeddings            -    393.216k  \n",
            "282_bert.embeddings.Embedding_token_type_embedd...           -      1.536k  \n",
            "283_bert.embeddings.BertLayerNorm_LayerNorm                  -       768.0  \n",
            "284_bert.embeddings.Dropout_dropout                          -           -  \n",
            "285_bert.encoder.layer.0.attention.self.Linear_...           -    589.824k  \n",
            "286_bert.encoder.layer.0.attention.self.Linear_key           -    589.824k  \n",
            "287_bert.encoder.layer.0.attention.self.Linear_...           -    589.824k  \n",
            "288_bert.encoder.layer.0.attention.self.Dropout...           -           -  \n",
            "289_bert.encoder.layer.0.attention.output.Linea...           -    589.824k  \n",
            "290_bert.encoder.layer.0.attention.output.Dropo...           -           -  \n",
            "291_bert.encoder.layer.0.attention.output.BertL...           -       768.0  \n",
            "292_bert.encoder.layer.0.intermediate.Linear_dense           -   2.359296M  \n",
            "293_bert.encoder.layer.0.output.Linear_dense                 -   2.359296M  \n",
            "294_bert.encoder.layer.0.output.Dropout_dropout              -           -  \n",
            "295_bert.encoder.layer.0.output.BertLayerNorm_L...           -       768.0  \n",
            "296_bert.encoder.layer.1.attention.self.Linear_...           -    589.824k  \n",
            "297_bert.encoder.layer.1.attention.self.Linear_key           -    589.824k  \n",
            "298_bert.encoder.layer.1.attention.self.Linear_...           -    589.824k  \n",
            "299_bert.encoder.layer.1.attention.self.Dropout...           -           -  \n",
            "300_bert.encoder.layer.1.attention.output.Linea...           -    589.824k  \n",
            "301_bert.encoder.layer.1.attention.output.Dropo...           -           -  \n",
            "302_bert.encoder.layer.1.attention.output.BertL...           -       768.0  \n",
            "303_bert.encoder.layer.1.intermediate.Linear_dense           -   2.359296M  \n",
            "304_bert.encoder.layer.1.output.Linear_dense                 -   2.359296M  \n",
            "305_bert.encoder.layer.1.output.Dropout_dropout              -           -  \n",
            "306_bert.encoder.layer.1.output.BertLayerNorm_L...           -       768.0  \n",
            "307_bert.encoder.layer.2.attention.self.Linear_...           -    589.824k  \n",
            "308_bert.encoder.layer.2.attention.self.Linear_key           -    589.824k  \n",
            "309_bert.encoder.layer.2.attention.self.Linear_...           -    589.824k  \n",
            "310_bert.encoder.layer.2.attention.self.Dropout...           -           -  \n",
            "311_bert.encoder.layer.2.attention.output.Linea...           -    589.824k  \n",
            "312_bert.encoder.layer.2.attention.output.Dropo...           -           -  \n",
            "313_bert.encoder.layer.2.attention.output.BertL...           -       768.0  \n",
            "314_bert.encoder.layer.2.intermediate.Linear_dense           -   2.359296M  \n",
            "315_bert.encoder.layer.2.output.Linear_dense                 -   2.359296M  \n",
            "316_bert.encoder.layer.2.output.Dropout_dropout              -           -  \n",
            "317_bert.encoder.layer.2.output.BertLayerNorm_L...           -       768.0  \n",
            "318_bert.encoder.layer.3.attention.self.Linear_...           -    589.824k  \n",
            "319_bert.encoder.layer.3.attention.self.Linear_key           -    589.824k  \n",
            "320_bert.encoder.layer.3.attention.self.Linear_...           -    589.824k  \n",
            "321_bert.encoder.layer.3.attention.self.Dropout...           -           -  \n",
            "322_bert.encoder.layer.3.attention.output.Linea...           -    589.824k  \n",
            "323_bert.encoder.layer.3.attention.output.Dropo...           -           -  \n",
            "324_bert.encoder.layer.3.attention.output.BertL...           -       768.0  \n",
            "325_bert.encoder.layer.3.intermediate.Linear_dense           -   2.359296M  \n",
            "326_bert.encoder.layer.3.output.Linear_dense                 -   2.359296M  \n",
            "327_bert.encoder.layer.3.output.Dropout_dropout              -           -  \n",
            "328_bert.encoder.layer.3.output.BertLayerNorm_L...           -       768.0  \n",
            "329_bert.encoder.layer.4.attention.self.Linear_...           -    589.824k  \n",
            "330_bert.encoder.layer.4.attention.self.Linear_key           -    589.824k  \n",
            "331_bert.encoder.layer.4.attention.self.Linear_...           -    589.824k  \n",
            "332_bert.encoder.layer.4.attention.self.Dropout...           -           -  \n",
            "333_bert.encoder.layer.4.attention.output.Linea...           -    589.824k  \n",
            "334_bert.encoder.layer.4.attention.output.Dropo...           -           -  \n",
            "335_bert.encoder.layer.4.attention.output.BertL...           -       768.0  \n",
            "336_bert.encoder.layer.4.intermediate.Linear_dense           -   2.359296M  \n",
            "337_bert.encoder.layer.4.output.Linear_dense                 -   2.359296M  \n",
            "338_bert.encoder.layer.4.output.Dropout_dropout              -           -  \n",
            "339_bert.encoder.layer.4.output.BertLayerNorm_L...           -       768.0  \n",
            "340_bert.encoder.layer.5.attention.self.Linear_...           -    589.824k  \n",
            "341_bert.encoder.layer.5.attention.self.Linear_key           -    589.824k  \n",
            "342_bert.encoder.layer.5.attention.self.Linear_...           -    589.824k  \n",
            "343_bert.encoder.layer.5.attention.self.Dropout...           -           -  \n",
            "344_bert.encoder.layer.5.attention.output.Linea...           -    589.824k  \n",
            "345_bert.encoder.layer.5.attention.output.Dropo...           -           -  \n",
            "346_bert.encoder.layer.5.attention.output.BertL...           -       768.0  \n",
            "347_bert.encoder.layer.5.intermediate.Linear_dense           -   2.359296M  \n",
            "348_bert.encoder.layer.5.output.Linear_dense                 -   2.359296M  \n",
            "349_bert.encoder.layer.5.output.Dropout_dropout              -           -  \n",
            "350_bert.encoder.layer.5.output.BertLayerNorm_L...           -       768.0  \n",
            "351_bert.encoder.layer.6.attention.self.Linear_...           -    589.824k  \n",
            "352_bert.encoder.layer.6.attention.self.Linear_key           -    589.824k  \n",
            "353_bert.encoder.layer.6.attention.self.Linear_...           -    589.824k  \n",
            "354_bert.encoder.layer.6.attention.self.Dropout...           -           -  \n",
            "355_bert.encoder.layer.6.attention.output.Linea...           -    589.824k  \n",
            "356_bert.encoder.layer.6.attention.output.Dropo...           -           -  \n",
            "357_bert.encoder.layer.6.attention.output.BertL...           -       768.0  \n",
            "358_bert.encoder.layer.6.intermediate.Linear_dense           -   2.359296M  \n",
            "359_bert.encoder.layer.6.output.Linear_dense                 -   2.359296M  \n",
            "360_bert.encoder.layer.6.output.Dropout_dropout              -           -  \n",
            "361_bert.encoder.layer.6.output.BertLayerNorm_L...           -       768.0  \n",
            "362_bert.encoder.layer.7.attention.self.Linear_...           -    589.824k  \n",
            "363_bert.encoder.layer.7.attention.self.Linear_key           -    589.824k  \n",
            "364_bert.encoder.layer.7.attention.self.Linear_...           -    589.824k  \n",
            "365_bert.encoder.layer.7.attention.self.Dropout...           -           -  \n",
            "366_bert.encoder.layer.7.attention.output.Linea...           -    589.824k  \n",
            "367_bert.encoder.layer.7.attention.output.Dropo...           -           -  \n",
            "368_bert.encoder.layer.7.attention.output.BertL...           -       768.0  \n",
            "369_bert.encoder.layer.7.intermediate.Linear_dense           -   2.359296M  \n",
            "370_bert.encoder.layer.7.output.Linear_dense                 -   2.359296M  \n",
            "371_bert.encoder.layer.7.output.Dropout_dropout              -           -  \n",
            "372_bert.encoder.layer.7.output.BertLayerNorm_L...           -       768.0  \n",
            "373_bert.encoder.layer.8.attention.self.Linear_...           -    589.824k  \n",
            "374_bert.encoder.layer.8.attention.self.Linear_key           -    589.824k  \n",
            "375_bert.encoder.layer.8.attention.self.Linear_...           -    589.824k  \n",
            "376_bert.encoder.layer.8.attention.self.Dropout...           -           -  \n",
            "377_bert.encoder.layer.8.attention.output.Linea...           -    589.824k  \n",
            "378_bert.encoder.layer.8.attention.output.Dropo...           -           -  \n",
            "379_bert.encoder.layer.8.attention.output.BertL...           -       768.0  \n",
            "380_bert.encoder.layer.8.intermediate.Linear_dense           -   2.359296M  \n",
            "381_bert.encoder.layer.8.output.Linear_dense                 -   2.359296M  \n",
            "382_bert.encoder.layer.8.output.Dropout_dropout              -           -  \n",
            "383_bert.encoder.layer.8.output.BertLayerNorm_L...           -       768.0  \n",
            "384_bert.encoder.layer.9.attention.self.Linear_...           -    589.824k  \n",
            "385_bert.encoder.layer.9.attention.self.Linear_key           -    589.824k  \n",
            "386_bert.encoder.layer.9.attention.self.Linear_...           -    589.824k  \n",
            "387_bert.encoder.layer.9.attention.self.Dropout...           -           -  \n",
            "388_bert.encoder.layer.9.attention.output.Linea...           -    589.824k  \n",
            "389_bert.encoder.layer.9.attention.output.Dropo...           -           -  \n",
            "390_bert.encoder.layer.9.attention.output.BertL...           -       768.0  \n",
            "391_bert.encoder.layer.9.intermediate.Linear_dense           -   2.359296M  \n",
            "392_bert.encoder.layer.9.output.Linear_dense                 -   2.359296M  \n",
            "393_bert.encoder.layer.9.output.Dropout_dropout              -           -  \n",
            "394_bert.encoder.layer.9.output.BertLayerNorm_L...           -       768.0  \n",
            "395_bert.encoder.layer.10.attention.self.Linear...           -    589.824k  \n",
            "396_bert.encoder.layer.10.attention.self.Linear...           -    589.824k  \n",
            "397_bert.encoder.layer.10.attention.self.Linear...           -    589.824k  \n",
            "398_bert.encoder.layer.10.attention.self.Dropou...           -           -  \n",
            "399_bert.encoder.layer.10.attention.output.Line...           -    589.824k  \n",
            "400_bert.encoder.layer.10.attention.output.Drop...           -           -  \n",
            "401_bert.encoder.layer.10.attention.output.Bert...           -       768.0  \n",
            "402_bert.encoder.layer.10.intermediate.Linear_d...           -   2.359296M  \n",
            "403_bert.encoder.layer.10.output.Linear_dense                -   2.359296M  \n",
            "404_bert.encoder.layer.10.output.Dropout_dropout             -           -  \n",
            "405_bert.encoder.layer.10.output.BertLayerNorm_...           -       768.0  \n",
            "406_bert.encoder.layer.11.attention.self.Linear...           -    589.824k  \n",
            "407_bert.encoder.layer.11.attention.self.Linear...           -    589.824k  \n",
            "408_bert.encoder.layer.11.attention.self.Linear...           -    589.824k  \n",
            "409_bert.encoder.layer.11.attention.self.Dropou...           -           -  \n",
            "410_bert.encoder.layer.11.attention.output.Line...           -    589.824k  \n",
            "411_bert.encoder.layer.11.attention.output.Drop...           -           -  \n",
            "412_bert.encoder.layer.11.attention.output.Bert...           -       768.0  \n",
            "413_bert.encoder.layer.11.intermediate.Linear_d...           -   2.359296M  \n",
            "414_bert.encoder.layer.11.output.Linear_dense                -   2.359296M  \n",
            "415_bert.encoder.layer.11.output.Dropout_dropout             -           -  \n",
            "416_bert.encoder.layer.11.output.BertLayerNorm_...           -       768.0  \n",
            "417_bert.pooler.Linear_dense                                 -    589.824k  \n",
            "418_bert.pooler.Tanh_activation                              -           -  \n",
            "419_dropout                                                  -           -  \n",
            "420_classifier                                           4.61k      4.608k  \n",
            "---------------------------------------------------------------------------------------------------------------------------\n",
            "                           Totals\n",
            "Total params           109.48685M\n",
            "Trainable params       109.48685M\n",
            "Non-trainable params          0.0\n",
            "Mult-Adds             328.142592M\n",
            "===========================================================================================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    Kernel Shape  \\\n",
              "Layer                                                              \n",
              "0_bert.embeddings.Embedding_word_embeddings         [768, 30522]   \n",
              "1_bert.embeddings.Embedding_position_embeddings       [768, 512]   \n",
              "2_bert.embeddings.Embedding_token_type_embeddings       [768, 2]   \n",
              "3_bert.embeddings.BertLayerNorm_LayerNorm                  [768]   \n",
              "4_bert.embeddings.Dropout_dropout                              -   \n",
              "...                                                          ...   \n",
              "416_bert.encoder.layer.11.output.BertLayerNorm_...         [768]   \n",
              "417_bert.pooler.Linear_dense                          [768, 768]   \n",
              "418_bert.pooler.Tanh_activation                                -   \n",
              "419_dropout                                                    -   \n",
              "420_classifier                                         [2304, 2]   \n",
              "\n",
              "                                                    Output Shape      Params  \\\n",
              "Layer                                                                          \n",
              "0_bert.embeddings.Embedding_word_embeddings         [8, 64, 768]  23440896.0   \n",
              "1_bert.embeddings.Embedding_position_embeddings     [8, 64, 768]    393216.0   \n",
              "2_bert.embeddings.Embedding_token_type_embeddings   [8, 64, 768]      1536.0   \n",
              "3_bert.embeddings.BertLayerNorm_LayerNorm           [8, 64, 768]      1536.0   \n",
              "4_bert.embeddings.Dropout_dropout                   [8, 64, 768]         NaN   \n",
              "...                                                          ...         ...   \n",
              "416_bert.encoder.layer.11.output.BertLayerNorm_...  [8, 32, 768]         NaN   \n",
              "417_bert.pooler.Linear_dense                            [8, 768]         NaN   \n",
              "418_bert.pooler.Tanh_activation                         [8, 768]         NaN   \n",
              "419_dropout                                             [8, 768]         NaN   \n",
              "420_classifier                                            [8, 2]      4610.0   \n",
              "\n",
              "                                                     Mult-Adds  \n",
              "Layer                                                           \n",
              "0_bert.embeddings.Embedding_word_embeddings         23440896.0  \n",
              "1_bert.embeddings.Embedding_position_embeddings       393216.0  \n",
              "2_bert.embeddings.Embedding_token_type_embeddings       1536.0  \n",
              "3_bert.embeddings.BertLayerNorm_LayerNorm                768.0  \n",
              "4_bert.embeddings.Dropout_dropout                          NaN  \n",
              "...                                                        ...  \n",
              "416_bert.encoder.layer.11.output.BertLayerNorm_...       768.0  \n",
              "417_bert.pooler.Linear_dense                          589824.0  \n",
              "418_bert.pooler.Tanh_activation                            NaN  \n",
              "419_dropout                                                NaN  \n",
              "420_classifier                                          4608.0  \n",
              "\n",
              "[421 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c96c4237-05b4-415a-831b-823f630344a3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_bert.embeddings.Embedding_word_embeddings</th>\n",
              "      <td>[768, 30522]</td>\n",
              "      <td>[8, 64, 768]</td>\n",
              "      <td>23440896.0</td>\n",
              "      <td>23440896.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_bert.embeddings.Embedding_position_embeddings</th>\n",
              "      <td>[768, 512]</td>\n",
              "      <td>[8, 64, 768]</td>\n",
              "      <td>393216.0</td>\n",
              "      <td>393216.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_bert.embeddings.Embedding_token_type_embeddings</th>\n",
              "      <td>[768, 2]</td>\n",
              "      <td>[8, 64, 768]</td>\n",
              "      <td>1536.0</td>\n",
              "      <td>1536.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_bert.embeddings.BertLayerNorm_LayerNorm</th>\n",
              "      <td>[768]</td>\n",
              "      <td>[8, 64, 768]</td>\n",
              "      <td>1536.0</td>\n",
              "      <td>768.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_bert.embeddings.Dropout_dropout</th>\n",
              "      <td>-</td>\n",
              "      <td>[8, 64, 768]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>416_bert.encoder.layer.11.output.BertLayerNorm_LayerNorm</th>\n",
              "      <td>[768]</td>\n",
              "      <td>[8, 32, 768]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>768.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>417_bert.pooler.Linear_dense</th>\n",
              "      <td>[768, 768]</td>\n",
              "      <td>[8, 768]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>589824.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>418_bert.pooler.Tanh_activation</th>\n",
              "      <td>-</td>\n",
              "      <td>[8, 768]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>419_dropout</th>\n",
              "      <td>-</td>\n",
              "      <td>[8, 768]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>420_classifier</th>\n",
              "      <td>[2304, 2]</td>\n",
              "      <td>[8, 2]</td>\n",
              "      <td>4610.0</td>\n",
              "      <td>4608.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>421 rows  4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c96c4237-05b4-415a-831b-823f630344a3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c96c4237-05b4-415a-831b-823f630344a3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c96c4237-05b4-415a-831b-823f630344a3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBwunYpyugFg"
      },
      "source": [
        "# Training Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "WyCA5fvyVkb0"
      },
      "outputs": [],
      "source": [
        "gc.collect() # These commands help you when you face CUDA OOM error\n",
        "torch.cuda.empty_cache()\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "GnTLL-5gMBrY"
      },
      "outputs": [],
      "source": [
        "train_acc = []\n",
        "val_acc = []\n",
        "train_loss = []\n",
        "val_loss = []"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, criterion, optimizer, scheduler):\n",
        "    since = Time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 100\n",
        "    best_acc = 0\n",
        "\n",
        "    scheduler.step()\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    fakeness_corrects = 0\n",
        "     # Progress Bar \n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5) \n",
        "    # Iterate over data.\n",
        "    for data in train_loader:\n",
        "        inputs = data[0:4]\n",
        "        fakeness = data[-1]\n",
        "\n",
        "        inputs1 = inputs[0] # News statement input\n",
        "        inputs2 = inputs[1] # Justification input\n",
        "        inputs3 = inputs[2] # Meta data input\n",
        "        inputs4 = inputs[3] # Credit scores input\n",
        "\n",
        "        inputs1 = inputs1.to(device)\n",
        "        inputs2 = inputs2.to(device)\n",
        "        inputs3 = inputs3.to(device)\n",
        "        inputs4 = inputs4.to(device)\n",
        "\n",
        "        fakeness = fakeness.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        # track history if only in train\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(inputs1, inputs2, inputs3, inputs4)\n",
        "            outputs = F.softmax(outputs,dim=1)\n",
        "            loss = criterion(outputs, torch.max(fakeness.float(), 1)[1])\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss.item() * inputs1.size(0)\n",
        "\n",
        "\n",
        "        fakeness_corrects += torch.sum(torch.max(outputs, 1)[1] == torch.max(fakeness, 1)[1])\n",
        "        batch_bar.set_postfix(\n",
        "            acc=\"{:.04f}%\".format(100 * fakeness_corrects / (config['batch_size']*(i + 1))),\n",
        "            loss=\"{:.04f}\".format(float(running_loss / (i + 1))),\n",
        "            num_correct=fakeness_corrects.item(),\n",
        "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "        \n",
        "        # loss.backward()\n",
        "        scaler.scale(loss).backward()\n",
        "        # optimizer.step()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "\n",
        "        batch_bar.update()\n",
        "\n",
        "    batch_bar.close()\n",
        "    epoch_loss = running_loss / len(X_train)\n",
        "\n",
        "\n",
        "    fakeness_acc = fakeness_corrects.double() / len(X_train)\n",
        "\n",
        "    print('Train total loss: {:.4f} '.format(epoch_loss))\n",
        "    print('Train fakeness_acc: {:.4f}'.format(fakeness_acc))\n",
        "\n",
        "    # Saving training acc and loss for each epoch\n",
        "    fakeness_acc1 = fakeness_acc.data\n",
        "    fakeness_acc1 = fakeness_acc1.cpu()\n",
        "    fakeness_acc1 = fakeness_acc1.numpy()\n",
        "    train_acc.append(fakeness_acc1)\n",
        "\n",
        "    #epoch_loss1 = epoch_loss.data\n",
        "    #epoch_loss1 = epoch_loss1.cpu()\n",
        "    #epoch_loss1 = epoch_loss1.numpy()\n",
        "    train_loss.append(epoch_loss)\n",
        "\n",
        "    return train_acc, train_loss"
      ],
      "metadata": {
        "id": "oiuoAWxgPo_p"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validation(model, criterion, optimizer, scheduler):\n",
        "    since = Time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_loss = 100\n",
        "    best_acc = 0\n",
        "    # Each epoch has a training and validation phase\n",
        "\n",
        "    model.eval()   # Set model to evaluate mode\n",
        "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val', ncols=5)\n",
        "\n",
        "    running_loss = 0.0\n",
        "\n",
        "    fakeness_corrects = 0\n",
        "\n",
        "    # Iterate over data.\n",
        "    for data in val_loader:\n",
        "        inputs = data[0:4]\n",
        "        fakeness = data[-1]\n",
        "        inputs1 = inputs[0] # News statement input\n",
        "        inputs2 = inputs[1] # Justification input\n",
        "        inputs3 = inputs[2] # Meta data input\n",
        "        inputs4 = inputs[3] # Credit scores input\n",
        "\n",
        "        inputs1 = inputs1.to(device)\n",
        "        inputs2 = inputs2.to(device)\n",
        "        inputs3 = inputs3.to(device)\n",
        "        inputs4 = inputs4.to(device)\n",
        "\n",
        "        fakeness = fakeness.to(device)\n",
        "\n",
        "        # forward\n",
        "        # track history if only in train\n",
        "        outputs = model(inputs1, inputs2, inputs3, inputs4)\n",
        "\n",
        "        outputs = F.softmax(outputs,dim=1)\n",
        "\n",
        "        loss = criterion(outputs, torch.max(fakeness.float(), 1)[1])\n",
        "        # backward + optimize only if in training phase\n",
        "        # statistics\n",
        "        running_loss += loss.item() * inputs1.size(0)\n",
        "\n",
        "\n",
        "        fakeness_corrects += torch.sum(torch.max(outputs, 1)[1] == torch.max(fakeness, 1)[1])\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            acc=\"{:.04f}%\".format(100 * fakeness_corrects / (config['batch_size']*(i + 1))),\n",
        "            loss=\"{:.04f}\".format(float(running_loss / (i + 1))),\n",
        "            num_correct=fakeness_corrects.item(),\n",
        "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "\n",
        "        batch_bar.update()\n",
        "\n",
        "    batch_bar.close()\n",
        "\n",
        "\n",
        "    epoch_loss = running_loss / len(X_val)\n",
        "    fakeness_acc = fakeness_corrects.double() / len(X_val)\n",
        "\n",
        "    print('Validation total loss: {:.4f} '.format(epoch_loss ))\n",
        "    print('Validation fakeness_acc: {:.4f}'.format(fakeness_acc))\n",
        "    best_acc = fakeness_acc\n",
        "\n",
        "    # Saving val acc and loss for each epoch\n",
        "    fakeness_acc1 = fakeness_acc.data\n",
        "    fakeness_acc1 = fakeness_acc1.cpu()\n",
        "    fakeness_acc1 = fakeness_acc1.numpy()\n",
        "    val_acc.append(fakeness_acc1)\n",
        "\n",
        "    #epoch_loss1 = epoch_loss.data\n",
        "    #epoch_loss1 = epoch_loss1.cpu()\n",
        "    #epoch_loss1 = epoch_loss1.numpy()\n",
        "    val_loss.append(epoch_loss)\n",
        "\n",
        "    print('Best val Acc: {:4f}'.format(float(best_acc)))\n",
        "\n",
        "    return val_acc, val_loss"
      ],
      "metadata": {
        "id": "fbpr1VaQT8O1"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiDduMaDIARE",
        "outputId": "14860e7b-054b-4be3-80ae-65e53ddbc864"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mngaddam\u001b[0m (\u001b[33mthe-spinning-top\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "wandb.login(key=\"3c0882202a0a1f93d55e16a0e94007adf1a84943\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "4s52yBOvICPZ",
        "outputId": "bba8eaa0-3198-4c3b-a90f-4448381972a3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221109_042803-1wshqldd</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/the-spinning-top/uncategorized/runs/1wshqldd\" target=\"_blank\">project-1</a></strong> to <a href=\"https://wandb.ai/the-spinning-top/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "run = wandb.init(\n",
        "    name = \"project-1\", ## Wandb creates random run names if you skip this field\n",
        "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    #id = '2lq40515', #Insert specific run id here if you want to resume a previous run\n",
        "    #resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    #project=\"hw3p2\", ### Project should be created in your wandb account \n",
        "    config=config, ### Wandb Config for your run\n",
        "    entity=\"the-spinning-top\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fLLj5KIMMOe"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "0nqLiAmkMMBc"
      },
      "outputs": [],
      "source": [
        "lrlast = .0001\n",
        "lrmain = .00001\n",
        "optim1 = torch.optim.Adam(\n",
        "    [\n",
        "        {\"params\":model.bert.parameters(),\"lr\": lrmain},\n",
        "        {\"params\":model.classifier.parameters(), \"lr\": lrlast},\n",
        "\n",
        "   ])\n",
        "\n",
        "#optim1 = optim.Adam(model.parameters(), lr=0.001)#,momentum=.9)\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim1\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "'''import focal_loss\n",
        "loss_args = {\"alpha\": 0.5, \"gamma\": 2.0}\n",
        "criterion = focal_loss.FocalLoss(*loss_args)'''\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 3 epochs\n",
        "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_ft, step_size=3, gamma=0.1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpYExu4vT4_g"
      },
      "source": [
        "### Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "tExvyl1BIdMC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41988c6f-c634-4441-ce83-25d3c84eb769"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train total loss: 0.5802 \n",
            "Train fakeness_acc: 0.7137\n"
          ]
        }
      ],
      "source": [
        "#sanity check\n",
        "train_acc, train_loss = train(model, criterion, optimizer_ft, exp_lr_scheduler)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sanity check\n",
        "val_acc, val_loss = validation(model, criterion, optimizer_ft, exp_lr_scheduler)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dc80qm1jL3sP",
        "outputId": "d5535458-46e5-4c97-8fc8-e6769c20a1f1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation total loss: 0.5769 \n",
            "Validation fakeness_acc: 0.7103\n",
            "Best val Acc: 0.710280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xu4joDb8wZAb",
        "outputId": "f4988f47-1f66-4d90-8c37-5922681729ca"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array(0.71028037), array(0.72897196), array(0.72352025)]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over number of epochs to train and evaluate your model\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "best_acc = 0.0 ### Monitor best accuracy in your run\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "    print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n",
        "    #t0 = time.time()\n",
        "\n",
        "    train_acc, train_loss = train(model, criterion, optimizer_ft, exp_lr_scheduler)\n",
        "    accuracy, val_loss = validation(model, criterion, optimizer_ft, exp_lr_scheduler)\n",
        "\n",
        "    #scheduler.step(accuracy) #ReduceLRonPlateau\n",
        "    exp_lr_scheduler.step() #StepLR\n",
        "\n",
        "    ### Log metrics at each epoch in your run - Optionally, you can log at each batch inside train/eval functions (explore wandb documentation/wandb recitation)\n",
        "    wandb.log({\"train loss\": train_loss, \"validation accuracy\": accuracy})\n",
        "\n",
        "    ### Save checkpoint at each epoch\n",
        "    ### Save checkpoint with information you want\n",
        "    torch.save({'epoch': epoch,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer_ft.state_dict(),\n",
        "              'loss': train_loss,\n",
        "              'acc': accuracy}, \n",
        "        './model_checkpoint.pth')\n",
        "      \n",
        "      ### Save checkpoint in wandb\n",
        "    wandb.save('checkpoint.pth')\n",
        "\n",
        "    # Is your training time very high? Look into mixed precision training if your GPU (Tesla T4, V100, etc) can make use of it \n",
        "    # Refer - https://pytorch.org/docs/stable/notes/amp_examples.html\n",
        "    #print('Duration:',time.time() - t0)\n",
        "### Finish your wandb run\n",
        "run.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWUEh273d1Wz",
        "outputId": "37979af8-1675-43ae-c01f-8bed5defff7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train total loss: 0.5308 \n",
            "Train fakeness_acc: 0.7706\n",
            "starting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation total loss: 0.5602 \n",
            "Validation fakeness_acc: 0.7282\n",
            "Best val Acc: 0.728193\n",
            "\n",
            "Epoch 2/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train total loss: 0.5302 \n",
            "Train fakeness_acc: 0.7716\n",
            "starting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation total loss: 0.5616 \n",
            "Validation fakeness_acc: 0.7251\n",
            "Best val Acc: 0.725078\n",
            "\n",
            "Epoch 3/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train total loss: 0.5281 \n",
            "Train fakeness_acc: 0.7745\n",
            "starting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation total loss: 0.5616 \n",
            "Validation fakeness_acc: 0.7243\n",
            "Best val Acc: 0.724299\n",
            "\n",
            "Epoch 4/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train total loss: 0.5278 \n",
            "Train fakeness_acc: 0.7773\n",
            "starting\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation total loss: 0.5616 \n",
            "Validation fakeness_acc: 0.7243\n",
            "Best val Acc: 0.724299\n",
            "\n",
            "Epoch 5/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train:   6%|         | 79/1280 [00:23<05:57,  3.36it/s, acc=5.0809%, loss=0.2491, lr=0.0000, num_correct=515]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pbO4RiEhwl4z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "fb4a7f07a05e69ccd70b8610395539af4ea9eff6155ae8b28a9ee451516f169c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}